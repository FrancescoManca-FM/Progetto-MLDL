{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copia di ProjectMLDL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j40FSXGxD2VD","colab_type":"text"},"source":["**Install requirements**"]},{"cell_type":"code","metadata":{"id":"uudv9Cj8E8OI","colab_type":"code","outputId":"3f865cf2-447c-4e04-8e69-3eab3200e99c","executionInfo":{"status":"ok","timestamp":1589824783345,"user_tz":-120,"elapsed":882,"user":{"displayName":"Christian Paesante","photoUrl":"","userId":"15693625301728617108"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"!pip3 install 'torch==1.3.1'\n","!pip3 install 'torchvision==0.5.0'\n","!pip3 install 'Pillow-SIMD'\n","!pip3 install 'tqdm'\"\"\"\n","# !pip install --upgrade wandb"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"0dC-rYdjD-E3","colab_type":"text"},"source":["**Import libraries**"]},{"cell_type":"code","metadata":{"id":"KQ6tCA_s2rru","colab_type":"code","colab":{}},"source":["import os\n","import logging\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Subset, DataLoader\n","from torch.backends import cudnn\n","\n","import torchvision\n","from torchvision import transforms\n","from torchvision.models import alexnet # , resnet18, resnet34\n","\n","from PIL import Image\n","from tqdm import tqdm\n","import random\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XOn3bHMEBzX","colab_type":"text"},"source":["**Set arguments**"]},{"cell_type":"code","metadata":{"id":"AmMFuQNV2ueu","colab_type":"code","colab":{}},"source":["DEVICE = 'cuda' # 'cuda' or 'cpu'\n","DATA_DIR = 'DATA' # here the dataset will be downloaded\n","\n","NUM_CLASSES = 100 \n","\n","# @toupdate the following vals (look at icarl paper)\n","\n","BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","LR = 0.01            # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 70      # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 49       # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Lf-WK3hEJCM","colab_type":"text"},"source":["**Retrieving dataset CIFAR1000**"]},{"cell_type":"code","metadata":{"id":"2n1do9ln3OVE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"82858c89-e6a2-4b1d-e68f-9f32a35e564c","executionInfo":{"status":"ok","timestamp":1589824792221,"user_tz":-120,"elapsed":9740,"user":{"displayName":"Christian Paesante","photoUrl":"","userId":"15693625301728617108"}}},"source":["# Clone github repository with dataset handler\n","# !rm -r Cifar100/ #debug purposes\n","if not os.path.isdir('./Cifar100'):\n","  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n","  !mv 'Progetto-MLDL' 'Cifar100'\n","  !rm -r Cifar100/Theoretical-Sources\n","  !rm -rf Cifar100/ProjectMLDL.ipynb\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Cloning into 'Progetto-MLDL'...\n","remote: Enumerating objects: 127, done.\u001b[K\n","remote: Counting objects: 100% (127/127), done.\u001b[K\n","remote: Compressing objects: 100% (86/86), done.\u001b[K\n","remote: Total 127 (delta 51), reused 94 (delta 28), pack-reused 0\u001b[K\n","Receiving objects: 100% (127/127), 3.44 MiB | 44.06 MiB/s, done.\n","Resolving deltas: 100% (51/51), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ysghtAWOPYZD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"5645b311-928c-468d-f030-c40b577addcf","executionInfo":{"status":"ok","timestamp":1589824839258,"user_tz":-120,"elapsed":8835,"user":{"displayName":"Christian Paesante","photoUrl":"","userId":"15693625301728617108"}}},"source":["from Cifar100.Dataset.cifar100 import CIFAR100\n","# Download dataset from the official sourse and save it into DATA\n","\n","# if not os.path.isdir('./{}'.format(DATA_DIR)):\n","!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","!tar -xf 'cifar-100-python.tar.gz'  \n","!mv 'cifar-100-python' $DATA_DIR\n","!rm -rf 'cifar-100-python.tar.gz'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["--2020-05-18 18:00:30--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 169001437 (161M) [application/x-gzip]\n","Saving to: ‘cifar-100-python.tar.gz’\n","\n","cifar-100-python.ta 100%[===================>] 161.17M  97.4MB/s    in 1.7s    \n","\n","2020-05-18 18:00:32 (97.4 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5oJ5m4V-ERDh","colab_type":"text"},"source":["**Define data preprocessing**"]},{"cell_type":"code","metadata":{"id":"rD2_Re8kPxRC","colab_type":"code","colab":{}},"source":["# it is ok to use also .5 mean and .5 std (faq1)\n","# @tocheck\n","# ref: https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/main.py + pytorch resnet documentation\n","# Define transformations for training\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n","])\n","\n","# Define transformations for evaluation\n","eval_transform = transforms.Compose([transforms.Resize(32),\n","                                      transforms.ToTensor(),\n","                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n","])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A0hvvskAS2Ia","colab_type":"text"},"source":["**Prepare dataset**"]},{"cell_type":"code","metadata":{"id":"cGQdJQqPkqVR","colab_type":"code","colab":{}},"source":["from Cifar100.Dataset.cifar100 import CIFAR100\n","# Import dataset\n","full_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n","train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n","test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n","\n","\n","# @todo\n","# split into train, test, \n","# print(len(train_dataset))\n","# print(len(test_dataset))\n","\n","\n","random.seed(30)\n","rand_subset = random.sample(list(full_dataset.df.index), 10000)\n","\n","dataset = Subset(full_dataset, rand_subset)\n","\n","\n","train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=30)\n","test_splits = test_dataset.split_classes(seed=30, dictionary_of='indices')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"657e8sY3SeeT","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","\n","def build_reverse_index():\n","    reverse_index = pd.DataFrame(columns=['group', 'labels'])\n","\n","    for k in train_splits.keys():\n","        labels = list(train_dataset.df.loc[train_splits[k]['train'],'labels'].value_counts().index)\n","        group = [k for i in range(len(labels))]\n","        data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n","        reverse_index = reverse_index.append(data, ignore_index=True)\n","\n","    return reverse_index\n","\n","def getLabels(reverse_index, outputs):\n","    outs = outputs.cpu().numpy()\n","    labels = reverse_index.loc[outs, 'labels']\n","\n","    labels = torch.tensor(list(labels))\n","    return labels.to(DEVICE)\n","\n","\n","outputs_labels_mapping = build_reverse_index()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B7fov9YAFTlj","colab_type":"text"},"source":["**Prepare dataloaders**"]},{"cell_type":"code","metadata":{"id":"M5MSItI0QVpn","colab_type":"code","colab":{}},"source":["# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n","train_subsets = []\n","val_subsets = []\n","test_subsets = []\n","\n","for v in train_splits.values():\n","    train_subs = Subset(train_dataset, v['train'])\n","    val_subs = Subset(train_dataset, v['val'])\n","    # train_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n","    # val_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n","    train_subsets.append(train_subs)\n","    val_subsets.append(val_subs)\n","\n","for v in test_splits.values():\n","    test_subs = Subset(test_dataset, v)\n","    # test_dl = DataLoader(test_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n","    test_subsets.append(test_subs)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzG6w15UudAP","colab_type":"code","colab":{}},"source":["import time\n","\n","def fake_train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n","    # By default, everything is loaded to cpu\n","    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","    cudnn.benchmark # Calling this optimizes runtime\n","    \n","    current_step = 0\n","    # Start iterating over the epochs\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        net.train()\n","        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n","\n","        running_corrects = 0\n","        running_loss = 0.0\n","        for images, labels in train_dataloader:\n","            print(labels)\n","            # Bring data over the device of choice\n","            images = images.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            optimizer.zero_grad() # Zero-ing the gradients\n","\n","            outputs = net(images)\n","\n","            loss = criterion(outputs, labels)\n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","            print(preds)\n","            preds = getLabels(outputs_labels_mapping, preds)\n","            print(preds)\n","\n","            break\n","        break\n","    print('end of fake train')\n","    \n","\n","\n","def train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):     \n","    # By default, everything is loaded to cpu\n","    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n","\n","    cudnn.benchmark # Calling this optimizes runtime\n","    \n","    current_step = 0\n","    # Start iterating over the epochs\n","    start_time = time.time()\n","    for epoch in range(num_epochs):\n","        net.train()\n","        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n","\n","        running_corrects = 0\n","        running_loss = 0.0\n","        for images, labels in train_dataloader:\n","            # Bring data over the device of choice\n","            images = images.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            optimizer.zero_grad() # Zero-ing the gradients\n","\n","            outputs = net(images)\n","\n","            loss = criterion(outputs, labels)\n","            \n","            # Get predictions\n","            _, preds = torch.max(outputs.data, 1)\n","            # preds = getLabels(outputs_labels_mapping, preds)\n","            # print(preds)\n","            \n","            # Update Corrects & Loss\n","            running_loss += loss.item() * images.size(0)\n","            running_corrects += torch.sum(preds == labels.data).data.item()\n","\n","            # Log loss\n","            if current_step % LOG_FREQUENCY == 0:\n","                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n","\n","            # Compute gradients for each layer and update weights\n","            loss.backward()  # backward pass: computes gradients\n","            optimizer.step() # update weights based on accumulated gradients\n","\n","            current_step += 1\n","        \n","        \n","        # Step the scheduler\n","        scheduler.step()\n","\n","        # Calculate Accuracy & Loss\n","        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n","        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n","        \n","        # wandb.log({'Epochs': epoch, 'Train Accuracy': epoch_acc, 'Train Loss': epoch_loss})\n","        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n","    print('Training finished in {} seconds'.format(time.time() - start_time))\n","\n","def validate(net, val_dataloader, criterion=None):\n","    net.eval()\n","\n","    running_corrects = 0\n","    running_loss = 0.0\n","    for images, labels in val_dataloader:\n","        # Bring data over the device of choice\n","        images = images.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        # Forward pass to the network\n","        outputs = net(images)\n","        \n","        # Update Corrects & Loss\n","        if criterion is not None:\n","            loss = criterion(outputs, labels)\n","            running_loss += loss.item() * images.size(0)\n","\n","        _, preds = torch.max(outputs.data, 1)\n","        # preds = getLabels(outputs_labels_mapping, preds)\n","        running_corrects += torch.sum(preds == labels.data).data.item()\n","        \n","    # Calculate Accuracy & Loss\n","    loss = running_loss / float(len(val_dataloader.dataset))\n","    acc = running_corrects / float(len(val_dataloader.dataset))\n","\n","    return acc, loss\n","\n","def test(net, test_dataloader):\n","    acc, _ = validate(net, test_dataloader)\n","    return acc\n","\n","# Joins 2+ subsets into a new Subset\n","def joinSubsets(dataset, subsets):\n","    indices = []\n","    for s in subsets:\n","        indices += s.indices\n","    return Subset(dataset, indices)\n","\n","def jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets):\n","    # wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n","\n","    net, criterion, optimizer, scheduler = getNet()\n","    # wandb.watch(net)\n","\n","    train_set = None\n","    test_set = None\n","    first_pass = True\n","    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n","\n","        # Builds growing train and test set. The new sets include data from previous class groups and current class group\n","        if train_set is None:\n","            train_set = train_subset\n","        else:\n","            train_set = joinSubsets(train_dataset, [train_set, train_subset])\n","        if test_set is None:\n","            test_set = test_subset\n","        else:\n","            test_set = joinSubsets(test_dataset, [test_set, test_subset])\n","\n","        if first_pass:\n","            first_pass = False\n","        else:\n","            addOutputs(net, 10)\n","\n","        # Trains model on previous and current class groups\n","        _, _, optimizer, scheduler = getNet()\n","        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n","        train(net, train_loader, criterion, optimizer, scheduler)\n","\n","        # Validate model on current class group\n","        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n","        acc, loss = validate(net, val_loader, criterion)\n","        print(acc, loss)\n","\n","        # Test the model on previous and current class groups\n","        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n","        acc = test(net, test_loader)\n","        print(acc)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Ff9pwV10b0v","colab_type":"code","colab":{}},"source":["from Cifar100.resnet import resnet34\n","\n","def getResNet34(output_size):\n","    net = resnet34(num_classes=output_size)\n","    # net.fc = nn.Linear(net.fc.in_features, output_size)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    parameters_to_optimize = net.parameters()\n","    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","    return net, criterion, optimizer, scheduler\n","\n","def addOutputsToResNet(net, new_outputs):\n","    in_features = net.fc.in_features\n","    out_features = net.fc.out_features\n","    weight = net.fc.weight.data\n","\n","    net.fc = nn.Linear(in_features, out_features + new_outputs)\n","    net.fc.weight.data[:out_features] = weight\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VDecUBiHl4G","colab_type":"code","outputId":"1fc083b0-dbb4-4f2f-c0a8-2596cf91af6b","executionInfo":{"status":"error","timestamp":1589825271350,"user_tz":-120,"elapsed":201071,"user":{"displayName":"Christian Paesante","photoUrl":"","userId":"15693625301728617108"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def getNet():\n","    return getResNet34(10)\n","\n","# jointTraining(getNet, addOutputsToResNet, train_subsets, val_subsets, test_subsets)\n","# wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n","net, criterion, optimizer, scheduler = getResNet34(100)\n","# wandb.watch(net)\n","# train_dataloader = DataLoader(train_subsets[0], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n","train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n","train(net, train_dataloader, criterion, optimizer, scheduler)\n","# outputs_labels_mapping.head(11)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Starting epoch 1/70, LR = [0.01]\n"],"name":"stdout"},{"output_type":"stream","text":["/content/Cifar100/resnet.py:105: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n","  nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Train step - Step 0, Loss 7.189126491546631\n","Train step - Step 10, Loss 6.164320468902588\n","Train step - Step 20, Loss 5.053770542144775\n","Train step - Step 30, Loss 4.779919147491455\n","Train step - Step 40, Loss 4.583662509918213\n","Train step - Step 50, Loss 4.727930545806885\n","Train step - Step 60, Loss 4.449949264526367\n","Train step - Step 70, Loss 4.345332145690918\n","Train epoch - Accuracy: 0.0296 Loss: 4.936385810852051 Corrects: 296\n","Starting epoch 2/70, LR = [0.01]\n","Train step - Step 80, Loss 4.038785934448242\n","Train step - Step 90, Loss 4.101312160491943\n","Train step - Step 100, Loss 4.226395130157471\n","Train step - Step 110, Loss 4.223204612731934\n","Train step - Step 120, Loss 4.123162269592285\n","Train step - Step 130, Loss 4.076954364776611\n","Train step - Step 140, Loss 3.964519500732422\n","Train step - Step 150, Loss 4.157322406768799\n","Train epoch - Accuracy: 0.0682 Loss: 4.131284858322144 Corrects: 682\n","Starting epoch 3/70, LR = [0.01]\n","Train step - Step 160, Loss 3.8597450256347656\n","Train step - Step 170, Loss 3.853320598602295\n","Train step - Step 180, Loss 3.821927309036255\n","Train step - Step 190, Loss 3.754955291748047\n","Train step - Step 200, Loss 3.766902208328247\n","Train step - Step 210, Loss 4.0400285720825195\n","Train step - Step 220, Loss 3.7925775051116943\n","Train step - Step 230, Loss 3.823925733566284\n","Train epoch - Accuracy: 0.1065 Loss: 3.868063869094849 Corrects: 1065\n","Starting epoch 4/70, LR = [0.01]\n","Train step - Step 240, Loss 3.7720470428466797\n","Train step - Step 250, Loss 3.5224342346191406\n","Train step - Step 260, Loss 3.5535411834716797\n","Train step - Step 270, Loss 3.389711856842041\n","Train step - Step 280, Loss 3.5383355617523193\n","Train step - Step 290, Loss 3.8406577110290527\n","Train step - Step 300, Loss 3.3943281173706055\n","Train step - Step 310, Loss 3.47029447555542\n","Train epoch - Accuracy: 0.1521 Loss: 3.6114921936035156 Corrects: 1521\n","Starting epoch 5/70, LR = [0.01]\n","Train step - Step 320, Loss 3.4899277687072754\n","Train step - Step 330, Loss 3.384838104248047\n","Train step - Step 340, Loss 3.348792552947998\n","Train step - Step 350, Loss 3.205402135848999\n","Train step - Step 360, Loss 3.2974720001220703\n","Train step - Step 370, Loss 3.0453858375549316\n","Train step - Step 380, Loss 3.3268799781799316\n","Train step - Step 390, Loss 3.153026819229126\n","Train epoch - Accuracy: 0.1868 Loss: 3.3767387619018554 Corrects: 1868\n","Starting epoch 6/70, LR = [0.01]\n","Train step - Step 400, Loss 3.2795567512512207\n","Train step - Step 410, Loss 3.267859935760498\n","Train step - Step 420, Loss 3.02764892578125\n","Train step - Step 430, Loss 3.0506949424743652\n","Train step - Step 440, Loss 2.891101121902466\n","Train step - Step 450, Loss 3.3795976638793945\n","Train step - Step 460, Loss 3.401554822921753\n","Train step - Step 470, Loss 2.998892307281494\n","Train epoch - Accuracy: 0.2414 Loss: 3.1269490734100343 Corrects: 2414\n","Starting epoch 7/70, LR = [0.01]\n","Train step - Step 480, Loss 2.9722256660461426\n","Train step - Step 490, Loss 2.784026861190796\n","Train step - Step 500, Loss 2.959451198577881\n","Train step - Step 510, Loss 2.9238169193267822\n","Train step - Step 520, Loss 3.2260398864746094\n","Train step - Step 530, Loss 2.7473134994506836\n","Train step - Step 540, Loss 2.660492181777954\n","Train step - Step 550, Loss 2.6502737998962402\n","Train epoch - Accuracy: 0.2938 Loss: 2.821654962539673 Corrects: 2938\n","Starting epoch 8/70, LR = [0.01]\n","Train step - Step 560, Loss 2.530430316925049\n","Train step - Step 570, Loss 2.653911828994751\n","Train step - Step 580, Loss 2.588574171066284\n","Train step - Step 590, Loss 2.5717573165893555\n","Train step - Step 600, Loss 2.5311391353607178\n","Train step - Step 610, Loss 2.4409444332122803\n","Train step - Step 620, Loss 2.3716726303100586\n","Train step - Step 630, Loss 2.389357089996338\n","Train epoch - Accuracy: 0.367 Loss: 2.4843366607666018 Corrects: 3670\n","Starting epoch 9/70, LR = [0.01]\n","Train step - Step 640, Loss 2.277952194213867\n","Train step - Step 650, Loss 1.9932971000671387\n","Train step - Step 660, Loss 2.0869576930999756\n","Train step - Step 670, Loss 2.327805757522583\n","Train step - Step 680, Loss 2.187587022781372\n","Train step - Step 690, Loss 2.0471315383911133\n","Train step - Step 700, Loss 2.1030397415161133\n","Train step - Step 710, Loss 2.3533661365509033\n","Train epoch - Accuracy: 0.447 Loss: 2.1295357898712157 Corrects: 4470\n","Starting epoch 10/70, LR = [0.01]\n","Train step - Step 720, Loss 1.775482416152954\n","Train step - Step 730, Loss 1.5437899827957153\n","Train step - Step 740, Loss 1.5440627336502075\n","Train step - Step 750, Loss 1.5926834344863892\n","Train step - Step 760, Loss 1.43913733959198\n","Train step - Step 770, Loss 1.7127360105514526\n","Train step - Step 780, Loss 1.7233426570892334\n","Train epoch - Accuracy: 0.5645 Loss: 1.6431568214416503 Corrects: 5645\n","Starting epoch 11/70, LR = [0.01]\n","Train step - Step 790, Loss 1.1066503524780273\n","Train step - Step 800, Loss 1.140978217124939\n","Train step - Step 810, Loss 1.1674542427062988\n","Train step - Step 820, Loss 1.2791719436645508\n","Train step - Step 830, Loss 1.1685280799865723\n","Train step - Step 840, Loss 1.2139002084732056\n","Train step - Step 850, Loss 1.1032886505126953\n","Train step - Step 860, Loss 1.0536930561065674\n","Train epoch - Accuracy: 0.6857 Loss: 1.1697092372894287 Corrects: 6857\n","Starting epoch 12/70, LR = [0.01]\n","Train step - Step 870, Loss 0.7131261825561523\n","Train step - Step 880, Loss 0.8024439811706543\n","Train step - Step 890, Loss 0.7643500566482544\n","Train step - Step 900, Loss 0.750810980796814\n","Train step - Step 910, Loss 0.3710107207298279\n","Train step - Step 920, Loss 0.7181937098503113\n","Train step - Step 930, Loss 0.4263610541820526\n","Train step - Step 940, Loss 0.6766608953475952\n","Train epoch - Accuracy: 0.8327 Loss: 0.6513112396240235 Corrects: 8327\n","Starting epoch 13/70, LR = [0.01]\n","Train step - Step 950, Loss 0.7859281301498413\n","Train step - Step 960, Loss 0.5891374349594116\n","Train step - Step 970, Loss 0.6702249050140381\n","Train step - Step 980, Loss 0.436679869890213\n","Train step - Step 990, Loss 0.32387518882751465\n","Train step - Step 1000, Loss 0.3811442255973816\n","Train step - Step 1010, Loss 0.30837222933769226\n","Train step - Step 1020, Loss 0.3700354993343353\n","Train epoch - Accuracy: 0.8946 Loss: 0.4383300189495087 Corrects: 8946\n","Starting epoch 14/70, LR = [0.01]\n","Train step - Step 1030, Loss 0.3098587691783905\n","Train step - Step 1040, Loss 0.13065770268440247\n","Train step - Step 1050, Loss 0.2224305272102356\n","Train step - Step 1060, Loss 0.10054623335599899\n","Train step - Step 1070, Loss 0.13657405972480774\n","Train step - Step 1080, Loss 0.0941883772611618\n","Train step - Step 1090, Loss 0.050583064556121826\n","Train step - Step 1100, Loss 0.05226413160562515\n","Train epoch - Accuracy: 0.9807 Loss: 0.1194126895904541 Corrects: 9807\n","Starting epoch 15/70, LR = [0.01]\n","Train step - Step 1110, Loss 0.034617915749549866\n","Train step - Step 1120, Loss 0.034406960010528564\n","Train step - Step 1130, Loss 0.04095479100942612\n","Train step - Step 1140, Loss 0.02418157085776329\n","Train step - Step 1150, Loss 0.018468208611011505\n","Train step - Step 1160, Loss 0.018767617642879486\n","Train step - Step 1170, Loss 0.021240219473838806\n","Train step - Step 1180, Loss 0.031933486461639404\n","Train epoch - Accuracy: 0.9956 Loss: 0.03887869162559509 Corrects: 9956\n","Starting epoch 16/70, LR = [0.01]\n","Train step - Step 1190, Loss 0.11038921773433685\n","Train step - Step 1200, Loss 0.07051258534193039\n","Train step - Step 1210, Loss 0.03366728872060776\n","Train step - Step 1220, Loss 0.0170062854886055\n","Train step - Step 1230, Loss 0.03224533796310425\n","Train step - Step 1240, Loss 0.015350140631198883\n","Train step - Step 1250, Loss 0.012730851769447327\n","Train step - Step 1260, Loss 0.01462063193321228\n","Train epoch - Accuracy: 0.9951 Loss: 0.03307413396835327 Corrects: 9951\n","Starting epoch 17/70, LR = [0.01]\n","Train step - Step 1270, Loss 0.011382333934307098\n","Train step - Step 1280, Loss 0.017196036875247955\n","Train step - Step 1290, Loss 0.009421370923519135\n","Train step - Step 1300, Loss 0.008725568652153015\n","Train step - Step 1310, Loss 0.012544505298137665\n","Train step - Step 1320, Loss 0.007703736424446106\n","Train step - Step 1330, Loss 0.014536440372467041\n","Train step - Step 1340, Loss 0.009744703769683838\n","Train epoch - Accuracy: 0.9992 Loss: 0.016451544094085693 Corrects: 9992\n","Starting epoch 18/70, LR = [0.01]\n","Train step - Step 1350, Loss 0.042801618576049805\n","Train step - Step 1360, Loss 0.02057516574859619\n","Train step - Step 1370, Loss 0.010601796209812164\n","Train step - Step 1380, Loss 0.009840138256549835\n","Train step - Step 1390, Loss 0.010555148124694824\n","Train step - Step 1400, Loss 0.008647315204143524\n","Train step - Step 1410, Loss 0.06496122479438782\n","Train step - Step 1420, Loss 0.008617281913757324\n","Train epoch - Accuracy: 0.9981 Loss: 0.023902715873718262 Corrects: 9981\n","Starting epoch 19/70, LR = [0.01]\n","Train step - Step 1430, Loss 0.023043468594551086\n","Train step - Step 1440, Loss 0.01221095398068428\n","Train step - Step 1450, Loss 0.010278597474098206\n","Train step - Step 1460, Loss 0.10588180273771286\n","Train step - Step 1470, Loss 0.006885536015033722\n","Train step - Step 1480, Loss 0.07007315009832382\n","Train step - Step 1490, Loss 0.0097956582903862\n","Train step - Step 1500, Loss 0.01343446969985962\n","Train epoch - Accuracy: 0.9983 Loss: 0.012994726133346558 Corrects: 9983\n","Starting epoch 20/70, LR = [0.01]\n","Train step - Step 1510, Loss 0.0057191625237464905\n","Train step - Step 1520, Loss 0.004637926816940308\n","Train step - Step 1530, Loss 0.005259357392787933\n","Train step - Step 1540, Loss 0.0042861104011535645\n","Train step - Step 1550, Loss 0.0032594799995422363\n","Train step - Step 1560, Loss 0.0869014635682106\n","Train step - Step 1570, Loss 0.0034877583384513855\n","Train epoch - Accuracy: 0.9992 Loss: 0.010494920444488525 Corrects: 9992\n","Starting epoch 21/70, LR = [0.01]\n","Train step - Step 1580, Loss 0.009822927415370941\n","Train step - Step 1590, Loss 0.3484567403793335\n","Train step - Step 1600, Loss 0.2634833753108978\n","Train step - Step 1610, Loss 0.19039666652679443\n","Train step - Step 1620, Loss 0.11238868534564972\n","Train step - Step 1630, Loss 0.07958704978227615\n","Train step - Step 1640, Loss 0.07821492105722427\n","Train step - Step 1650, Loss 0.05253363400697708\n","Train epoch - Accuracy: 0.971 Loss: 0.15088438844680785 Corrects: 9710\n","Starting epoch 22/70, LR = [0.01]\n","Train step - Step 1660, Loss 0.02046952396631241\n","Train step - Step 1670, Loss 0.02681858092546463\n","Train step - Step 1680, Loss 0.03147798031568527\n","Train step - Step 1690, Loss 0.012393735349178314\n","Train step - Step 1700, Loss 0.014400124549865723\n","Train step - Step 1710, Loss 0.08837749809026718\n","Train step - Step 1720, Loss 0.020318306982517242\n","Train step - Step 1730, Loss 0.02408628910779953\n","Train epoch - Accuracy: 0.9973 Loss: 0.026361264085769654 Corrects: 9973\n","Starting epoch 23/70, LR = [0.01]\n","Train step - Step 1740, Loss 0.010963387787342072\n","Train step - Step 1750, Loss 0.016224324703216553\n","Train step - Step 1760, Loss 0.021586425602436066\n","Train step - Step 1770, Loss 0.01309153437614441\n","Train step - Step 1780, Loss 0.01709664613008499\n","Train step - Step 1790, Loss 0.011030681431293488\n","Train step - Step 1800, Loss 0.010087572038173676\n","Train step - Step 1810, Loss 0.00788804143667221\n","Train epoch - Accuracy: 0.9973 Loss: 0.02176654372215271 Corrects: 9973\n","Starting epoch 24/70, LR = [0.01]\n","Train step - Step 1820, Loss 0.003941766917705536\n","Train step - Step 1830, Loss 0.004745744168758392\n","Train step - Step 1840, Loss 0.006152167916297913\n","Train step - Step 1850, Loss 0.006359994411468506\n","Train step - Step 1860, Loss 0.003650389611721039\n","Train step - Step 1870, Loss 0.004328399896621704\n","Train step - Step 1880, Loss 0.007804617285728455\n","Train step - Step 1890, Loss 0.0037346333265304565\n","Train epoch - Accuracy: 0.9993 Loss: 0.011150198888778686 Corrects: 9993\n","Starting epoch 25/70, LR = [0.01]\n","Train step - Step 1900, Loss 0.0052138566970825195\n","Train step - Step 1910, Loss 0.004504181444644928\n","Train step - Step 1920, Loss 0.002742953598499298\n","Train step - Step 1930, Loss 0.003435969352722168\n","Train step - Step 1940, Loss 0.0036697611212730408\n","Train step - Step 1950, Loss 0.0051347315311431885\n","Train step - Step 1960, Loss 0.0033902153372764587\n","Train step - Step 1970, Loss 0.0033629238605499268\n","Train epoch - Accuracy: 0.9995 Loss: 0.008569302463531494 Corrects: 9995\n","Starting epoch 26/70, LR = [0.01]\n","Train step - Step 1980, Loss 0.002951517701148987\n","Train step - Step 1990, Loss 0.0026308298110961914\n","Train step - Step 2000, Loss 0.003091089427471161\n","Train step - Step 2010, Loss 0.0025115162134170532\n","Train step - Step 2020, Loss 0.05283355712890625\n","Train step - Step 2030, Loss 0.0033535435795783997\n","Train step - Step 2040, Loss 0.0023820847272872925\n","Train step - Step 2050, Loss 0.0023331642150878906\n","Train epoch - Accuracy: 0.9995 Loss: 0.007136747598648071 Corrects: 9995\n","Starting epoch 27/70, LR = [0.01]\n","Train step - Step 2060, Loss 0.004500828683376312\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-a12cff7a1713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# train_dataloader = DataLoader(train_subsets[0], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# outputs_labels_mapping.head(11)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-a6f509fe5100>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Compute gradients for each layer and update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backward pass: computes gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update weights based on accumulated gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}