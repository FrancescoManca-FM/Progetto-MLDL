{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FUNZIONANTE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40FSXGxD2VD",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudv9Cj8E8OI",
        "colab_type": "code",
        "outputId": "2ec89ff5-bbe9-4d93-f1da-802acca17cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dC-rYdjD-E3",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet # , resnet18, resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOn3bHMEBzX",
        "colab_type": "text"
      },
      "source": [
        "**Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmMFuQNV2ueu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "NUM_CLASSES = 100 \n",
        "\n",
        "# @toupdate the following vals (look at icarl paper)\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.05           # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 10      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 49       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-WK3hEJCM",
        "colab_type": "text"
      },
      "source": [
        "**Retrieving dataset CIFAR1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n1do9ln3OVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone github repository with dataset handler\n",
        "# !rm -r Cifar100/ #debug purposes\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab_type": "code",
        "outputId": "700fc8f3-5c50-4b50-a2f4-a9c948191bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Download dataset from the official sourse and save it into DATA\n",
        "\n",
        "# if not os.path.isdir('./{}'.format(DATA_DIR)):\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xf 'cifar-100-python.tar.gz'  \n",
        "!mv 'cifar-100-python' $DATA_DIR\n",
        "!rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-18 21:44:04--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  14.3MB/s    in 13s     \n",
            "\n",
            "2020-05-18 21:44:18 (12.6 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ5m4V-ERDh",
        "colab_type": "text"
      },
      "source": [
        "**Define data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD2_Re8kPxRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is ok to use also .5 mean and .5 std (faq1)\n",
        "# @tocheck\n",
        "# ref: https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/main.py + pytorch resnet documentation\n",
        "# Define transformations for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.507, 0.487, 0.441], std=[0.267, 0.256, 0.276])\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.Resize(32),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hvvskAS2Ia",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQdJQqPkqVR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Import dataset\n",
        "full_dataset = CIFAR100(DATA_DIR, split='train', transform=transform_train)\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=transform_train)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "\n",
        "# @todo\n",
        "# split into train, test, \n",
        "# print(len(train_dataset))\n",
        "# print(len(test_dataset))\n",
        "\n",
        "\n",
        "random.seed(30)\n",
        "rand_subset = random.sample(list(full_dataset.df.index), 10000)\n",
        "\n",
        "dataset = Subset(full_dataset, rand_subset)\n",
        "\n",
        "\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=30)\n",
        "test_splits = test_dataset.split_classes(seed=30, dictionary_of='indices')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "657e8sY3SeeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def build_reverse_index():\n",
        "    reverse_index = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "    for k in train_splits.keys():\n",
        "        labels = list(train_dataset.df.loc[train_splits[k]['train'],'labels'].value_counts().index)\n",
        "        group = [k for i in range(len(labels))]\n",
        "        data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "        reverse_index = reverse_index.append(data, ignore_index=True)\n",
        "\n",
        "    return reverse_index\n",
        "\n",
        "def getLabels(reverse_index, outputs):\n",
        "    outs = outputs.cpu().numpy()\n",
        "    labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "    labels = torch.tensor(list(labels))\n",
        "    return labels.to(DEVICE)\n",
        "\n",
        "\n",
        "outputs_labels_mapping = build_reverse_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fov9YAFTlj",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    # train_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    # val_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for v in test_splits.values():\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    # test_dl = DataLoader(test_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    test_subsets.append(test_subs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzG6w15UudAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def fake_train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):\n",
        "    # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            print(labels)\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            print(preds)\n",
        "            preds = getLabels(outputs_labels_mapping, preds)\n",
        "            print(preds)\n",
        "\n",
        "            break\n",
        "        break\n",
        "    print('end of fake train')\n",
        "    \n",
        "\n",
        "\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):     \n",
        "    # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            # preds = getLabels(outputs_labels_mapping, preds)\n",
        "            # print(preds)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        # wandb.log({'Epochs': epoch, 'Train Accuracy': epoch_acc, 'Train Loss': epoch_loss})\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion=None):\n",
        "    net.eval()\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for images, labels in val_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        # preds = getLabels(outputs_labels_mapping, preds)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "    # Calculate Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss\n",
        "\n",
        "def test(net, test_dataloader):\n",
        "    acc, _ = validate(net, test_dataloader)\n",
        "    return acc\n",
        "\n",
        "# Joins 2+ subsets into a new Subset\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets):\n",
        "    # wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n",
        "\n",
        "    net, criterion, optimizer, scheduler = getNet()\n",
        "    # wandb.watch(net)\n",
        "\n",
        "    train_set = None\n",
        "    test_set = None\n",
        "    first_pass = True\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "\n",
        "        # Builds growing train and test set. The new sets include data from previous class groups and current class group\n",
        "        if train_set is None:\n",
        "            train_set = train_subset\n",
        "        else:\n",
        "            train_set = joinSubsets(train_dataset, [train_set, train_subset])\n",
        "        if test_set is None:\n",
        "            test_set = test_subset\n",
        "        else:\n",
        "            test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        if first_pass:\n",
        "            first_pass = False\n",
        "        else:\n",
        "            addOutputs(net, 10)\n",
        "\n",
        "        # Trains model on previous and current class groups\n",
        "        _, _, optimizer, scheduler = getNet()\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        train(net, train_loader, criterion, optimizer, scheduler)\n",
        "\n",
        "        # Validate model on current class group\n",
        "        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc, loss = validate(net, val_loader, criterion)\n",
        "        print(acc, loss)\n",
        "\n",
        "        # Test the model on previous and current class groups\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        print(acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ff9pwV10b0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.resnet import resnet34\n",
        "\n",
        "def getResNet34(output_size):\n",
        "    net = resnet34(num_classes=output_size)\n",
        "    # net.fc = nn.Linear(net.fc.in_features, output_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputsToResNet(net, new_outputs):\n",
        "    in_features = net.fc.in_features\n",
        "    out_features = net.fc.out_features\n",
        "    weight = net.fc.weight.data\n",
        "\n",
        "    net.fc = nn.Linear(in_features, out_features + new_outputs)\n",
        "    net.fc.weight.data[:out_features] = weight\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VDecUBiHl4G",
        "colab_type": "code",
        "outputId": "a492460e-35b2-48b8-d0a3-98288fe3aeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def getNet():\n",
        "    return getResNet34(10)\n",
        "\n",
        "# jointTraining(getNet, addOutputsToResNet, train_subsets, val_subsets, test_subsets)\n",
        "# wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n",
        "net, criterion, optimizer, scheduler = getResNet34(100)\n",
        "# wandb.watch(net)\n",
        "# train_dataloader = DataLoader(train_subsets[0], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "train(net, train_dataloader, criterion, optimizer, scheduler)\n",
        "# outputs_labels_mapping.head(11)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/10, LR = [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/Cifar100/resnet.py:105: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "  nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 6.94850492477417\n",
            "Train step - Step 10, Loss 5.917949676513672\n",
            "Train step - Step 20, Loss 5.246488571166992\n",
            "Train step - Step 30, Loss 5.342779636383057\n",
            "Train step - Step 40, Loss 4.609941482543945\n",
            "Train step - Step 50, Loss 4.429506301879883\n",
            "Train step - Step 60, Loss 4.270692825317383\n",
            "Train step - Step 70, Loss 4.31650972366333\n",
            "Train epoch - Accuracy: 0.0277 Loss: 4.990581446838379 Corrects: 277\n",
            "Starting epoch 2/10, LR = [0.01]\n",
            "Train step - Step 80, Loss 4.289324760437012\n",
            "Train step - Step 90, Loss 4.433895111083984\n",
            "Train step - Step 100, Loss 4.598770618438721\n",
            "Train step - Step 110, Loss 4.422326564788818\n",
            "Train step - Step 120, Loss 4.330116271972656\n",
            "Train step - Step 130, Loss 4.126266002655029\n",
            "Train step - Step 140, Loss 4.166408538818359\n",
            "Train step - Step 150, Loss 4.168452739715576\n",
            "Train epoch - Accuracy: 0.0537 Loss: 4.222596956634521 Corrects: 537\n",
            "Starting epoch 3/10, LR = [0.01]\n",
            "Train step - Step 160, Loss 4.100866317749023\n",
            "Train step - Step 170, Loss 3.987889051437378\n",
            "Train step - Step 180, Loss 4.072512626647949\n",
            "Train step - Step 190, Loss 3.9954617023468018\n",
            "Train step - Step 200, Loss 4.028830528259277\n",
            "Train step - Step 210, Loss 4.077909469604492\n",
            "Train step - Step 220, Loss 3.8859176635742188\n",
            "Train step - Step 230, Loss 3.7743237018585205\n",
            "Train epoch - Accuracy: 0.0888 Loss: 3.993833604431152 Corrects: 888\n",
            "Starting epoch 4/10, LR = [0.01]\n",
            "Train step - Step 240, Loss 3.947824478149414\n",
            "Train step - Step 250, Loss 3.929934501647949\n",
            "Train step - Step 260, Loss 3.8337671756744385\n",
            "Train step - Step 270, Loss 3.8574013710021973\n",
            "Train step - Step 280, Loss 3.8535499572753906\n",
            "Train step - Step 290, Loss 3.696765184402466\n",
            "Train step - Step 300, Loss 3.6766748428344727\n",
            "Train step - Step 310, Loss 3.5327305793762207\n",
            "Train epoch - Accuracy: 0.1073 Loss: 3.844103350067139 Corrects: 1073\n",
            "Starting epoch 5/10, LR = [0.01]\n",
            "Train step - Step 320, Loss 3.8012285232543945\n",
            "Train step - Step 330, Loss 3.6173453330993652\n",
            "Train step - Step 340, Loss 3.7769408226013184\n",
            "Train step - Step 350, Loss 3.527475118637085\n",
            "Train step - Step 360, Loss 3.600555896759033\n",
            "Train step - Step 370, Loss 3.6381702423095703\n",
            "Train step - Step 380, Loss 3.511826992034912\n",
            "Train step - Step 390, Loss 3.6414096355438232\n",
            "Train epoch - Accuracy: 0.1284 Loss: 3.707388432312012 Corrects: 1284\n",
            "Starting epoch 6/10, LR = [0.01]\n",
            "Train step - Step 400, Loss 3.592768669128418\n",
            "Train step - Step 410, Loss 3.4666171073913574\n",
            "Train step - Step 420, Loss 3.526531934738159\n",
            "Train step - Step 430, Loss 3.5990116596221924\n",
            "Train step - Step 440, Loss 3.751523971557617\n",
            "Train step - Step 450, Loss 3.5882625579833984\n",
            "Train step - Step 460, Loss 3.464203357696533\n",
            "Train step - Step 470, Loss 3.4831621646881104\n",
            "Train epoch - Accuracy: 0.1431 Loss: 3.6007515342712404 Corrects: 1431\n",
            "Starting epoch 7/10, LR = [0.01]\n",
            "Train step - Step 480, Loss 3.5857930183410645\n",
            "Train step - Step 490, Loss 3.5888218879699707\n",
            "Train step - Step 500, Loss 3.294365406036377\n",
            "Train step - Step 510, Loss 3.523771286010742\n",
            "Train step - Step 520, Loss 3.648378849029541\n",
            "Train step - Step 530, Loss 3.608952283859253\n",
            "Train step - Step 540, Loss 3.461289882659912\n",
            "Train step - Step 550, Loss 3.338352680206299\n",
            "Train epoch - Accuracy: 0.1599 Loss: 3.500439820098877 Corrects: 1599\n",
            "Starting epoch 8/10, LR = [0.01]\n",
            "Train step - Step 560, Loss 3.2763421535491943\n",
            "Train step - Step 570, Loss 3.524879217147827\n",
            "Train step - Step 580, Loss 3.2336018085479736\n",
            "Train step - Step 590, Loss 3.380439281463623\n",
            "Train step - Step 600, Loss 3.3954434394836426\n",
            "Train step - Step 610, Loss 3.2312657833099365\n",
            "Train step - Step 620, Loss 3.2945539951324463\n",
            "Train step - Step 630, Loss 3.4654150009155273\n",
            "Train epoch - Accuracy: 0.1811 Loss: 3.38861277923584 Corrects: 1811\n",
            "Starting epoch 9/10, LR = [0.01]\n",
            "Train step - Step 640, Loss 3.357489585876465\n",
            "Train step - Step 650, Loss 3.209465742111206\n",
            "Train step - Step 660, Loss 3.1804327964782715\n",
            "Train step - Step 670, Loss 3.433178663253784\n",
            "Train step - Step 680, Loss 3.246856927871704\n",
            "Train step - Step 690, Loss 3.1736602783203125\n",
            "Train step - Step 700, Loss 3.3729119300842285\n",
            "Train step - Step 710, Loss 4.086258888244629\n",
            "Train epoch - Accuracy: 0.2076 Loss: 3.2802842086791992 Corrects: 2076\n",
            "Starting epoch 10/10, LR = [0.01]\n",
            "Train step - Step 720, Loss 3.215310573577881\n",
            "Train step - Step 730, Loss 3.269526481628418\n",
            "Train step - Step 740, Loss 3.2120959758758545\n",
            "Train step - Step 750, Loss 3.0998871326446533\n",
            "Train step - Step 760, Loss 3.189574718475342\n",
            "Train step - Step 770, Loss 3.1478707790374756\n",
            "Train step - Step 780, Loss 3.159602165222168\n",
            "Train epoch - Accuracy: 0.2212 Loss: 3.166721435546875 Corrects: 2212\n",
            "Training finished in 89.22502279281616 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pts78KY42gXj",
        "colab_type": "text"
      },
      "source": [
        "**Catastrophic Forgetting**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrA3WhUzuK67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Catastrophic Forgetting\n",
        "def sequentialLearning(train_subsets, val_subsets, test_subsets):\n",
        "    net, criterion, optimizer, scheduler = getResNet34(100)\n",
        "    groups_accuracies=[]\n",
        "\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      \n",
        "      # Train on current group\n",
        "      train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "      train(net, train_dataloader, criterion, optimizer, scheduler)\n",
        "\n",
        "      # Validate on current group\n",
        "      val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "      acc, loss = validate(net, val_loader, criterion)\n",
        "      print(\"EVALUATION: \",acc, loss)\n",
        "\n",
        "      # Testo on current group\n",
        "      test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "      acc = test(net, test_loader)\n",
        "      groups_accuracies.append(acc)\n",
        "      print(\"TEST: \",acc)\n",
        "\n",
        "    return net, groups_accuracies\n",
        "\n",
        "def printAccuracyDifference(net, old_accuracies):\n",
        "    dif_accuracies=[]\n",
        "    id_group=0\n",
        "    for test_subset in test_subsets:\n",
        "        test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        dif_accuracies.append((id_group+1,old_accuracies[id_group],acc))\n",
        "    return dif_accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkrMQy2TuUAb",
        "colab_type": "code",
        "outputId": "0a04d16e-5eac-4c31-ab06-372180fe9779",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net, old_accuracies=sequentialLearning(train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/10, LR = [0.05]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/Cifar100/resnet.py:105: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "  nn.init.kaiming_normal(m.weight.data, mode='fan_out')\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:351: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train step - Step 0, Loss 6.016814231872559\n",
            "Train step - Step 10, Loss 10.680671691894531\n",
            "Train step - Step 20, Loss 6.5873823165893555\n",
            "Train step - Step 30, Loss 5.181732177734375\n",
            "Train step - Step 40, Loss 4.748598575592041\n",
            "Train step - Step 50, Loss 4.820867538452148\n",
            "Train step - Step 60, Loss 5.413323879241943\n",
            "Train step - Step 70, Loss 5.334465503692627\n",
            "Train epoch - Accuracy: 0.0144 Loss: 6.006224116516114 Corrects: 144\n",
            "Starting epoch 2/10, LR = [0.05]\n",
            "Train step - Step 80, Loss 4.9884352684021\n",
            "Train step - Step 90, Loss 4.7125396728515625\n",
            "Train step - Step 100, Loss 5.093074321746826\n",
            "Train step - Step 110, Loss 5.133219242095947\n",
            "Train step - Step 120, Loss 5.078371047973633\n",
            "Train step - Step 130, Loss 4.828057765960693\n",
            "Train step - Step 140, Loss 5.841559886932373\n",
            "Train step - Step 150, Loss 5.066982269287109\n",
            "Train epoch - Accuracy: 0.0178 Loss: 4.875747262573242 Corrects: 178\n",
            "Starting epoch 3/10, LR = [0.05]\n",
            "Train step - Step 160, Loss 5.067509174346924\n",
            "Train step - Step 170, Loss 4.553382396697998\n",
            "Train step - Step 180, Loss 4.74177360534668\n",
            "Train step - Step 190, Loss 4.52184534072876\n",
            "Train step - Step 200, Loss 4.610500335693359\n",
            "Train step - Step 210, Loss 4.554184913635254\n",
            "Train step - Step 220, Loss 4.677628993988037\n",
            "Train step - Step 230, Loss 4.607661724090576\n",
            "Train epoch - Accuracy: 0.0257 Loss: 4.643712144470215 Corrects: 257\n",
            "Starting epoch 4/10, LR = [0.05]\n",
            "Train step - Step 240, Loss 4.5759358406066895\n",
            "Train step - Step 250, Loss 4.555006980895996\n",
            "Train step - Step 260, Loss 4.591240406036377\n",
            "Train step - Step 270, Loss 4.5087056159973145\n",
            "Train step - Step 280, Loss 4.426421165466309\n",
            "Train step - Step 290, Loss 4.4927496910095215\n",
            "Train step - Step 300, Loss 4.506320953369141\n",
            "Train step - Step 310, Loss 4.511621952056885\n",
            "Train epoch - Accuracy: 0.0235 Loss: 4.540928835296631 Corrects: 235\n",
            "Starting epoch 5/10, LR = [0.05]\n",
            "Train step - Step 320, Loss 4.6012163162231445\n",
            "Train step - Step 330, Loss 4.529836177825928\n",
            "Train step - Step 340, Loss 4.501110076904297\n",
            "Train step - Step 350, Loss 4.55519962310791\n",
            "Train step - Step 360, Loss 4.4428863525390625\n",
            "Train step - Step 370, Loss 4.472463130950928\n",
            "Train step - Step 380, Loss 4.419076919555664\n",
            "Train step - Step 390, Loss 4.463791847229004\n",
            "Train epoch - Accuracy: 0.0286 Loss: 4.482036613464356 Corrects: 286\n",
            "Starting epoch 6/10, LR = [0.05]\n",
            "Train step - Step 400, Loss 4.564053058624268\n",
            "Train step - Step 410, Loss 4.322634696960449\n",
            "Train step - Step 420, Loss 4.364289283752441\n",
            "Train step - Step 430, Loss 4.494974136352539\n",
            "Train step - Step 440, Loss 4.426093578338623\n",
            "Train step - Step 450, Loss 4.449727535247803\n",
            "Train step - Step 460, Loss 4.434741497039795\n",
            "Train step - Step 470, Loss 4.334654331207275\n",
            "Train epoch - Accuracy: 0.0355 Loss: 4.415051129150391 Corrects: 355\n",
            "Starting epoch 7/10, LR = [0.05]\n",
            "Train step - Step 480, Loss 4.285735607147217\n",
            "Train step - Step 490, Loss 4.237849712371826\n",
            "Train step - Step 500, Loss 4.342350006103516\n",
            "Train step - Step 510, Loss 4.162230491638184\n",
            "Train step - Step 520, Loss 4.113491058349609\n",
            "Train step - Step 530, Loss 4.105564117431641\n",
            "Train step - Step 540, Loss 4.210885047912598\n",
            "Train step - Step 550, Loss 4.410119533538818\n",
            "Train epoch - Accuracy: 0.0506 Loss: 4.212453338623047 Corrects: 506\n",
            "Starting epoch 8/10, LR = [0.05]\n",
            "Train step - Step 560, Loss 4.095580577850342\n",
            "Train step - Step 570, Loss 4.029468536376953\n",
            "Train step - Step 580, Loss 4.028396129608154\n",
            "Train step - Step 590, Loss 4.0639753341674805\n",
            "Train step - Step 600, Loss 4.110158920288086\n",
            "Train step - Step 610, Loss 4.018665313720703\n",
            "Train step - Step 620, Loss 3.943751335144043\n",
            "Train step - Step 630, Loss 4.227600574493408\n",
            "Train epoch - Accuracy: 0.0646 Loss: 4.08847977142334 Corrects: 646\n",
            "Starting epoch 9/10, LR = [0.05]\n",
            "Train step - Step 640, Loss 4.080387115478516\n",
            "Train step - Step 650, Loss 4.249252796173096\n",
            "Train step - Step 660, Loss 3.9362030029296875\n",
            "Train step - Step 670, Loss 4.088445663452148\n",
            "Train step - Step 680, Loss 3.7926177978515625\n",
            "Train step - Step 690, Loss 3.8709890842437744\n",
            "Train step - Step 700, Loss 4.047475814819336\n",
            "Train step - Step 710, Loss 4.593386173248291\n",
            "Train epoch - Accuracy: 0.0797 Loss: 4.006002802276611 Corrects: 797\n",
            "Starting epoch 10/10, LR = [0.05]\n",
            "Train step - Step 720, Loss 4.122995853424072\n",
            "Train step - Step 730, Loss 3.7936270236968994\n",
            "Train step - Step 740, Loss 3.775177001953125\n",
            "Train step - Step 750, Loss 4.021771430969238\n",
            "Train step - Step 760, Loss 3.867140531539917\n",
            "Train step - Step 770, Loss 3.942610740661621\n",
            "Train step - Step 780, Loss 3.9406633377075195\n",
            "Train epoch - Accuracy: 0.0914 Loss: 3.9030974609375 Corrects: 914\n",
            "Training finished in 90.31879687309265 seconds\n",
            "EVALUATION:  0.14 3.517378330230713\n",
            "TEST:  0.125\n",
            "Starting epoch 1/10, LR = [0.05]\n",
            "Train step - Step 0, Loss 3.8998563289642334\n",
            "Train step - Step 10, Loss 3.840193033218384\n",
            "Train step - Step 20, Loss 3.801384210586548\n",
            "Train step - Step 30, Loss 3.6685736179351807\n",
            "Train step - Step 40, Loss 3.9829788208007812\n",
            "Train step - Step 50, Loss 3.753476142883301\n",
            "Train step - Step 60, Loss 3.900362968444824\n",
            "Train step - Step 70, Loss 3.640021800994873\n",
            "Train epoch - Accuracy: 0.1016 Loss: 3.84074130859375 Corrects: 1016\n",
            "Starting epoch 2/10, LR = [0.05]\n",
            "Train step - Step 80, Loss 3.8277385234832764\n",
            "Train step - Step 90, Loss 3.732959508895874\n",
            "Train step - Step 100, Loss 3.684913396835327\n",
            "Train step - Step 110, Loss 3.8199267387390137\n",
            "Train step - Step 120, Loss 3.8730995655059814\n",
            "Train step - Step 130, Loss 3.791246175765991\n",
            "Train step - Step 140, Loss 3.520467519760132\n",
            "Train step - Step 150, Loss 3.893031120300293\n",
            "Train epoch - Accuracy: 0.1206 Loss: 3.7545307014465332 Corrects: 1206\n",
            "Starting epoch 3/10, LR = [0.05]\n",
            "Train step - Step 160, Loss 3.6413049697875977\n",
            "Train step - Step 170, Loss 3.6832082271575928\n",
            "Train step - Step 180, Loss 3.723155975341797\n",
            "Train step - Step 190, Loss 3.7638161182403564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-eaa9131768c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_accuracies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequentialLearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_subsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_subsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-950eb20a34ff>\u001b[0m in \u001b[0;36msequentialLearning\u001b[0;34m(train_subsets, val_subsets, test_subsets)\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m# Train on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m# Validate on current group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-a6f509fe5100>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_dataloader, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Zero-ing the gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Cifar100/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Cifar100/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                 \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}