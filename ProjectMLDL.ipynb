{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectMLDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/master/ProjectMLDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40FSXGxD2VD",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudv9Cj8E8OI",
        "colab_type": "code",
        "outputId": "6486f58d-cd36-489a-bd31-59dfd730fbb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip install --upgrade wandb\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\\n!pip install --upgrade wandb\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQQQe5khNyKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dC-rYdjD-E3",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet, resnet18, resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "#import wandb\n",
        "\n",
        "# Everything available at https://app.wandb.ai/danver/progetto-mldl\n",
        "#wandb.login('1eb973e575b3a7ecf03049dcb7e3ec62b5d6d96b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOn3bHMEBzX",
        "colab_type": "text"
      },
      "source": [
        "**Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmMFuQNV2ueu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "DATA_DIR = 'DATA/cifar-100-python' # here the dataset will be downloaded\n",
        "\n",
        "NUM_CLASSES = 100 \n",
        "\n",
        "# @toupdate the following vals (look at icarl paper)\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.02            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 2      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 49       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-WK3hEJCM",
        "colab_type": "text"
      },
      "source": [
        "**Retrieving dataset CIFAR1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n1do9ln3OVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c20f8978-b82e-4d88-d5ee-8c41084961d2"
      },
      "source": [
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/ #debug purposes\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 121, done.\u001b[K\n",
            "remote: Counting objects: 100% (121/121), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 121 (delta 48), reused 90 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (121/121), 3.44 MiB | 24.29 MiB/s, done.\n",
            "Resolving deltas: 100% (48/48), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "88faeb9e-c580-4b3a-ac3f-e41c845a2a4a"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA\n",
        "\n",
        "if not os.path.isdir('./{}'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mv 'cifar-100-python' $DATA_DIR\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-18 10:04:38--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  77.9MB/s    in 2.1s    \n",
            "\n",
            "2020-05-18 10:04:41 (77.9 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ5m4V-ERDh",
        "colab_type": "text"
      },
      "source": [
        "**Define data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD2_Re8kPxRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is ok to use also .5 mean and .5 std (faq1)\n",
        "# @tocheck\n",
        "# ref: https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/main.py + pytorch resnet documentation\n",
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.Resize(32), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.Resize(32),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hvvskAS2Ia",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQdJQqPkqVR",
        "colab_type": "code",
        "outputId": "2ce5bf8a-c60f-40ba-9f64-a4e79eac5236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(\"DATA\", split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(\"DATA\", split='test', transform=eval_transform)\n",
        "\n",
        "# @todo\n",
        "# split into train, test, \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=30)\n",
        "test_splits = test_dataset.split_classes(seed=30, dictionary_of='indices')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBUNjo6uFuTm",
        "colab_type": "text"
      },
      "source": [
        "**Build reverse index**  \n",
        "Builds the reverse index used to get outputs labels from shuffled classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4fISGA4FuTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def build_reverse_index():\n",
        "    reverse_index = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "    for k in train_splits.keys():\n",
        "        labels = list(train_dataset.df.loc[train_splits[k]['train'],'labels'].value_counts().index)\n",
        "        group = [k for i in range(len(labels))]\n",
        "        data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "        reverse_index = reverse_index.append(data, ignore_index=True)\n",
        "\n",
        "    return reverse_index\n",
        "\n",
        "def getLabels(reverse_index, outputs):\n",
        "    outs = outputs.cpu().numpy()\n",
        "    labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "    labels = torch.tensor(list(labels))\n",
        "    return labels.to(DEVICE)\n",
        "\n",
        "\n",
        "outputs_labels_mapping = build_reverse_index()\n",
        "outputs_labels_mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fov9YAFTlj",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    # train_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    # val_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for v in test_splits.values():\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    # test_dl = DataLoader(test_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    test_subsets.append(test_subs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YCjEDH9FuTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):     \n",
        "    # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    net.train()\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            preds = getLabels(outputs_labels_mapping, preds)\n",
        "            # print(preds)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        #wandb.log({'Epochs': epoch, 'Train Accuracy': epoch_acc, 'Train Loss': epoch_loss})\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion=None):\n",
        "    net.eval()\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for images, labels in val_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        preds = getLabels(outputs_labels_mapping, preds)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "    # Calculate Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss\n",
        "\n",
        "def test(net, test_dataloader):\n",
        "    acc, _ = validate(net, test_dataloader)\n",
        "    return acc\n",
        "\n",
        "# Joins 2+ subsets into a new Subset\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets):\n",
        "    #wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n",
        "\n",
        "    net, criterion, optimizer, scheduler = getNet()\n",
        "    #wandb.watch(net)\n",
        "\n",
        "    train_set = None\n",
        "    test_set = None\n",
        "    first_pass = True\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "\n",
        "        # Builds growing train and test set. The new sets include data from previous class groups and current class group\n",
        "        if train_set is None:\n",
        "            train_set = train_subset\n",
        "        else:\n",
        "            train_set = joinSubsets(train_dataset, [train_set, train_subset])\n",
        "        if test_set is None:\n",
        "            test_set = test_subset\n",
        "        else:\n",
        "            test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        # Adds new output nodes to the network for the new incoming classes\n",
        "        if first_pass:\n",
        "            first_pass = False\n",
        "        else:\n",
        "            addOutputs(net, 10)\n",
        "\n",
        "        # Trains model on previous and current class groups\n",
        "        _, _, optimizer, scheduler = getNet() # Resets optimizer & scheduler\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        train(net, train_loader, criterion, optimizer, scheduler)\n",
        "\n",
        "        # Validate model on current class group\n",
        "        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc, loss = validate(net, val_loader, criterion)\n",
        "        print(acc, loss)\n",
        "\n",
        "        # Test the model on previous and current class groups\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        print(acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0SQy6LlFuTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getResNet34(output_size):\n",
        "    net = resnet34(pretrained=False, progress=True)\n",
        "    net.fc = nn.Linear(net.fc.in_features, output_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputsToResNet(net, new_outputs):\n",
        "    in_features = net.fc.in_features\n",
        "    out_features = net.fc.out_features\n",
        "    weight = net.fc.weight.data\n",
        "\n",
        "    net.fc = nn.Linear(in_features, out_features + new_outputs)\n",
        "    net.fc.weight.data[:out_features] = weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tO-sPq3FuTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getNet():\n",
        "    return getResNet34(10)\n",
        "\n",
        "jointTraining(getNet, addOutputsToResNet, train_subsets, val_subsets, test_subsets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5nIXEPpLkqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}