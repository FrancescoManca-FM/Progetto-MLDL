{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40FSXGxD2VD",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudv9Cj8E8OI",
        "colab_type": "code",
        "outputId": "a41e3977-ab9e-4c1e-e1b4-aab740a87e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dC-rYdjD-E3",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOn3bHMEBzX",
        "colab_type": "text"
      },
      "source": [
        "**Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmMFuQNV2ueu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "NUM_CLASSES = 100 \n",
        "\n",
        "# @toupdate the following vals (look at icarl paper)\n",
        "\n",
        "BATCH_SIZE = 256     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 1e-3            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-WK3hEJCM",
        "colab_type": "text"
      },
      "source": [
        "**Retrieving dataset CIFAR1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n1do9ln3OVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "03914e36-a2d5-4e37-fa07-b65703a3a6ac"
      },
      "source": [
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/ #debug purposes\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb\n",
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/75)\u001b[K\rremote: Counting objects:   2% (2/75)\u001b[K\rremote: Counting objects:   4% (3/75)\u001b[K\rremote: Counting objects:   5% (4/75)\u001b[K\rremote: Counting objects:   6% (5/75)\u001b[K\rremote: Counting objects:   8% (6/75)\u001b[K\rremote: Counting objects:   9% (7/75)\u001b[K\rremote: Counting objects:  10% (8/75)\u001b[K\rremote: Counting objects:  12% (9/75)\u001b[K\rremote: Counting objects:  13% (10/75)\u001b[K\rremote: Counting objects:  14% (11/75)\u001b[K\rremote: Counting objects:  16% (12/75)\u001b[K\rremote: Counting objects:  17% (13/75)\u001b[K\rremote: Counting objects:  18% (14/75)\u001b[K\rremote: Counting objects:  20% (15/75)\u001b[K\rremote: Counting objects:  21% (16/75)\u001b[K\rremote: Counting objects:  22% (17/75)\u001b[K\rremote: Counting objects:  24% (18/75)\u001b[K\rremote: Counting objects:  25% (19/75)\u001b[K\rremote: Counting objects:  26% (20/75)\u001b[K\rremote: Counting objects:  28% (21/75)\u001b[K\rremote: Counting objects:  29% (22/75)\u001b[K\rremote: Counting objects:  30% (23/75)\u001b[K\rremote: Counting objects:  32% (24/75)\u001b[K\rremote: Counting objects:  33% (25/75)\u001b[K\rremote: Counting objects:  34% (26/75)\u001b[K\rremote: Counting objects:  36% (27/75)\u001b[K\rremote: Counting objects:  37% (28/75)\u001b[K\rremote: Counting objects:  38% (29/75)\u001b[K\rremote: Counting objects:  40% (30/75)\u001b[K\rremote: Counting objects:  41% (31/75)\u001b[K\rremote: Counting objects:  42% (32/75)\u001b[K\rremote: Counting objects:  44% (33/75)\u001b[K\rremote: Counting objects:  45% (34/75)\u001b[K\rremote: Counting objects:  46% (35/75)\u001b[K\rremote: Counting objects:  48% (36/75)\u001b[K\rremote: Counting objects:  49% (37/75)\u001b[K\rremote: Counting objects:  50% (38/75)\u001b[K\rremote: Counting objects:  52% (39/75)\u001b[K\rremote: Counting objects:  53% (40/75)\u001b[K\rremote: Counting objects:  54% (41/75)\u001b[K\rremote: Counting objects:  56% (42/75)\u001b[K\rremote: Counting objects:  57% (43/75)\u001b[K\rremote: Counting objects:  58% (44/75)\u001b[K\rremote: Counting objects:  60% (45/75)\u001b[K\rremote: Counting objects:  61% (46/75)\u001b[K\rremote: Counting objects:  62% (47/75)\u001b[K\rremote: Counting objects:  64% (48/75)\u001b[K\rremote: Counting objects:  65% (49/75)\u001b[K\rremote: Counting objects:  66% (50/75)\u001b[K\rremote: Counting objects:  68% (51/75)\u001b[K\rremote: Counting objects:  69% (52/75)\u001b[K\rremote: Counting objects:  70% (53/75)\u001b[K\rremote: Counting objects:  72% (54/75)\u001b[K\rremote: Counting objects:  73% (55/75)\u001b[K\rremote: Counting objects:  74% (56/75)\u001b[K\rremote: Counting objects:  76% (57/75)\u001b[K\rremote: Counting objects:  77% (58/75)\u001b[K\rremote: Counting objects:  78% (59/75)\u001b[K\rremote: Counting objects:  80% (60/75)\u001b[K\rremote: Counting objects:  81% (61/75)\u001b[K\rremote: Counting objects:  82% (62/75)\u001b[K\rremote: Counting objects:  84% (63/75)\u001b[K\rremote: Counting objects:  85% (64/75)\u001b[K\rremote: Counting objects:  86% (65/75)\u001b[K\rremote: Counting objects:  88% (66/75)\u001b[K\rremote: Counting objects:  89% (67/75)\u001b[K\rremote: Counting objects:  90% (68/75)\u001b[K\rremote: Counting objects:  92% (69/75)\u001b[K\rremote: Counting objects:  93% (70/75)\u001b[K\rremote: Counting objects:  94% (71/75)\u001b[K\rremote: Counting objects:  96% (72/75)\u001b[K\rremote: Counting objects:  97% (73/75)\u001b[K\rremote: Counting objects:  98% (74/75)\u001b[K\rremote: Counting objects: 100% (75/75)\u001b[K\rremote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/51)\u001b[K\rremote: Compressing objects:   3% (2/51)\u001b[K\rremote: Compressing objects:   5% (3/51)\u001b[K\rremote: Compressing objects:   7% (4/51)\u001b[K\rremote: Compressing objects:   9% (5/51)\u001b[K\rremote: Compressing objects:  11% (6/51)\u001b[K\rremote: Compressing objects:  13% (7/51)\u001b[K\rremote: Compressing objects:  15% (8/51)\u001b[K\rremote: Compressing objects:  17% (9/51)\u001b[K\rremote: Compressing objects:  19% (10/51)\u001b[K\rremote: Compressing objects:  21% (11/51)\u001b[K\rremote: Compressing objects:  23% (12/51)\u001b[K\rremote: Compressing objects:  25% (13/51)\u001b[K\rremote: Compressing objects:  27% (14/51)\u001b[K\rremote: Compressing objects:  29% (15/51)\u001b[K\rremote: Compressing objects:  31% (16/51)\u001b[K\rremote: Compressing objects:  33% (17/51)\u001b[K\rremote: Compressing objects:  35% (18/51)\u001b[K\rremote: Compressing objects:  37% (19/51)\u001b[K\rremote: Compressing objects:  39% (20/51)\u001b[K\rremote: Compressing objects:  41% (21/51)\u001b[K\rremote: Compressing objects:  43% (22/51)\u001b[K\rremote: Compressing objects:  45% (23/51)\u001b[K\rremote: Compressing objects:  47% (24/51)\u001b[K\rremote: Compressing objects:  49% (25/51)\u001b[K\rremote: Compressing objects:  50% (26/51)\u001b[K\rremote: Compressing objects:  52% (27/51)\u001b[K\rremote: Compressing objects:  54% (28/51)\u001b[K\rremote: Compressing objects:  56% (29/51)\u001b[K\rremote: Compressing objects:  58% (30/51)\u001b[K\rremote: Compressing objects:  60% (31/51)\u001b[K\rremote: Compressing objects:  62% (32/51)\u001b[K\rremote: Compressing objects:  64% (33/51)\u001b[K\rremote: Compressing objects:  66% (34/51)\u001b[K\rremote: Compressing objects:  68% (35/51)\u001b[K\rremote: Compressing objects:  70% (36/51)\u001b[K\rremote: Compressing objects:  72% (37/51)\u001b[K\rremote: Compressing objects:  74% (38/51)\u001b[K\rremote: Compressing objects:  76% (39/51)\u001b[K\rremote: Compressing objects:  78% (40/51)\u001b[K\rremote: Compressing objects:  80% (41/51)\u001b[K\rremote: Compressing objects:  82% (42/51)\u001b[K\rremote: Compressing objects:  84% (43/51)\u001b[K\rremote: Compressing objects:  86% (44/51)\u001b[K\rremote: Compressing objects:  88% (45/51)\u001b[K\rremote: Compressing objects:  90% (46/51)\u001b[K\rremote: Compressing objects:  92% (47/51)\u001b[K\rremote: Compressing objects:  94% (48/51)\u001b[K\rremote: Compressing objects:  96% (49/51)\u001b[K\rremote: Compressing objects:  98% (50/51)\u001b[K\rremote: Compressing objects: 100% (51/51)\u001b[K\rremote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "Unpacking objects:   1% (1/75)   \rUnpacking objects:   2% (2/75)   \rUnpacking objects:   4% (3/75)   \rUnpacking objects:   5% (4/75)   \rUnpacking objects:   6% (5/75)   \rUnpacking objects:   8% (6/75)   \rUnpacking objects:   9% (7/75)   \rUnpacking objects:  10% (8/75)   \rUnpacking objects:  12% (9/75)   \rUnpacking objects:  13% (10/75)   \rUnpacking objects:  14% (11/75)   \rUnpacking objects:  16% (12/75)   \rUnpacking objects:  17% (13/75)   \rUnpacking objects:  18% (14/75)   \rUnpacking objects:  20% (15/75)   \rUnpacking objects:  21% (16/75)   \rUnpacking objects:  22% (17/75)   \rUnpacking objects:  24% (18/75)   \rUnpacking objects:  25% (19/75)   \rUnpacking objects:  26% (20/75)   \rUnpacking objects:  28% (21/75)   \rUnpacking objects:  29% (22/75)   \rUnpacking objects:  30% (23/75)   \rUnpacking objects:  32% (24/75)   \rUnpacking objects:  33% (25/75)   \rUnpacking objects:  34% (26/75)   \rUnpacking objects:  36% (27/75)   \rUnpacking objects:  37% (28/75)   \rUnpacking objects:  38% (29/75)   \rUnpacking objects:  40% (30/75)   \rUnpacking objects:  41% (31/75)   \rUnpacking objects:  42% (32/75)   \rUnpacking objects:  44% (33/75)   \rUnpacking objects:  45% (34/75)   \rUnpacking objects:  46% (35/75)   \rUnpacking objects:  48% (36/75)   \rUnpacking objects:  49% (37/75)   \rUnpacking objects:  50% (38/75)   \rUnpacking objects:  52% (39/75)   \rUnpacking objects:  53% (40/75)   \rUnpacking objects:  54% (41/75)   \rUnpacking objects:  56% (42/75)   \rUnpacking objects:  57% (43/75)   \rUnpacking objects:  58% (44/75)   \rremote: Total 75 (delta 22), reused 59 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects:  60% (45/75)   \rUnpacking objects:  61% (46/75)   \rUnpacking objects:  62% (47/75)   \rUnpacking objects:  64% (48/75)   \rUnpacking objects:  65% (49/75)   \rUnpacking objects:  66% (50/75)   \rUnpacking objects:  68% (51/75)   \rUnpacking objects:  69% (52/75)   \rUnpacking objects:  70% (53/75)   \rUnpacking objects:  72% (54/75)   \rUnpacking objects:  73% (55/75)   \rUnpacking objects:  74% (56/75)   \rUnpacking objects:  76% (57/75)   \rUnpacking objects:  77% (58/75)   \rUnpacking objects:  78% (59/75)   \rUnpacking objects:  80% (60/75)   \rUnpacking objects:  81% (61/75)   \rUnpacking objects:  82% (62/75)   \rUnpacking objects:  84% (63/75)   \rUnpacking objects:  85% (64/75)   \rUnpacking objects:  86% (65/75)   \rUnpacking objects:  88% (66/75)   \rUnpacking objects:  89% (67/75)   \rUnpacking objects:  90% (68/75)   \rUnpacking objects:  92% (69/75)   \rUnpacking objects:  93% (70/75)   \rUnpacking objects:  94% (71/75)   \rUnpacking objects:  96% (72/75)   \rUnpacking objects:  97% (73/75)   \rUnpacking objects:  98% (74/75)   \rUnpacking objects: 100% (75/75)   \rUnpacking objects: 100% (75/75), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab_type": "code",
        "outputId": "a24c60ec-f301-423b-8466-fe7f9cdeb25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Download dataset from the official sourse and save it into DATA\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xf 'cifar-100-python.tar.gz'  \n",
        "!mv 'cifar-100-python' $DATA_DIR\n",
        "!rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-15 14:48:02--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  49.3MB/s    in 3.7s    \n",
            "\n",
            "2020-05-15 14:48:06 (44.0 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ5m4V-ERDh",
        "colab_type": "text"
      },
      "source": [
        "**Define data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD2_Re8kPxRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is ok to use also .5 mean and .5 std (faq1)\n",
        "# @tocheck\n",
        "# ref: https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/main.py + pytorch resnet documentation\n",
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.Resize(32), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.Resize(32),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hvvskAS2Ia",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQdJQqPkqVR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "152e0fae-b9d5-4609-9fed-92a5267cbd57"
      },
      "source": [
        "from random import shuffle\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# @todo\n",
        "# split into train, test, \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "## debug: both sets have 100 classes\n",
        "#print(len(train_dataset.getTargets()))\n",
        "#print(len(test_dataset.getTargets()))\n",
        "##\n",
        "\n",
        "# output a dictionary containing\n",
        "# group_i: 10 unordered classes\n",
        "# 10 groups = 100 classes\n",
        "def createClassesGroups(all_classes):\n",
        "  dictionary = {}\n",
        "  group = 1\n",
        "  shuffle(all_classes)\n",
        "  j = 0\n",
        "  for i in range(1,11):\n",
        "    subgroup = all_classes[j:i*10]\n",
        "    dictionary[i] = subgroup\n",
        "    j = i*10\n",
        "  return dictionary\n",
        "\n",
        "# create 10 groups of 10 classes each\n",
        "all_classes = train_dataset.getTargets()\n",
        "dictionaryClassesGroups = createClassesGroups(list(all_classes))\n",
        "#print(dictionaryClassesGroups) # debug\n",
        "\n",
        "# TEST\n",
        "# given a group, load a small training set containing the corresponding classes\n",
        "# ex. debug purposes => group1\n",
        "classes1 = dictionaryClassesGroups[1]\n",
        "idx = train_dataset.get_indices(classes1)\n",
        "class1_imgs = Subset(train_dataset, idx)\n",
        "print(len(class1_imgs))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splits = train_dataset.split_in_train_val_groups(ratio=0.5, seed=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fov9YAFTlj",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_dataloaders = []\n",
        "val_dataloaders = []\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "for v in splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "\n",
        "    train_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    val_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "\n",
        "    train_dataloaders.append(train_dl)\n",
        "    val_dataloaders.append(val_dl)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "ProjectMLDL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}