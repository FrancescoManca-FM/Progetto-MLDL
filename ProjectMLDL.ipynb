{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j40FSXGxD2VD",
        "colab_type": "text"
      },
      "source": [
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uudv9Cj8E8OI",
        "colab_type": "code",
        "outputId": "efccba02-3a63-46e6-dd37-00c145ce222d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "!pip install --upgrade wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dC-rYdjD-E3",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ6tCA_s2rru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet, resnet18, resnet34\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import wandb\n",
        "\n",
        "# Everything available at https://app.wandb.ai/danver/progetto-mldl\n",
        "wandb.login('1eb973e575b3a7ecf03049dcb7e3ec62b5d6d96b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XOn3bHMEBzX",
        "colab_type": "text"
      },
      "source": [
        "**Set arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmMFuQNV2ueu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "NUM_CLASSES = 100 \n",
        "\n",
        "# @toupdate the following vals (look at icarl paper)\n",
        "\n",
        "BATCH_SIZE = 128     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = 0.02            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 49       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lf-WK3hEJCM",
        "colab_type": "text"
      },
      "source": [
        "**Retrieving dataset CIFAR1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n1do9ln3OVE",
        "colab_type": "code",
        "outputId": "a5b732c8-391a-4210-ea30-ca8c21b6475c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Clone github repository with dataset handler\n",
        "# !rm -r Cifar100/ #debug purposes\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 85, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/85)\u001b[K\rremote: Counting objects:   2% (2/85)\u001b[K\rremote: Counting objects:   3% (3/85)\u001b[K\rremote: Counting objects:   4% (4/85)\u001b[K\rremote: Counting objects:   5% (5/85)\u001b[K\rremote: Counting objects:   7% (6/85)\u001b[K\rremote: Counting objects:   8% (7/85)\u001b[K\rremote: Counting objects:   9% (8/85)\u001b[K\rremote: Counting objects:  10% (9/85)\u001b[K\rremote: Counting objects:  11% (10/85)\u001b[K\rremote: Counting objects:  12% (11/85)\u001b[K\rremote: Counting objects:  14% (12/85)\u001b[K\rremote: Counting objects:  15% (13/85)\u001b[K\rremote: Counting objects:  16% (14/85)\u001b[K\rremote: Counting objects:  17% (15/85)\u001b[K\rremote: Counting objects:  18% (16/85)\u001b[K\rremote: Counting objects:  20% (17/85)\u001b[K\rremote: Counting objects:  21% (18/85)\u001b[K\rremote: Counting objects:  22% (19/85)\u001b[K\rremote: Counting objects:  23% (20/85)\u001b[K\rremote: Counting objects:  24% (21/85)\u001b[K\rremote: Counting objects:  25% (22/85)\u001b[K\rremote: Counting objects:  27% (23/85)\u001b[K\rremote: Counting objects:  28% (24/85)\u001b[K\rremote: Counting objects:  29% (25/85)\u001b[K\rremote: Counting objects:  30% (26/85)\u001b[K\rremote: Counting objects:  31% (27/85)\u001b[K\rremote: Counting objects:  32% (28/85)\u001b[K\rremote: Counting objects:  34% (29/85)\u001b[K\rremote: Counting objects:  35% (30/85)\u001b[K\rremote: Counting objects:  36% (31/85)\u001b[K\rremote: Counting objects:  37% (32/85)\u001b[K\rremote: Counting objects:  38% (33/85)\u001b[K\rremote: Counting objects:  40% (34/85)\u001b[K\rremote: Counting objects:  41% (35/85)\u001b[K\rremote: Counting objects:  42% (36/85)\u001b[K\rremote: Counting objects:  43% (37/85)\u001b[K\rremote: Counting objects:  44% (38/85)\u001b[K\rremote: Counting objects:  45% (39/85)\u001b[K\rremote: Counting objects:  47% (40/85)\u001b[K\rremote: Counting objects:  48% (41/85)\u001b[K\rremote: Counting objects:  49% (42/85)\u001b[K\rremote: Counting objects:  50% (43/85)\u001b[K\rremote: Counting objects:  51% (44/85)\u001b[K\rremote: Counting objects:  52% (45/85)\u001b[K\rremote: Counting objects:  54% (46/85)\u001b[K\rremote: Counting objects:  55% (47/85)\u001b[K\rremote: Counting objects:  56% (48/85)\u001b[K\rremote: Counting objects:  57% (49/85)\u001b[K\rremote: Counting objects:  58% (50/85)\u001b[K\rremote: Counting objects:  60% (51/85)\u001b[K\rremote: Counting objects:  61% (52/85)\u001b[K\rremote: Counting objects:  62% (53/85)\u001b[K\rremote: Counting objects:  63% (54/85)\u001b[K\rremote: Counting objects:  64% (55/85)\u001b[K\rremote: Counting objects:  65% (56/85)\u001b[K\rremote: Counting objects:  67% (57/85)\u001b[K\rremote: Counting objects:  68% (58/85)\u001b[K\rremote: Counting objects:  69% (59/85)\u001b[K\rremote: Counting objects:  70% (60/85)\u001b[K\rremote: Counting objects:  71% (61/85)\u001b[K\rremote: Counting objects:  72% (62/85)\u001b[K\rremote: Counting objects:  74% (63/85)\u001b[K\rremote: Counting objects:  75% (64/85)\u001b[K\rremote: Counting objects:  76% (65/85)\u001b[K\rremote: Counting objects:  77% (66/85)\u001b[K\rremote: Counting objects:  78% (67/85)\u001b[K\rremote: Counting objects:  80% (68/85)\u001b[K\rremote: Counting objects:  81% (69/85)\u001b[K\rremote: Counting objects:  82% (70/85)\u001b[K\rremote: Counting objects:  83% (71/85)\u001b[K\rremote: Counting objects:  84% (72/85)\u001b[K\rremote: Counting objects:  85% (73/85)\u001b[K\rremote: Counting objects:  87% (74/85)\u001b[K\rremote: Counting objects:  88% (75/85)\u001b[K\rremote: Counting objects:  89% (76/85)\u001b[K\rremote: Counting objects:  90% (77/85)\u001b[K\rremote: Counting objects:  91% (78/85)\u001b[K\rremote: Counting objects:  92% (79/85)\u001b[K\rremote: Counting objects:  94% (80/85)\u001b[K\rremote: Counting objects:  95% (81/85)\u001b[K\rremote: Counting objects:  96% (82/85)\u001b[K\rremote: Counting objects:  97% (83/85)\u001b[K\rremote: Counting objects:  98% (84/85)\u001b[K\rremote: Counting objects: 100% (85/85)\u001b[K\rremote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects:   1% (1/57)\u001b[K\rremote: Compressing objects:   3% (2/57)\u001b[K\rremote: Compressing objects:   5% (3/57)\u001b[K\rremote: Compressing objects:   7% (4/57)\u001b[K\rremote: Compressing objects:   8% (5/57)\u001b[K\rremote: Compressing objects:  10% (6/57)\u001b[K\rremote: Compressing objects:  12% (7/57)\u001b[K\rremote: Compressing objects:  14% (8/57)\u001b[K\rremote: Compressing objects:  15% (9/57)\u001b[K\rremote: Compressing objects:  17% (10/57)\u001b[K\rremote: Compressing objects:  19% (11/57)\u001b[K\rremote: Compressing objects:  21% (12/57)\u001b[K\rremote: Compressing objects:  22% (13/57)\u001b[K\rremote: Compressing objects:  24% (14/57)\u001b[K\rremote: Compressing objects:  26% (15/57)\u001b[K\rremote: Compressing objects:  28% (16/57)\u001b[K\rremote: Compressing objects:  29% (17/57)\u001b[K\rremote: Compressing objects:  31% (18/57)\u001b[K\rremote: Compressing objects:  33% (19/57)\u001b[K\rremote: Compressing objects:  35% (20/57)\u001b[K\rremote: Compressing objects:  36% (21/57)\u001b[K\rremote: Compressing objects:  38% (22/57)\u001b[K\rremote: Compressing objects:  40% (23/57)\u001b[K\rremote: Compressing objects:  42% (24/57)\u001b[K\rremote: Compressing objects:  43% (25/57)\u001b[K\rremote: Compressing objects:  45% (26/57)\u001b[K\rremote: Compressing objects:  47% (27/57)\u001b[K\rremote: Compressing objects:  49% (28/57)\u001b[K\rremote: Compressing objects:  50% (29/57)\u001b[K\rremote: Compressing objects:  52% (30/57)\u001b[K\rremote: Compressing objects:  54% (31/57)\u001b[K\rremote: Compressing objects:  56% (32/57)\u001b[K\rremote: Compressing objects:  57% (33/57)\u001b[K\rremote: Compressing objects:  59% (34/57)\u001b[K\rremote: Compressing objects:  61% (35/57)\u001b[K\rremote: Compressing objects:  63% (36/57)\u001b[K\rremote: Compressing objects:  64% (37/57)\u001b[K\rremote: Compressing objects:  66% (38/57)\u001b[K\rremote: Compressing objects:  68% (39/57)\u001b[K\rremote: Compressing objects:  70% (40/57)\u001b[K\rremote: Compressing objects:  71% (41/57)\u001b[K\rremote: Compressing objects:  73% (42/57)\u001b[K\rremote: Compressing objects:  75% (43/57)\u001b[K\rremote: Compressing objects:  77% (44/57)\u001b[K\rremote: Compressing objects:  78% (45/57)\u001b[K\rremote: Compressing objects:  80% (46/57)\u001b[K\rremote: Compressing objects:  82% (47/57)\u001b[K\rremote: Compressing objects:  84% (48/57)\u001b[K\rremote: Compressing objects:  85% (49/57)\u001b[K\rremote: Compressing objects:  87% (50/57)\u001b[K\rremote: Compressing objects:  89% (51/57)\u001b[K\rremote: Compressing objects:  91% (52/57)\u001b[K\rremote: Compressing objects:  92% (53/57)\u001b[K\rremote: Compressing objects:  94% (54/57)\u001b[K\rremote: Compressing objects:  96% (55/57)\u001b[K\rremote: Compressing objects:  98% (56/57)\u001b[K\rremote: Compressing objects: 100% (57/57)\u001b[K\rremote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "Unpacking objects:   1% (1/85)   \rUnpacking objects:   2% (2/85)   \rUnpacking objects:   3% (3/85)   \rUnpacking objects:   4% (4/85)   \rUnpacking objects:   5% (5/85)   \rUnpacking objects:   7% (6/85)   \rUnpacking objects:   8% (7/85)   \rUnpacking objects:   9% (8/85)   \rUnpacking objects:  10% (9/85)   \rUnpacking objects:  11% (10/85)   \rUnpacking objects:  12% (11/85)   \rUnpacking objects:  14% (12/85)   \rUnpacking objects:  15% (13/85)   \rUnpacking objects:  16% (14/85)   \rUnpacking objects:  17% (15/85)   \rUnpacking objects:  18% (16/85)   \rUnpacking objects:  20% (17/85)   \rUnpacking objects:  21% (18/85)   \rUnpacking objects:  22% (19/85)   \rUnpacking objects:  23% (20/85)   \rUnpacking objects:  24% (21/85)   \rUnpacking objects:  25% (22/85)   \rUnpacking objects:  27% (23/85)   \rUnpacking objects:  28% (24/85)   \rUnpacking objects:  29% (25/85)   \rUnpacking objects:  30% (26/85)   \rUnpacking objects:  31% (27/85)   \rUnpacking objects:  32% (28/85)   \rUnpacking objects:  34% (29/85)   \rUnpacking objects:  35% (30/85)   \rUnpacking objects:  36% (31/85)   \rUnpacking objects:  37% (32/85)   \rUnpacking objects:  38% (33/85)   \rUnpacking objects:  40% (34/85)   \rUnpacking objects:  41% (35/85)   \rUnpacking objects:  42% (36/85)   \rUnpacking objects:  43% (37/85)   \rUnpacking objects:  44% (38/85)   \rUnpacking objects:  45% (39/85)   \rUnpacking objects:  47% (40/85)   \rUnpacking objects:  48% (41/85)   \rUnpacking objects:  49% (42/85)   \rUnpacking objects:  50% (43/85)   \rUnpacking objects:  51% (44/85)   \rUnpacking objects:  52% (45/85)   \rUnpacking objects:  54% (46/85)   \rUnpacking objects:  55% (47/85)   \rUnpacking objects:  56% (48/85)   \rUnpacking objects:  57% (49/85)   \rremote: Total 85 (delta 27), reused 68 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects:  58% (50/85)   \rUnpacking objects:  60% (51/85)   \rUnpacking objects:  61% (52/85)   \rUnpacking objects:  62% (53/85)   \rUnpacking objects:  63% (54/85)   \rUnpacking objects:  64% (55/85)   \rUnpacking objects:  65% (56/85)   \rUnpacking objects:  67% (57/85)   \rUnpacking objects:  68% (58/85)   \rUnpacking objects:  69% (59/85)   \rUnpacking objects:  70% (60/85)   \rUnpacking objects:  71% (61/85)   \rUnpacking objects:  72% (62/85)   \rUnpacking objects:  74% (63/85)   \rUnpacking objects:  75% (64/85)   \rUnpacking objects:  76% (65/85)   \rUnpacking objects:  77% (66/85)   \rUnpacking objects:  78% (67/85)   \rUnpacking objects:  80% (68/85)   \rUnpacking objects:  81% (69/85)   \rUnpacking objects:  82% (70/85)   \rUnpacking objects:  83% (71/85)   \rUnpacking objects:  84% (72/85)   \rUnpacking objects:  85% (73/85)   \rUnpacking objects:  87% (74/85)   \rUnpacking objects:  88% (75/85)   \rUnpacking objects:  89% (76/85)   \rUnpacking objects:  90% (77/85)   \rUnpacking objects:  91% (78/85)   \rUnpacking objects:  92% (79/85)   \rUnpacking objects:  94% (80/85)   \rUnpacking objects:  95% (81/85)   \rUnpacking objects:  96% (82/85)   \rUnpacking objects:  97% (83/85)   \rUnpacking objects:  98% (84/85)   \rUnpacking objects: 100% (85/85)   \rUnpacking objects: 100% (85/85), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysghtAWOPYZD",
        "colab_type": "code",
        "outputId": "1b21b7ad-c0d0-47b1-c1eb-4939dbf35539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Download dataset from the official sourse and save it into DATA\n",
        "\n",
        "if not os.path.isdir('./{}'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mv 'cifar-100-python' $DATA_DIR\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-15 21:12:16--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  92.1MB/s    in 1.7s    \n",
            "\n",
            "2020-05-15 21:12:18 (92.1 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oJ5m4V-ERDh",
        "colab_type": "text"
      },
      "source": [
        "**Define data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD2_Re8kPxRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# it is ok to use also .5 mean and .5 std (faq1)\n",
        "# @tocheck\n",
        "# ref: https://github.com/chengyangfu/pytorch-vgg-cifar10/blob/master/main.py + pytorch resnet documentation\n",
        "# Define transformations for training\n",
        "train_transform = transforms.Compose([transforms.Resize(32), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Define transformations for evaluation\n",
        "eval_transform = transforms.Compose([transforms.Resize(32),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                                   \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0hvvskAS2Ia",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQdJQqPkqVR",
        "colab_type": "code",
        "outputId": "e381aaf8-2189-4f04-e153-232f01a9cfe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# @todo\n",
        "# split into train, test, \n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=30)\n",
        "test_splits = test_dataset.split_classes(seed=30, dictionary_of='indices')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n",
            "{1: [42, 41, 91, 9, 65, 50, 1, 70, 15, 78], 2: [73, 10, 55, 56, 72, 45, 48, 92, 76, 37], 3: [30, 21, 32, 96, 80, 49, 83, 26, 87, 33], 4: [8, 47, 59, 63, 74, 44, 98, 52, 85, 12], 5: [36, 23, 39, 40, 18, 66, 61, 60, 7, 34], 6: [99, 46, 2, 51, 16, 38, 58, 68, 22, 62], 7: [24, 5, 6, 67, 82, 19, 79, 43, 90, 20], 8: [0, 95, 57, 93, 53, 89, 25, 71, 84, 77], 9: [64, 29, 27, 88, 97, 4, 54, 75, 11, 69], 10: [86, 13, 17, 28, 31, 35, 94, 3, 14, 81]}\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Build reverse index**  \n",
        "Builds the reverse index used to get outputs labels from shuffled classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def build_reverse_index():\n",
        "    reverse_index = pd.DataFrame(columns=['group', 'labels'])\n",
        "\n",
        "    for k in train_splits.keys():\n",
        "        labels = list(train_dataset.df.loc[train_splits[k]['train'],'labels'].value_counts().index)\n",
        "        group = [k for i in range(len(labels))]\n",
        "        data = pd.DataFrame(np.array([group, labels]).T, columns=['group', 'labels'])\n",
        "        reverse_index = reverse_index.append(data, ignore_index=True)\n",
        "\n",
        "    return reverse_index\n",
        "\n",
        "def getLabels(reverse_index, outputs):\n",
        "    outs = outputs.cpu().numpy()\n",
        "    labels = reverse_index.loc[outs, 'labels']\n",
        "\n",
        "    labels = torch.tensor(list(labels))\n",
        "    return labels.to(DEVICE)\n",
        "\n",
        "\n",
        "outputs_labels_mapping = build_reverse_index()\n",
        "outputs_labels_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7fov9YAFTlj",
        "colab_type": "text"
      },
      "source": [
        "**Prepare dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MSItI0QVpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    # train_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "    # val_dl = DataLoader(train_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for v in test_splits.values():\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    # test_dl = DataLoader(test_subs, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=False)\n",
        "    test_subsets.append(test_subs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train(net, train_dataloader, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS):     \n",
        "    # By default, everything is loaded to cpu\n",
        "    net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    \n",
        "    net.train()\n",
        "    current_step = 0\n",
        "    # Start iterating over the epochs\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "        running_corrects = 0\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_dataloader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Get predictions\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            preds = getLabels(outputs_labels_mapping, preds)\n",
        "            # print(preds)\n",
        "            \n",
        "            # Update Corrects & Loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "            # Log loss\n",
        "            if current_step % LOG_FREQUENCY == 0:\n",
        "                print('Train step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            loss.backward()  # backward pass: computes gradients\n",
        "            optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "            current_step += 1\n",
        "        \n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate Accuracy & Loss\n",
        "        epoch_loss = running_loss / float(len(train_dataloader.dataset))\n",
        "        epoch_acc = running_corrects / float(len(train_dataloader.dataset))\n",
        "        \n",
        "        wandb.log({'Epochs': epoch, 'Train Accuracy': epoch_acc, 'Train Loss': epoch_loss})\n",
        "        print('Train epoch - Accuracy: {} Loss: {} Corrects: {}'.format(epoch_acc, epoch_loss, running_corrects))\n",
        "    print('Training finished in {} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def validate(net, val_dataloader, criterion=None):\n",
        "    net.eval()\n",
        "\n",
        "    running_corrects = 0\n",
        "    running_loss = 0.0\n",
        "    for images, labels in val_dataloader:\n",
        "        # Bring data over the device of choice\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass to the network\n",
        "        outputs = net(images)\n",
        "        \n",
        "        # Update Corrects & Loss\n",
        "        if criterion is not None:\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        preds = getLabels(outputs_labels_mapping, preds)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "    # Calculate Accuracy & Loss\n",
        "    loss = running_loss / float(len(val_dataloader.dataset))\n",
        "    acc = running_corrects / float(len(val_dataloader.dataset))\n",
        "\n",
        "    return acc, loss\n",
        "\n",
        "def test(net, test_dataloader):\n",
        "    acc, _ = validate(net, test_dataloader)\n",
        "    return acc\n",
        "\n",
        "# Joins 2+ subsets into a new Subset\n",
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)\n",
        "\n",
        "def jointTraining(getNet, addOutputs, train_subsets, val_subsets, test_subsets):\n",
        "    wandb.init(project=\"progetto-mldl\", name='joint-training', anonymous='never')\n",
        "\n",
        "    net, criterion, optimizer, scheduler = getNet()\n",
        "    wandb.watch(net)\n",
        "\n",
        "    train_set = None\n",
        "    test_set = None\n",
        "    first_pass = True\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "\n",
        "        # Builds growing train and test set. The new sets include data from previous class groups and current class group\n",
        "        if train_set is None:\n",
        "            train_set = train_subset\n",
        "        else:\n",
        "            train_set = joinSubsets(train_dataset, [train_set, train_subset])\n",
        "        if test_set is None:\n",
        "            test_set = test_subset\n",
        "        else:\n",
        "            test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        # Adds new output nodes to the network for the new incoming classes\n",
        "        if first_pass:\n",
        "            first_pass = False\n",
        "        else:\n",
        "            addOutputs(net, 10)\n",
        "\n",
        "        # Trains model on previous and current class groups\n",
        "        _, _, optimizer, scheduler = getNet() # Resets optimizer & scheduler\n",
        "        train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        train(net, train_loader, criterion, optimizer, scheduler)\n",
        "\n",
        "        # Validate model on current class group\n",
        "        val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc, loss = validate(net, val_loader, criterion)\n",
        "        print(acc, loss)\n",
        "\n",
        "        # Test the model on previous and current class groups\n",
        "        test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
        "        acc = test(net, test_loader)\n",
        "        print(acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getResNet34(output_size):\n",
        "    net = resnet34(pretrained=True, progress=True)\n",
        "    net.fc = nn.Linear(net.fc.in_features, output_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    parameters_to_optimize = net.parameters()\n",
        "    optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "    return net, criterion, optimizer, scheduler\n",
        "\n",
        "def addOutputsToResNet(net, new_outputs):\n",
        "    in_features = net.fc.in_features\n",
        "    out_features = net.fc.out_features\n",
        "    weight = net.fc.weight.data\n",
        "\n",
        "    net.fc = nn.Linear(in_features, out_features + new_outputs)\n",
        "    net.fc.weight.data[:out_features] = weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getNet():\n",
        "    return getResNet34(10)\n",
        "\n",
        "jointTraining(getNet, addOutputsToResNet, train_subsets, val_subsets, test_subsets)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ProjectMLDL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}