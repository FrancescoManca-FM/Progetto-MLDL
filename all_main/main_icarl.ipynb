{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_icarl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/master/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88af2f81-d5a8-48aa-92a5-aadab8c21fd6"
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6a85079-cdb3-436c-ed5f-c01fa2ad50b7"
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e23c7808-aa9b-403a-aa05-45aacbf8024e"
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 2278 (delta 85), reused 107 (delta 38), pack-reused 2119\u001b[K\n",
            "Receiving objects: 100% (2278/2278), 27.53 MiB | 10.36 MiB/s, done.\n",
            "Resolving deltas: 100% (1351/1351), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "71c86d95-a7b2-4506-a89b-2228923b1114"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}/cifar-100-python'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-24 21:03:21--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz.1’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  16.5MB/s    in 11s     \n",
            "\n",
            "2020-06-24 21:03:33 (14.8 MB/s) - ‘cifar-100-python.tar.gz.1’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "db26d4f4-f4a8-4fa7-d474-03dd714bb01a"
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]\n",
        "\n",
        "# icarl params\n",
        "herding = True # if false random exemplars, if true nme (herding)\n",
        "classifier = \"SVC\" # NCM, FCC, KNN, SVC, COS"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 66, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "001bc338-cffe-45f2-f3ad-d2176f945d8c"
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f326af86-d991-4070-cde3-e7cb71622f4d"
      },
      "source": [
        "from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, herding, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "     \n",
        "        # add other classifiers\n",
        "        if classifier == 'NCM':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.classify(images)\n",
        "        elif classifier == 'FCC':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.FCC_classify(images)\n",
        "        elif classifier == 'KNN' or classifier == 'SVC':\n",
        "          preds = net.KNN_SVC_classify(images)\n",
        "          preds = preds.to(DEVICE)\n",
        "        elif classifier == 'COS':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.COS_classify(images)\n",
        "\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        if method == 'test':\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "  accuracy = correct/len(dataset)\n",
        "\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        all_preds_cm = []\n",
        "        all_labels_cm = []\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "          train_set_big = train_subset\n",
        "        else:\n",
        "          test_set = utils.joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, train_dataset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(math.ceil(K/icarl.n_classes))\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          icarl.construct_exemplar_set(class_train_subset,m,y)\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier\n",
        "        icarl.computeMeans()\n",
        "\n",
        "        # common training on exemplars for KNN and SVC classifier\n",
        "        if classifier == 'KNN':\n",
        "          K_nn = int(math.ceil(m/10))\n",
        "          #print(K_nn)\n",
        "          #K_nn = 5\n",
        "          icarl.modelTrain(classifier, K_nn)\n",
        "        elif classifier == 'SVC':\n",
        "          icarl.modelTrain(classifier)\n",
        "\n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1dc75ebe-789a-4a4d-9fd8-4df5562167e0"
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.3147086799144745 class loss 0.3147086799144745 dist loss None\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.31007120013237 class loss 0.31007120013237 dist loss None\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.30922815203666687 class loss 0.30922815203666687 dist loss None\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.3079407811164856 class loss 0.3079407811164856 dist loss None\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.3023874759674072 class loss 0.3023874759674072 dist loss None\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.2989642322063446 class loss 0.2989642322063446 dist loss None\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.2915760278701782 class loss 0.2915760278701782 dist loss None\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.29518386721611023 class loss 0.29518386721611023 dist loss None\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.2702023983001709 class loss 0.2702023983001709 dist loss None\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.26380789279937744 class loss 0.26380789279937744 dist loss None\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.25995776057243347 class loss 0.25995776057243347 dist loss None\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.26949867606163025 class loss 0.26949867606163025 dist loss None\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.26494285464286804 class loss 0.26494285464286804 dist loss None\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.2703917920589447 class loss 0.2703917920589447 dist loss None\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.26308298110961914 class loss 0.26308298110961914 dist loss None\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.21540169417858124 class loss 0.21540169417858124 dist loss None\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.22618834674358368 class loss 0.22618834674358368 dist loss None\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.22645311057567596 class loss 0.22645311057567596 dist loss None\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.21879349648952484 class loss 0.21879349648952484 dist loss None\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.24258537590503693 class loss 0.24258537590503693 dist loss None\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.22193484008312225 class loss 0.22193484008312225 dist loss None\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.21745644509792328 class loss 0.21745644509792328 dist loss None\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.23161879181861877 class loss 0.23161879181861877 dist loss None\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.19787459075450897 class loss 0.19787459075450897 dist loss None\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.20663440227508545 class loss 0.20663440227508545 dist loss None\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.20050302147865295 class loss 0.20050302147865295 dist loss None\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.19590915739536285 class loss 0.19590915739536285 dist loss None\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.1912168562412262 class loss 0.1912168562412262 dist loss None\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.17910800874233246 class loss 0.17910800874233246 dist loss None\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.18878400325775146 class loss 0.18878400325775146 dist loss None\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.17351023852825165 class loss 0.17351023852825165 dist loss None\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.19195888936519623 class loss 0.19195888936519623 dist loss None\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.18033751845359802 class loss 0.18033751845359802 dist loss None\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.18754492700099945 class loss 0.18754492700099945 dist loss None\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.1487952470779419 class loss 0.1487952470779419 dist loss None\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.14949022233486176 class loss 0.14949022233486176 dist loss None\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.15664462745189667 class loss 0.15664462745189667 dist loss None\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.161441832780838 class loss 0.161441832780838 dist loss None\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.1326296180486679 class loss 0.1326296180486679 dist loss None\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12737607955932617 class loss 0.12737607955932617 dist loss None\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12038328498601913 class loss 0.12038328498601913 dist loss None\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.16626544296741486 class loss 0.16626544296741486 dist loss None\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1309097558259964 class loss 0.1309097558259964 dist loss None\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12015654146671295 class loss 0.12015654146671295 dist loss None\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12540002167224884 class loss 0.12540002167224884 dist loss None\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.11559275537729263 class loss 0.11559275537729263 dist loss None\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.13005366921424866 class loss 0.13005366921424866 dist loss None\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.10024368017911911 class loss 0.10024368017911911 dist loss None\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.09302472323179245 class loss 0.09302472323179245 dist loss None\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.10474719107151031 class loss 0.10474719107151031 dist loss None\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.087070032954216 class loss 0.087070032954216 dist loss None\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.08593868464231491 class loss 0.08593868464231491 dist loss None\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.0699930191040039 class loss 0.0699930191040039 dist loss None\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07683622092008591 class loss 0.07683622092008591 dist loss None\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07843562215566635 class loss 0.07843562215566635 dist loss None\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06370561569929123 class loss 0.06370561569929123 dist loss None\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.05672869831323624 class loss 0.05672869831323624 dist loss None\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.058343805372714996 class loss 0.058343805372714996 dist loss None\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06275641173124313 class loss 0.06275641173124313 dist loss None\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.08676470816135406 class loss 0.08676470816135406 dist loss None\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.0496772937476635 class loss 0.0496772937476635 dist loss None\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.04752875119447708 class loss 0.04752875119447708 dist loss None\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.05917338654398918 class loss 0.05917338654398918 dist loss None\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.042383451014757156 class loss 0.042383451014757156 dist loss None\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.05531812459230423 class loss 0.05531812459230423 dist loss None\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.03797850012779236 class loss 0.03797850012779236 dist loss None\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.039824239909648895 class loss 0.039824239909648895 dist loss None\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.028749680146574974 class loss 0.028749680146574974 dist loss None\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.03795575723052025 class loss 0.03795575723052025 dist loss None\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.03395327925682068 class loss 0.03395327925682068 dist loss None\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 93.54\n",
            "\n",
            "Test Accuracy (all groups seen so far): 80.50\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.15379442274570465 class loss 0.102266326546669 dist loss 0.051528096199035645\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.13738082349300385 class loss 0.0797501802444458 dist loss 0.057630639523267746\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.13425496220588684 class loss 0.07089922577142715 dist loss 0.06335573643445969\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.1279839277267456 class loss 0.07099803537130356 dist loss 0.05698589235544205\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.10464189946651459 class loss 0.050932396203279495 dist loss 0.05370950326323509\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.11090043187141418 class loss 0.05560492351651192 dist loss 0.05529550462961197\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.10862782597541809 class loss 0.05143625661730766 dist loss 0.05719156935811043\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.11209265887737274 class loss 0.05112897977232933 dist loss 0.06096367910504341\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.11942052841186523 class loss 0.05217210575938225 dist loss 0.06724842637777328\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.10274593532085419 class loss 0.044000305235385895 dist loss 0.05874563381075859\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.10077167302370071 class loss 0.046123068779706955 dist loss 0.05464860424399376\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.11105813086032867 class loss 0.050511617213487625 dist loss 0.06054651737213135\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.09175178408622742 class loss 0.037678707391023636 dist loss 0.05407307296991348\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.12289740145206451 class loss 0.0565587542951107 dist loss 0.06633865088224411\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08898574113845825 class loss 0.039262592792510986 dist loss 0.049723148345947266\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.10630813241004944 class loss 0.04677892476320267 dist loss 0.05952920392155647\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.09970781952142715 class loss 0.038121845573186874 dist loss 0.06158597394824028\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.11090581119060516 class loss 0.047189872711896896 dist loss 0.06371593475341797\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.10534913092851639 class loss 0.04669014364480972 dist loss 0.058658987283706665\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0933634489774704 class loss 0.029836732894182205 dist loss 0.0635267123579979\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08878451585769653 class loss 0.03678646311163902 dist loss 0.05199805647134781\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.09447076916694641 class loss 0.037726860493421555 dist loss 0.05674390867352486\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.0924888327717781 class loss 0.03475175052881241 dist loss 0.0577370822429657\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08968214690685272 class loss 0.026273155584931374 dist loss 0.0634089931845665\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08584862947463989 class loss 0.026589183136820793 dist loss 0.05925944447517395\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08594542741775513 class loss 0.02565767802298069 dist loss 0.060287751257419586\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.0910283774137497 class loss 0.030787745490670204 dist loss 0.06024063378572464\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08522447943687439 class loss 0.02741275168955326 dist loss 0.05781172588467598\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.07709769159555435 class loss 0.022441387176513672 dist loss 0.05465630441904068\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08265819400548935 class loss 0.025134379044175148 dist loss 0.05752381682395935\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08757677674293518 class loss 0.02775641344487667 dist loss 0.05982036516070366\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08393234759569168 class loss 0.02486390434205532 dist loss 0.05906844139099121\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08189526945352554 class loss 0.019293665885925293 dist loss 0.06260160356760025\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08889256417751312 class loss 0.02642124891281128 dist loss 0.06247131898999214\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08054173737764359 class loss 0.02023599110543728 dist loss 0.06030574440956116\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08318263292312622 class loss 0.018777666613459587 dist loss 0.06440496444702148\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07919886708259583 class loss 0.02359038218855858 dist loss 0.055608488619327545\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07916440069675446 class loss 0.023740103468298912 dist loss 0.05542429909110069\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08954020589590073 class loss 0.03080124966800213 dist loss 0.05873895436525345\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.09075330197811127 class loss 0.030245596542954445 dist loss 0.06050770357251167\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.09033482521772385 class loss 0.0379793606698513 dist loss 0.05235546454787254\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08879721164703369 class loss 0.024295328184962273 dist loss 0.06450188159942627\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08097723871469498 class loss 0.01824747584760189 dist loss 0.06272976100444794\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.09811946004629135 class loss 0.03399506211280823 dist loss 0.06412439793348312\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08135368674993515 class loss 0.020824095234274864 dist loss 0.06052958965301514\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.06967450678348541 class loss 0.019407156854867935 dist loss 0.05026734620332718\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.09030890464782715 class loss 0.02671193517744541 dist loss 0.06359697133302689\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.08026741445064545 class loss 0.01990029215812683 dist loss 0.060367126017808914\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.09113335609436035 class loss 0.02797459438443184 dist loss 0.06315876543521881\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.05942106246948242 class loss 0.007887126877903938 dist loss 0.05153393745422363\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.05575476959347725 class loss 0.007069906685501337 dist loss 0.04868486151099205\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06383798271417618 class loss 0.012423861771821976 dist loss 0.0514141209423542\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.0617109090089798 class loss 0.009046351537108421 dist loss 0.052664559334516525\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.061093416064977646 class loss 0.00927916169166565 dist loss 0.051814254373311996\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06359659135341644 class loss 0.008171922527253628 dist loss 0.05542466789484024\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.05900300294160843 class loss 0.006970460060983896 dist loss 0.05203254148364067\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.05641762912273407 class loss 0.006922345142811537 dist loss 0.049495283514261246\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06214127317070961 class loss 0.007447327021509409 dist loss 0.05469394475221634\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.05806829407811165 class loss 0.006013830658048391 dist loss 0.05205446481704712\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.05821571499109268 class loss 0.005139361135661602 dist loss 0.053076352924108505\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.05823246389627457 class loss 0.009070810861885548 dist loss 0.049161653965711594\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.059348028153181076 class loss 0.004504100885242224 dist loss 0.054843928664922714\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.055499717593193054 class loss 0.005946775432676077 dist loss 0.04955294355750084\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.05431711673736572 class loss 0.0019517180044203997 dist loss 0.05236539989709854\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.060268837958574295 class loss 0.0071541788056492805 dist loss 0.05311466008424759\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.04831003397703171 class loss 0.0026926554273813963 dist loss 0.04561737924814224\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.055478110909461975 class loss 0.004295791033655405 dist loss 0.05118231847882271\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.0523662269115448 class loss 0.002801906317472458 dist loss 0.04956432059407234\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.04994823411107063 class loss 0.0028365172911435366 dist loss 0.047111716121435165\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.05888545140624046 class loss 0.005983442533761263 dist loss 0.05290200933814049\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 97.02\n",
            "\n",
            "Test Accuracy (all groups seen so far): 76.10\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.106382817029953 class loss 0.05151773989200592 dist loss 0.05486507713794708\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.10285238176584244 class loss 0.044339776039123535 dist loss 0.0585126057267189\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.105149045586586 class loss 0.04510492831468582 dist loss 0.060044120997190475\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.09601400047540665 class loss 0.03541810065507889 dist loss 0.06059589982032776\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09145982563495636 class loss 0.032028019428253174 dist loss 0.059431806206703186\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.09119730442762375 class loss 0.030243588611483574 dist loss 0.060953717678785324\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08652538806200027 class loss 0.02522868476808071 dist loss 0.061296701431274414\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09082984179258347 class loss 0.03090658038854599 dist loss 0.059923261404037476\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08548074960708618 class loss 0.02445634827017784 dist loss 0.06102440506219864\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08848720788955688 class loss 0.026793166995048523 dist loss 0.06169404089450836\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08148907870054245 class loss 0.02050882577896118 dist loss 0.06098025292158127\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08725281804800034 class loss 0.025494806468486786 dist loss 0.06175801157951355\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.078761987388134 class loss 0.017994901165366173 dist loss 0.06076708436012268\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.07481327652931213 class loss 0.01650083437561989 dist loss 0.058312442153692245\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.07585757225751877 class loss 0.01648019254207611 dist loss 0.05937737971544266\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08907546103000641 class loss 0.02574268728494644 dist loss 0.06333277374505997\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.07550832629203796 class loss 0.016179325059056282 dist loss 0.05932900309562683\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07175315171480179 class loss 0.014455943368375301 dist loss 0.05729720741510391\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.07608252763748169 class loss 0.014906666241586208 dist loss 0.061175864189863205\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08032777905464172 class loss 0.018926952034235 dist loss 0.06140082702040672\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08236417174339294 class loss 0.018167365342378616 dist loss 0.06419680267572403\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.07746277749538422 class loss 0.013235792517662048 dist loss 0.06422698497772217\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.0797552838921547 class loss 0.018880268558859825 dist loss 0.06087501347064972\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.07014328986406326 class loss 0.010272413492202759 dist loss 0.059870876371860504\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.07857358455657959 class loss 0.014282199554145336 dist loss 0.06429138779640198\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07452625036239624 class loss 0.014053616672754288 dist loss 0.060472629964351654\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07922403514385223 class loss 0.01617427170276642 dist loss 0.06304976344108582\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08059181272983551 class loss 0.017459686845541 dist loss 0.06313212960958481\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08038131892681122 class loss 0.01911020651459694 dist loss 0.06127111241221428\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07853877544403076 class loss 0.013986114412546158 dist loss 0.0645526647567749\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08000034093856812 class loss 0.014350790530443192 dist loss 0.06564954668283463\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.07606451213359833 class loss 0.013909747824072838 dist loss 0.06215476244688034\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.06714784353971481 class loss 0.00889965333044529 dist loss 0.058248188346624374\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.0789220929145813 class loss 0.01584792509675026 dist loss 0.06307416409254074\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08060017228126526 class loss 0.014119703322649002 dist loss 0.06648046523332596\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.06996011734008789 class loss 0.009644261561334133 dist loss 0.06031585857272148\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07355540990829468 class loss 0.01351024117320776 dist loss 0.06004516780376434\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.06990969926118851 class loss 0.006351528689265251 dist loss 0.0635581687092781\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0703423023223877 class loss 0.01229028683155775 dist loss 0.05805201828479767\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07397100329399109 class loss 0.009741563349962234 dist loss 0.06422943621873856\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07326692342758179 class loss 0.011070787906646729 dist loss 0.06219613552093506\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.0704202651977539 class loss 0.011464383453130722 dist loss 0.05895588546991348\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07005441188812256 class loss 0.009350970387458801 dist loss 0.06070343777537346\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07623933255672455 class loss 0.012622404843568802 dist loss 0.06361693143844604\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.0726349949836731 class loss 0.013076025992631912 dist loss 0.059558965265750885\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07522277534008026 class loss 0.012266799807548523 dist loss 0.06295597553253174\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07317240536212921 class loss 0.014868536964058876 dist loss 0.058303870260715485\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07011774927377701 class loss 0.011386717669665813 dist loss 0.05873103067278862\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07185070216655731 class loss 0.008660821244120598 dist loss 0.06318987905979156\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.06258046627044678 class loss 0.003678454551845789 dist loss 0.058902010321617126\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.06071007251739502 class loss 0.0034905883949249983 dist loss 0.0572194829583168\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.05924757570028305 class loss 0.003408322110772133 dist loss 0.05583925172686577\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.0599086731672287 class loss 0.003483968321233988 dist loss 0.05642470344901085\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.05946826562285423 class loss 0.0024250049609690905 dist loss 0.05704326182603836\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.05479242652654648 class loss 0.002637310419231653 dist loss 0.05215511471033096\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.05916305631399155 class loss 0.002377789933234453 dist loss 0.05678526684641838\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.05757615715265274 class loss 0.002156196627765894 dist loss 0.055419959127902985\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.059040240943431854 class loss 0.00249216565862298 dist loss 0.05654807388782501\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.057643041014671326 class loss 0.003348458092659712 dist loss 0.05429458245635033\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.056439243257045746 class loss 0.0016789520159363747 dist loss 0.054760292172431946\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.05744480714201927 class loss 0.002975047565996647 dist loss 0.0544697605073452\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.05453387647867203 class loss 0.0031860133167356253 dist loss 0.05134786292910576\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.05554187670350075 class loss 0.0035153524950146675 dist loss 0.052026525139808655\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.05395319312810898 class loss 0.0013170837191864848 dist loss 0.05263610929250717\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.053507000207901 class loss 0.0015106659848242998 dist loss 0.05199633538722992\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.057064324617385864 class loss 0.0028538606129586697 dist loss 0.054210465401411057\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.05416126549243927 class loss 0.0017725885845720768 dist loss 0.05238867551088333\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.05361717566847801 class loss 0.0021681443322449923 dist loss 0.05144903063774109\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.056595467031002045 class loss 0.0016525362152606249 dist loss 0.05494293197989464\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.05676214024424553 class loss 0.001972158905118704 dist loss 0.05478998273611069\n",
            "Reducing each exemplar set to size: 67\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 96.52\n",
            "\n",
            "Test Accuracy (all groups seen so far): 69.37\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.09359556436538696 class loss 0.0366034135222435 dist loss 0.05699215084314346\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09565503895282745 class loss 0.03522857651114464 dist loss 0.060426466166973114\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.09447164833545685 class loss 0.03622990474104881 dist loss 0.058241743594408035\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.09201692044734955 class loss 0.02985859476029873 dist loss 0.06215832754969597\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09443647414445877 class loss 0.03078373707830906 dist loss 0.06365273892879486\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08945716917514801 class loss 0.027252722531557083 dist loss 0.062204450368881226\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08804615586996078 class loss 0.025368541479110718 dist loss 0.06267761439085007\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09028610587120056 class loss 0.025577841326594353 dist loss 0.06470826268196106\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08581038564443588 class loss 0.023225009441375732 dist loss 0.06258537620306015\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08471272885799408 class loss 0.021186871454119682 dist loss 0.06352585554122925\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08862049877643585 class loss 0.023836422711610794 dist loss 0.06478407979011536\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.0825052484869957 class loss 0.02094322256743908 dist loss 0.06156202778220177\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08908630162477493 class loss 0.022307133302092552 dist loss 0.06677916646003723\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08689980208873749 class loss 0.022340450435876846 dist loss 0.06455935537815094\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.07900942862033844 class loss 0.01499874610453844 dist loss 0.06401067972183228\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08672283589839935 class loss 0.021743163466453552 dist loss 0.0649796724319458\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.0839770957827568 class loss 0.018976517021656036 dist loss 0.06500057876110077\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07995564490556717 class loss 0.014647956006228924 dist loss 0.06530769169330597\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.07508117705583572 class loss 0.010367675684392452 dist loss 0.0647135004401207\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08515438437461853 class loss 0.01930427923798561 dist loss 0.06585010886192322\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08824025094509125 class loss 0.024733727797865868 dist loss 0.06350652128458023\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08018006384372711 class loss 0.0161824319511652 dist loss 0.06399763375520706\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08153142035007477 class loss 0.017394501715898514 dist loss 0.06413692235946655\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.07819907367229462 class loss 0.012405667454004288 dist loss 0.06579340994358063\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08374587446451187 class loss 0.016776539385318756 dist loss 0.06696933507919312\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07276006788015366 class loss 0.011384575627744198 dist loss 0.061375491321086884\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07540285587310791 class loss 0.010162425227463245 dist loss 0.06524042785167694\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07998394221067429 class loss 0.014540649019181728 dist loss 0.06544329226016998\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.07724009454250336 class loss 0.01064195018261671 dist loss 0.06659814715385437\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08066050708293915 class loss 0.012456368654966354 dist loss 0.0682041347026825\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.0814596563577652 class loss 0.014151917770504951 dist loss 0.0673077404499054\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08450423926115036 class loss 0.013900458812713623 dist loss 0.07060378044843674\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07600782811641693 class loss 0.014607761986553669 dist loss 0.06140006333589554\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07453583180904388 class loss 0.008472013287246227 dist loss 0.06606382131576538\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07468650490045547 class loss 0.0093655064702034 dist loss 0.06532099843025208\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07540447264909744 class loss 0.011973796412348747 dist loss 0.06343067437410355\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07519620656967163 class loss 0.009132297709584236 dist loss 0.06606391072273254\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07342857122421265 class loss 0.00972010288387537 dist loss 0.06370846927165985\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07480892539024353 class loss 0.00942241307348013 dist loss 0.06538651138544083\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.0718580111861229 class loss 0.008658974431455135 dist loss 0.06319903582334518\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07757289707660675 class loss 0.010022565722465515 dist loss 0.06755033135414124\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07564813643693924 class loss 0.008259989321231842 dist loss 0.0673881471157074\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07472575455904007 class loss 0.008987595327198505 dist loss 0.06573815643787384\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07142597436904907 class loss 0.006110552232712507 dist loss 0.06531542539596558\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07118593156337738 class loss 0.007581637240946293 dist loss 0.06360429525375366\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07357010245323181 class loss 0.007178747560828924 dist loss 0.06639135628938675\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07692979276180267 class loss 0.009223351255059242 dist loss 0.06770644336938858\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07208810746669769 class loss 0.006457872223109007 dist loss 0.0656302347779274\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07368187606334686 class loss 0.008803913369774818 dist loss 0.0648779645562172\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.06526599824428558 class loss 0.004797427449375391 dist loss 0.06046856939792633\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.0638444572687149 class loss 0.004604589194059372 dist loss 0.059239864349365234\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06169912591576576 class loss 0.002553969621658325 dist loss 0.05914515629410744\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.0618181973695755 class loss 0.0031544757075607777 dist loss 0.05866372212767601\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.061154890805482864 class loss 0.002714727772399783 dist loss 0.05844016373157501\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06139969453215599 class loss 0.002754849148914218 dist loss 0.058644846081733704\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06060713902115822 class loss 0.0013052349677309394 dist loss 0.05930190533399582\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06308121979236603 class loss 0.004228256177157164 dist loss 0.05885296314954758\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.062617726624012 class loss 0.002405670238658786 dist loss 0.06021205335855484\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06553807854652405 class loss 0.0035687219351530075 dist loss 0.06196935474872589\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.060074977576732635 class loss 0.0024323929101228714 dist loss 0.057642582803964615\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06112600862979889 class loss 0.00214182585477829 dist loss 0.0589841827750206\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.058716047555208206 class loss 0.002178322756662965 dist loss 0.056537725031375885\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.05848419666290283 class loss 0.0024443946313112974 dist loss 0.056039802730083466\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.060517240315675735 class loss 0.00323730381205678 dist loss 0.05727993696928024\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06528981029987335 class loss 0.0015539668966084719 dist loss 0.06373584270477295\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.05811633914709091 class loss 0.00151898805052042 dist loss 0.056597352027893066\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.05800643190741539 class loss 0.002628799993544817 dist loss 0.05537763237953186\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.060639433562755585 class loss 0.0016115776961669326 dist loss 0.05902785435318947\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06015082821249962 class loss 0.0011888638837262988 dist loss 0.05896196514368057\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.062179554253816605 class loss 0.002789820311591029 dist loss 0.05938973277807236\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 95.26\n",
            "\n",
            "Test Accuracy (all groups seen so far): 63.92\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.0926956832408905 class loss 0.027588728815317154 dist loss 0.06510695070028305\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09450733661651611 class loss 0.02973194234073162 dist loss 0.06477539241313934\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08443836867809296 class loss 0.020598696544766426 dist loss 0.06383967399597168\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08431155234575272 class loss 0.01877719908952713 dist loss 0.06553435325622559\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.0907401442527771 class loss 0.0226662065833807 dist loss 0.06807393580675125\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08512263000011444 class loss 0.01939716562628746 dist loss 0.06572546809911728\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.07964121550321579 class loss 0.014241469092667103 dist loss 0.06539974361658096\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08060292899608612 class loss 0.013529482297599316 dist loss 0.06707344949245453\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08297724276781082 class loss 0.01776878535747528 dist loss 0.06520845741033554\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08218804001808167 class loss 0.014995122328400612 dist loss 0.0671929195523262\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08534830808639526 class loss 0.018094416707754135 dist loss 0.06725389510393143\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.0813896432518959 class loss 0.01443630363792181 dist loss 0.06695333868265152\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.07907148450613022 class loss 0.013045690953731537 dist loss 0.06602579355239868\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.07863247394561768 class loss 0.011793072335422039 dist loss 0.06683940440416336\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.0829140767455101 class loss 0.014222494326531887 dist loss 0.06869158148765564\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.0801713615655899 class loss 0.010531600564718246 dist loss 0.06963976472616196\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.0781310424208641 class loss 0.010008424520492554 dist loss 0.06812261790037155\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07936228066682816 class loss 0.012375756166875362 dist loss 0.06698652356863022\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.07468251883983612 class loss 0.008801168762147427 dist loss 0.06588134914636612\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.07744693756103516 class loss 0.009091665036976337 dist loss 0.0683552697300911\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.07562900334596634 class loss 0.009798604063689709 dist loss 0.06583040207624435\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.07647862285375595 class loss 0.009052038192749023 dist loss 0.06742658466100693\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.07770807296037674 class loss 0.009280562400817871 dist loss 0.06842751055955887\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.07582080364227295 class loss 0.007156681269407272 dist loss 0.06866411864757538\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08123905956745148 class loss 0.01082240603864193 dist loss 0.0704166516661644\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07788591086864471 class loss 0.009042216464877129 dist loss 0.06884369254112244\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07594799995422363 class loss 0.009286095388233662 dist loss 0.06666190177202225\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07710883021354675 class loss 0.007693085353821516 dist loss 0.06941574811935425\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08218710124492645 class loss 0.0104244789108634 dist loss 0.07176262140274048\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07591366767883301 class loss 0.0070695700123906136 dist loss 0.06884409487247467\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07331395149230957 class loss 0.006519865244626999 dist loss 0.06679408997297287\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.07534398138523102 class loss 0.006931632291525602 dist loss 0.06841234862804413\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07547156512737274 class loss 0.008004733361303806 dist loss 0.06746683269739151\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07841988652944565 class loss 0.008532105945050716 dist loss 0.06988777965307236\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07528464496135712 class loss 0.004986486863344908 dist loss 0.07029815763235092\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07955226302146912 class loss 0.006819367408752441 dist loss 0.07273289561271667\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.0745379626750946 class loss 0.0069873095490038395 dist loss 0.0675506517291069\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07623538374900818 class loss 0.006566931959241629 dist loss 0.06966844946146011\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07567663490772247 class loss 0.006429403554648161 dist loss 0.06924723088741302\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.072756327688694 class loss 0.004808217287063599 dist loss 0.0679481104016304\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07145804911851883 class loss 0.004375574644654989 dist loss 0.0670824721455574\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07716859132051468 class loss 0.00813294481486082 dist loss 0.06903564929962158\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07292162626981735 class loss 0.0045322757214307785 dist loss 0.06838934868574142\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07541177421808243 class loss 0.007226078305393457 dist loss 0.06818569451570511\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07377155125141144 class loss 0.005530866794288158 dist loss 0.068240687251091\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07391925901174545 class loss 0.00456776050850749 dist loss 0.06935150176286697\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07541735470294952 class loss 0.004492625128477812 dist loss 0.07092472910881042\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.073372483253479 class loss 0.005004349164664745 dist loss 0.06836813688278198\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07693973183631897 class loss 0.007865083403885365 dist loss 0.06907464563846588\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.06917302310466766 class loss 0.0026484495028853416 dist loss 0.06652457267045975\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.06621412187814713 class loss 0.0025653077755123377 dist loss 0.06364881247282028\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06806568056344986 class loss 0.0032380521297454834 dist loss 0.06482762843370438\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.06519854813814163 class loss 0.0017503614071756601 dist loss 0.06344818323850632\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.06340492516756058 class loss 0.0018581930780783296 dist loss 0.06154673174023628\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06657019257545471 class loss 0.003069177968427539 dist loss 0.0635010153055191\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06456044316291809 class loss 0.001981435576453805 dist loss 0.06257900595664978\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06598442047834396 class loss 0.0019531846046447754 dist loss 0.06403123587369919\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.0653240829706192 class loss 0.0012849274789914489 dist loss 0.06403915584087372\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06527864187955856 class loss 0.0012724528787657619 dist loss 0.06400618702173233\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.0632358193397522 class loss 0.002084059175103903 dist loss 0.061151761561632156\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06538490206003189 class loss 0.001309023005887866 dist loss 0.06407587975263596\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0647648349404335 class loss 0.0012915139086544514 dist loss 0.06347332149744034\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.06584566831588745 class loss 0.0016662586713209748 dist loss 0.06417941302061081\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.06161748990416527 class loss 0.0011149848578497767 dist loss 0.06050250679254532\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06188442185521126 class loss 0.0012170006521046162 dist loss 0.06066742166876793\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.06659644097089767 class loss 0.0023670662194490433 dist loss 0.06422937661409378\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06460841000080109 class loss 0.0014689962845295668 dist loss 0.0631394162774086\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06279145181179047 class loss 0.0010154352057725191 dist loss 0.061776019632816315\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.0644284188747406 class loss 0.0011805967660620809 dist loss 0.06324782222509384\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06274387240409851 class loss 0.0010490638669580221 dist loss 0.061694808304309845\n",
            "Reducing each exemplar set to size: 40\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 93.62\n",
            "\n",
            "Test Accuracy (all groups seen so far): 59.02\n",
            "\n",
            "the model knows 50 classes:\n",
            " \n",
            "GROUP:  6\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.09004063904285431 class loss 0.023368515074253082 dist loss 0.06667212396860123\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08862697333097458 class loss 0.020605584606528282 dist loss 0.06802138686180115\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08791013062000275 class loss 0.019514743238687515 dist loss 0.06839539110660553\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.0844094529747963 class loss 0.014721381478011608 dist loss 0.06968807429075241\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08563505858182907 class loss 0.01545553095638752 dist loss 0.0701795294880867\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08156613260507584 class loss 0.012727716937661171 dist loss 0.06883841753005981\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08286818861961365 class loss 0.012673649936914444 dist loss 0.0701945424079895\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08296096324920654 class loss 0.011251723393797874 dist loss 0.07170923799276352\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08183500915765762 class loss 0.013430500403046608 dist loss 0.06840451061725616\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08233850449323654 class loss 0.011182754300534725 dist loss 0.07115574926137924\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.07990926504135132 class loss 0.011498997919261456 dist loss 0.06841026991605759\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08489573001861572 class loss 0.011148262768983841 dist loss 0.07374747097492218\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08035807311534882 class loss 0.010621512308716774 dist loss 0.06973656266927719\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08502037823200226 class loss 0.012237402610480785 dist loss 0.0727829784154892\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.07547774165868759 class loss 0.005903536453843117 dist loss 0.06957420706748962\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.079576276242733 class loss 0.007836511358618736 dist loss 0.07173976302146912\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08041393756866455 class loss 0.008514892309904099 dist loss 0.07189904898405075\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07756480574607849 class loss 0.007411920465528965 dist loss 0.0701528862118721\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.07574334740638733 class loss 0.006566304247826338 dist loss 0.06917704641819\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.07724212855100632 class loss 0.006707347463816404 dist loss 0.07053478062152863\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.0772300735116005 class loss 0.00511804549023509 dist loss 0.07211203128099442\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.07506390661001205 class loss 0.005632783751934767 dist loss 0.0694311261177063\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.0803857296705246 class loss 0.007519770413637161 dist loss 0.07286596298217773\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08021502196788788 class loss 0.006813232786953449 dist loss 0.0734017863869667\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.07643527537584305 class loss 0.005510715767741203 dist loss 0.0709245577454567\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07713180035352707 class loss 0.00645616976544261 dist loss 0.07067563384771347\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08146799355745316 class loss 0.008091852068901062 dist loss 0.0733761414885521\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07705382257699966 class loss 0.004261303227394819 dist loss 0.07279252260923386\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.07687497138977051 class loss 0.006821492686867714 dist loss 0.07005348056554794\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07859034091234207 class loss 0.006439675576984882 dist loss 0.07215066254138947\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.0805438756942749 class loss 0.006669153459370136 dist loss 0.07387471944093704\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.07543719559907913 class loss 0.003964013420045376 dist loss 0.07147318124771118\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07519857585430145 class loss 0.004425873979926109 dist loss 0.0707727000117302\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07733721286058426 class loss 0.005307778250426054 dist loss 0.07202943414449692\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07942604273557663 class loss 0.0058005740866065025 dist loss 0.07362546771764755\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07705431431531906 class loss 0.004772234242409468 dist loss 0.0722820833325386\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07799086719751358 class loss 0.004923397675156593 dist loss 0.07306747138500214\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07483575493097305 class loss 0.003695754799991846 dist loss 0.07113999873399734\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0738004595041275 class loss 0.004418488591909409 dist loss 0.0693819671869278\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07691089063882828 class loss 0.0054874420166015625 dist loss 0.07142344862222672\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07755385339260101 class loss 0.005042469594627619 dist loss 0.07251138240098953\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07842330634593964 class loss 0.00676638213917613 dist loss 0.07165692746639252\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07688531279563904 class loss 0.004415964242070913 dist loss 0.07246934622526169\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07472723722457886 class loss 0.0033665713854134083 dist loss 0.07136066257953644\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08026906847953796 class loss 0.005936706438660622 dist loss 0.07433236390352249\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07503384351730347 class loss 0.003924105316400528 dist loss 0.07110973447561264\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07906803488731384 class loss 0.0051198177970945835 dist loss 0.0739482194185257\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07015497237443924 class loss 0.003864516504108906 dist loss 0.06629045307636261\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.0742894634604454 class loss 0.0032877186313271523 dist loss 0.07100174576044083\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07049205899238586 class loss 0.0017233379185199738 dist loss 0.06876871734857559\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.06936628371477127 class loss 0.0026521789841353893 dist loss 0.06671410799026489\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06862187385559082 class loss 0.0016727899201214314 dist loss 0.06694908440113068\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07088393718004227 class loss 0.0013444966170936823 dist loss 0.06953944265842438\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.06890174001455307 class loss 0.0018156783189624548 dist loss 0.06708606332540512\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06925692409276962 class loss 0.001577853225171566 dist loss 0.06767906993627548\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06938184797763824 class loss 0.0014324863441288471 dist loss 0.06794936209917068\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06791182607412338 class loss 0.0010963452514261007 dist loss 0.06681548058986664\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06854411959648132 class loss 0.0014955338556319475 dist loss 0.0670485869050026\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06857459247112274 class loss 0.0020541269332170486 dist loss 0.06652046740055084\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.06801467388868332 class loss 0.0011584123130887747 dist loss 0.0668562650680542\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06713908165693283 class loss 0.0008400671067647636 dist loss 0.06629901379346848\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0688144862651825 class loss 0.0015254905447363853 dist loss 0.06728899478912354\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.06651336699724197 class loss 0.0010743218008428812 dist loss 0.06543904542922974\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.06839761883020401 class loss 0.0011190890800207853 dist loss 0.06727852672338486\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.065559521317482 class loss 0.0009017635602504015 dist loss 0.06465775519609451\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.06888705492019653 class loss 0.00147585803642869 dist loss 0.06741119921207428\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06641951203346252 class loss 0.0017061964608728886 dist loss 0.06471331417560577\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06622455269098282 class loss 0.0011395832989364862 dist loss 0.06508497148752213\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06700511276721954 class loss 0.0011376902693882585 dist loss 0.06586742401123047\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06628496199846268 class loss 0.0010889498516917229 dist loss 0.06519601494073868\n",
            "Reducing each exemplar set to size: 34\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 93.90\n",
            "\n",
            "Test Accuracy (all groups seen so far): 56.77\n",
            "\n",
            "the model knows 60 classes:\n",
            " \n",
            "GROUP:  7\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08493972569704056 class loss 0.014586903154850006 dist loss 0.07035282254219055\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.0809466540813446 class loss 0.01051699835807085 dist loss 0.07042965292930603\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08317308127880096 class loss 0.011959283612668514 dist loss 0.07121379673480988\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08305264264345169 class loss 0.010503304190933704 dist loss 0.07254933565855026\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08244878053665161 class loss 0.011370374821126461 dist loss 0.07107840478420258\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.07842293381690979 class loss 0.00843182485550642 dist loss 0.0699911117553711\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.0818018764257431 class loss 0.008591294288635254 dist loss 0.07321058213710785\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08236271142959595 class loss 0.010582473129034042 dist loss 0.0717802420258522\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08182617276906967 class loss 0.008175603114068508 dist loss 0.07365056872367859\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08274538069963455 class loss 0.007073283661156893 dist loss 0.07567209750413895\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.07872486114501953 class loss 0.007050486281514168 dist loss 0.07167437672615051\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08010273426771164 class loss 0.007617502007633448 dist loss 0.07248523086309433\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08103516697883606 class loss 0.006992947775870562 dist loss 0.07404221594333649\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.07711769640445709 class loss 0.005958705674856901 dist loss 0.0711589902639389\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08163659274578094 class loss 0.007498917169868946 dist loss 0.07413767278194427\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.07723286747932434 class loss 0.004193054977804422 dist loss 0.07303981482982635\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08151241391897202 class loss 0.006752963177859783 dist loss 0.07475945353507996\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.0811275914311409 class loss 0.005679531022906303 dist loss 0.07544805854558945\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08391836285591125 class loss 0.007140421308577061 dist loss 0.07677794247865677\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.07985902577638626 class loss 0.005816210526973009 dist loss 0.07404281198978424\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.07996699959039688 class loss 0.004825379233807325 dist loss 0.07514162361621857\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.07755564898252487 class loss 0.005666311364620924 dist loss 0.07188934087753296\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08177470415830612 class loss 0.007133824285119772 dist loss 0.07464087754487991\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.07393845170736313 class loss 0.003464951179921627 dist loss 0.07047349959611893\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08090302348136902 class loss 0.004855247214436531 dist loss 0.07604777812957764\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07830770313739777 class loss 0.005029848776757717 dist loss 0.07327785342931747\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07963044941425323 class loss 0.00449517322704196 dist loss 0.07513527572154999\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07682183384895325 class loss 0.004508765414357185 dist loss 0.07231307029724121\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.07269790768623352 class loss 0.002569167409092188 dist loss 0.07012873888015747\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07860151678323746 class loss 0.004339843988418579 dist loss 0.07426167279481888\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07648958265781403 class loss 0.00391405401751399 dist loss 0.07257553189992905\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08065374940633774 class loss 0.005913511849939823 dist loss 0.07474023848772049\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07748083770275116 class loss 0.004576634615659714 dist loss 0.07290419936180115\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07579870522022247 class loss 0.00441408297047019 dist loss 0.07138462364673615\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07911328971385956 class loss 0.003450293093919754 dist loss 0.0756630003452301\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07706822454929352 class loss 0.004196951165795326 dist loss 0.07287127524614334\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07799379527568817 class loss 0.0034283960703760386 dist loss 0.07456539571285248\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.0770564153790474 class loss 0.0033598854206502438 dist loss 0.07369653135538101\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0769040510058403 class loss 0.004149408545345068 dist loss 0.0727546438574791\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08057203888893127 class loss 0.003767716232687235 dist loss 0.07680432498455048\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07683148980140686 class loss 0.0032331212423741817 dist loss 0.07359836995601654\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07886560261249542 class loss 0.005427883006632328 dist loss 0.07343772053718567\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07583045959472656 class loss 0.0025510482955724 dist loss 0.07327941060066223\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07834690809249878 class loss 0.0030853827483952045 dist loss 0.07526152580976486\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.0759880319237709 class loss 0.0028404395561665297 dist loss 0.07314759492874146\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.0763336569070816 class loss 0.0020166647154837847 dist loss 0.07431699335575104\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07797360420227051 class loss 0.0027927181217819452 dist loss 0.07518088817596436\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0748448520898819 class loss 0.003522091079503298 dist loss 0.07132276147603989\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07528495788574219 class loss 0.0025750540662556887 dist loss 0.07270990312099457\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07052422314882278 class loss 0.0011519426479935646 dist loss 0.0693722814321518\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07357494533061981 class loss 0.0012535281712189317 dist loss 0.07232141494750977\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07055793702602386 class loss 0.0012844335287809372 dist loss 0.06927350163459778\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07401838898658752 class loss 0.0018596670124679804 dist loss 0.07215872406959534\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.0736004188656807 class loss 0.0012773729395121336 dist loss 0.07232304662466049\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.0703483372926712 class loss 0.0010657571256160736 dist loss 0.06928257644176483\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07188740372657776 class loss 0.001517686527222395 dist loss 0.07036972045898438\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07094528526067734 class loss 0.001340367365628481 dist loss 0.06960491836071014\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07188524305820465 class loss 0.0016352086095139384 dist loss 0.07025003433227539\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07142324000597 class loss 0.0011390766594558954 dist loss 0.0702841654419899\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07421237230300903 class loss 0.0012195404851809144 dist loss 0.07299283146858215\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06869538873434067 class loss 0.0010252741631120443 dist loss 0.06767011433839798\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0712379440665245 class loss 0.0009069845546036959 dist loss 0.07033096253871918\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07042937725782394 class loss 0.001137887709774077 dist loss 0.06929148733615875\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07194378972053528 class loss 0.001004546764306724 dist loss 0.07093924283981323\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.0715026706457138 class loss 0.000990695203654468 dist loss 0.07051197439432144\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07150732725858688 class loss 0.000985324615612626 dist loss 0.0705220028758049\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07024716585874557 class loss 0.001201350474730134 dist loss 0.06904581189155579\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07077652215957642 class loss 0.002040238119661808 dist loss 0.06873628497123718\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06948719173669815 class loss 0.0007023476646281779 dist loss 0.06878484040498734\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06987106055021286 class loss 0.0007405556389130652 dist loss 0.06913050264120102\n",
            "Reducing each exemplar set to size: 29\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 92.94\n",
            "\n",
            "Test Accuracy (all groups seen so far): 53.37\n",
            "\n",
            "the model knows 70 classes:\n",
            " \n",
            "GROUP:  8\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08747965097427368 class loss 0.01729254424571991 dist loss 0.07018710672855377\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08719433844089508 class loss 0.01652206853032112 dist loss 0.07067227363586426\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08377309888601303 class loss 0.012382171116769314 dist loss 0.07139092683792114\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08452355116605759 class loss 0.011573039926588535 dist loss 0.07295051217079163\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.0866275504231453 class loss 0.012882052920758724 dist loss 0.073745496571064\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08609267324209213 class loss 0.012797928415238857 dist loss 0.0732947438955307\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08374671638011932 class loss 0.012194736860692501 dist loss 0.07155197858810425\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08373040705919266 class loss 0.010149250738322735 dist loss 0.07358115911483765\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08541887253522873 class loss 0.011967520229518414 dist loss 0.07345135509967804\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0876857116818428 class loss 0.012320592068135738 dist loss 0.07536511868238449\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08141973614692688 class loss 0.009393271990120411 dist loss 0.07202646136283875\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08190439641475677 class loss 0.008592246100306511 dist loss 0.07331214845180511\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08211645483970642 class loss 0.008182014338672161 dist loss 0.07393444329500198\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08391300588846207 class loss 0.00808755960315466 dist loss 0.07582544535398483\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08405471593141556 class loss 0.007853636518120766 dist loss 0.07620108127593994\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.0827183723449707 class loss 0.009602155536413193 dist loss 0.07311621308326721\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08169436454772949 class loss 0.006273653358221054 dist loss 0.07542071491479874\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07897913455963135 class loss 0.005654092878103256 dist loss 0.07332504540681839\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.07999663800001144 class loss 0.006442762911319733 dist loss 0.07355387508869171\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08494141697883606 class loss 0.007817067205905914 dist loss 0.07712434977293015\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08272136747837067 class loss 0.008574765175580978 dist loss 0.07414659857749939\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08302102237939835 class loss 0.007862580008804798 dist loss 0.07515843957662582\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.07969684153795242 class loss 0.005846553947776556 dist loss 0.07385028898715973\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.07960686832666397 class loss 0.0064926533959805965 dist loss 0.07311421632766724\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.0806899145245552 class loss 0.005714222323149443 dist loss 0.07497569173574448\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08306725323200226 class loss 0.007411824073642492 dist loss 0.07565543055534363\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08060945570468903 class loss 0.004884458612650633 dist loss 0.0757249966263771\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08135008811950684 class loss 0.006560448091477156 dist loss 0.07478964328765869\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.0824437290430069 class loss 0.0064444406889379025 dist loss 0.07599928975105286\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08121225237846375 class loss 0.0070342072285711765 dist loss 0.074178047478199\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08119817823171616 class loss 0.0046501546166837215 dist loss 0.0765480250120163\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.07966647297143936 class loss 0.0053062657825648785 dist loss 0.0743602067232132\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08076344430446625 class loss 0.005880177021026611 dist loss 0.07488326728343964\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07860366255044937 class loss 0.005009762942790985 dist loss 0.07359389960765839\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08266560733318329 class loss 0.006205195095390081 dist loss 0.07646041363477707\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07781829684972763 class loss 0.00354711408726871 dist loss 0.07427117973566055\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07872051745653152 class loss 0.0030806774739176035 dist loss 0.07563983649015427\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07835851609706879 class loss 0.003552434965968132 dist loss 0.0748060792684555\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0785246193408966 class loss 0.004669744521379471 dist loss 0.07385487109422684\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08112984150648117 class loss 0.005621621385216713 dist loss 0.0755082219839096\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07748306542634964 class loss 0.0027219527401030064 dist loss 0.07476111501455307\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.0801621824502945 class loss 0.0049752723425626755 dist loss 0.07518690824508667\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08158540725708008 class loss 0.00545116513967514 dist loss 0.07613424211740494\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07942228019237518 class loss 0.003540127072483301 dist loss 0.07588215172290802\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07960288226604462 class loss 0.0029754035640507936 dist loss 0.07662747800350189\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07910175621509552 class loss 0.003738459898158908 dist loss 0.07536329329013824\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07879847288131714 class loss 0.003508116351440549 dist loss 0.07529035955667496\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0815364345908165 class loss 0.004059233702719212 dist loss 0.07747720181941986\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.0769428089261055 class loss 0.004077912773936987 dist loss 0.07286489754915237\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07312498241662979 class loss 0.0017756499582901597 dist loss 0.07134933024644852\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07133173942565918 class loss 0.0011121842544525862 dist loss 0.07021955400705338\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07349870353937149 class loss 0.0018917315173894167 dist loss 0.07160697132349014\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.0731472596526146 class loss 0.001394422259181738 dist loss 0.07175283879041672\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07091473042964935 class loss 0.0010141723323613405 dist loss 0.06990055739879608\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07171865552663803 class loss 0.0012290322920307517 dist loss 0.07048962265253067\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07250738888978958 class loss 0.00125943124294281 dist loss 0.07124795764684677\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07236000895500183 class loss 0.0011084619909524918 dist loss 0.07125154882669449\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07312329113483429 class loss 0.0009837457910180092 dist loss 0.07213954627513885\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07134076207876205 class loss 0.001109386677853763 dist loss 0.0702313780784607\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07348302751779556 class loss 0.0011155421379953623 dist loss 0.07236748188734055\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07215763628482819 class loss 0.0008122043800540268 dist loss 0.07134543359279633\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07174478471279144 class loss 0.0013982737436890602 dist loss 0.07034651190042496\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07344350218772888 class loss 0.0010349927470088005 dist loss 0.0724085122346878\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07312071323394775 class loss 0.0012910835212096572 dist loss 0.07182963192462921\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07112664729356766 class loss 0.001624225522391498 dist loss 0.06950242072343826\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07125908881425858 class loss 0.0009806181769818068 dist loss 0.07027847319841385\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.0726848617196083 class loss 0.0012088476214557886 dist loss 0.07147601246833801\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07196704298257828 class loss 0.0014973360812291503 dist loss 0.07046970725059509\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07141400128602982 class loss 0.0009453138336539268 dist loss 0.07046868652105331\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07293243706226349 class loss 0.0008191948872990906 dist loss 0.07211324572563171\n",
            "Reducing each exemplar set to size: 25\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 93.44\n",
            "\n",
            "Test Accuracy (all groups seen so far): 48.59\n",
            "\n",
            "the model knows 80 classes:\n",
            " \n",
            "GROUP:  9\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08609368652105331 class loss 0.014338531531393528 dist loss 0.07175515592098236\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08658348768949509 class loss 0.011585147120058537 dist loss 0.07499834150075912\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08473037928342819 class loss 0.009362882934510708 dist loss 0.07536749541759491\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08369921147823334 class loss 0.009834184311330318 dist loss 0.07386502623558044\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08515473455190659 class loss 0.009208410046994686 dist loss 0.07594632357358932\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08262290805578232 class loss 0.007804407738149166 dist loss 0.07481849938631058\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08400855958461761 class loss 0.00904748123139143 dist loss 0.07496108114719391\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08348735421895981 class loss 0.008083525113761425 dist loss 0.07540383189916611\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08193942904472351 class loss 0.006554449908435345 dist loss 0.07538498193025589\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08190720528364182 class loss 0.00800306536257267 dist loss 0.0739041417837143\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08297272771596909 class loss 0.0072867111302912235 dist loss 0.075686015188694\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08153169602155685 class loss 0.00686493469402194 dist loss 0.07466676086187363\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.07892782241106033 class loss 0.004850536119192839 dist loss 0.07407728582620621\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08123283088207245 class loss 0.0063620675355196 dist loss 0.074870765209198\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08154840767383575 class loss 0.0057089426554739475 dist loss 0.07583946734666824\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08041106909513474 class loss 0.005453981924802065 dist loss 0.07495708763599396\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08185701817274094 class loss 0.005249137058854103 dist loss 0.07660788297653198\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08099152147769928 class loss 0.004986207000911236 dist loss 0.07600531727075577\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08031821250915527 class loss 0.004750273190438747 dist loss 0.07556793838739395\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08125932514667511 class loss 0.003595320275053382 dist loss 0.07766400277614594\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.07839144766330719 class loss 0.0031210952438414097 dist loss 0.07527035474777222\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.07994192093610764 class loss 0.003351237392053008 dist loss 0.07659068703651428\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.07907375693321228 class loss 0.004980309400707483 dist loss 0.07409344613552094\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08025822043418884 class loss 0.003708183765411377 dist loss 0.07655003666877747\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08088761568069458 class loss 0.003417550353333354 dist loss 0.07747006416320801\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.07912030071020126 class loss 0.0027439885307103395 dist loss 0.07637631148099899\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07853072136640549 class loss 0.0028908399399369955 dist loss 0.07563988119363785\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07943001389503479 class loss 0.0026464685797691345 dist loss 0.07678354531526566\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08093719184398651 class loss 0.004136509262025356 dist loss 0.07680068165063858\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.0781894102692604 class loss 0.003906541969627142 dist loss 0.07428286969661713\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.0804523155093193 class loss 0.004216438625007868 dist loss 0.07623587548732758\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.07785316556692123 class loss 0.0021654232405126095 dist loss 0.07568774372339249\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07820205390453339 class loss 0.003621296724304557 dist loss 0.07458075881004333\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.0781785175204277 class loss 0.0033765335101634264 dist loss 0.0748019814491272\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.0798824355006218 class loss 0.0029525498393923044 dist loss 0.07692988216876984\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07936494052410126 class loss 0.002474195556715131 dist loss 0.0768907442688942\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07878520339727402 class loss 0.002466521691530943 dist loss 0.07631868124008179\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07970838993787766 class loss 0.002857858780771494 dist loss 0.0768505334854126\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0785369798541069 class loss 0.002525411080569029 dist loss 0.07601156830787659\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07888932526111603 class loss 0.0021523006726056337 dist loss 0.07673702389001846\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07791109383106232 class loss 0.002273367950692773 dist loss 0.07563772797584534\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08026666194200516 class loss 0.003323148237541318 dist loss 0.07694351673126221\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.0769626572728157 class loss 0.0018616331508383155 dist loss 0.07510102540254593\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07993663847446442 class loss 0.0027146674692630768 dist loss 0.07722196727991104\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07670009881258011 class loss 0.0017686568899080157 dist loss 0.07493144273757935\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07810056954622269 class loss 0.0020645237527787685 dist loss 0.07603604346513748\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07749246060848236 class loss 0.002196605084463954 dist loss 0.07529585808515549\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0778353363275528 class loss 0.003208296140655875 dist loss 0.07462704181671143\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.0798226147890091 class loss 0.0029424435924738646 dist loss 0.07688017189502716\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07593984156847 class loss 0.0013905575033277273 dist loss 0.07454928755760193\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07603514939546585 class loss 0.0007945963880047202 dist loss 0.07524055242538452\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07441242039203644 class loss 0.0009294927585870028 dist loss 0.0734829306602478\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07529847323894501 class loss 0.0012123628985136747 dist loss 0.07408610731363297\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07401255518198013 class loss 0.0009125440265052021 dist loss 0.0731000080704689\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07373028993606567 class loss 0.0010256394743919373 dist loss 0.07270465046167374\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07381097972393036 class loss 0.0009167548269033432 dist loss 0.07289422303438187\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07533447444438934 class loss 0.0008763757068663836 dist loss 0.07445809990167618\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07401596009731293 class loss 0.0008369804127141833 dist loss 0.0731789767742157\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07122605293989182 class loss 0.0006440094439312816 dist loss 0.07058204710483551\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.0750424936413765 class loss 0.0009821103885769844 dist loss 0.07406038045883179\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07335503399372101 class loss 0.0009979690657928586 dist loss 0.07235706597566605\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07340207695960999 class loss 0.0008754406590014696 dist loss 0.07252663373947144\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07262897491455078 class loss 0.0011664708144962788 dist loss 0.07146250456571579\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07291067391633987 class loss 0.0009481410379521549 dist loss 0.07196253538131714\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07234379649162292 class loss 0.0006400286802090704 dist loss 0.07170376926660538\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07358735054731369 class loss 0.0006183562218211591 dist loss 0.0729689970612526\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07340706884860992 class loss 0.000713653804268688 dist loss 0.07269341498613358\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07350528985261917 class loss 0.0009090027888305485 dist loss 0.07259628921747208\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07322824001312256 class loss 0.000503181538078934 dist loss 0.07272505760192871\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.0723634585738182 class loss 0.000666077365167439 dist loss 0.07169738411903381\n",
            "Reducing each exemplar set to size: 23\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 91.62\n",
            "\n",
            "Test Accuracy (all groups seen so far): 46.63\n",
            "\n",
            "the model knows 90 classes:\n",
            " \n",
            "GROUP:  10\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08859085291624069 class loss 0.014012189581990242 dist loss 0.0745786651968956\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08648756891489029 class loss 0.00982384942471981 dist loss 0.07666371762752533\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08666415512561798 class loss 0.009852402843534946 dist loss 0.07681175321340561\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.0845131203532219 class loss 0.00735065434128046 dist loss 0.07716246694326401\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08596398681402206 class loss 0.00886628869920969 dist loss 0.07709769904613495\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08223394304513931 class loss 0.007274176459759474 dist loss 0.07495976984500885\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.0861205980181694 class loss 0.009052611887454987 dist loss 0.07706798613071442\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08379646390676498 class loss 0.007323429919779301 dist loss 0.07647303491830826\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08237217366695404 class loss 0.006410535424947739 dist loss 0.0759616419672966\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0830506756901741 class loss 0.006182707380503416 dist loss 0.0768679678440094\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.0872824490070343 class loss 0.007214542478322983 dist loss 0.08006790280342102\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08318080753087997 class loss 0.004940441809594631 dist loss 0.07824036478996277\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08473559468984604 class loss 0.006216085981577635 dist loss 0.07851950824260712\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08366472274065018 class loss 0.005806566681712866 dist loss 0.07785815745592117\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.0826251208782196 class loss 0.004271558951586485 dist loss 0.07835356146097183\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.0830170065164566 class loss 0.006287770811468363 dist loss 0.07672923803329468\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08133110404014587 class loss 0.004791827406734228 dist loss 0.07653927803039551\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.0828886404633522 class loss 0.005755205173045397 dist loss 0.0771334320306778\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08361994475126266 class loss 0.0048042903654277325 dist loss 0.07881565392017365\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08370589464902878 class loss 0.004151398316025734 dist loss 0.0795544981956482\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08110778033733368 class loss 0.004454409703612328 dist loss 0.0766533687710762\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08346477895975113 class loss 0.004574689082801342 dist loss 0.07889009267091751\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08253928273916245 class loss 0.004761173389852047 dist loss 0.07777810841798782\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08140625804662704 class loss 0.004351641517132521 dist loss 0.07705461978912354\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08424228429794312 class loss 0.004133471753448248 dist loss 0.08010881394147873\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.0822732001543045 class loss 0.004091286566108465 dist loss 0.0781819149851799\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08195764571428299 class loss 0.003992361947894096 dist loss 0.07796528190374374\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08371549844741821 class loss 0.004294761922210455 dist loss 0.07942073792219162\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08107215166091919 class loss 0.003327484941110015 dist loss 0.07774467021226883\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08128571510314941 class loss 0.0031064727809280157 dist loss 0.0781792402267456\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08087797462940216 class loss 0.003962124232202768 dist loss 0.07691585272550583\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08419971913099289 class loss 0.003692135214805603 dist loss 0.08050758391618729\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08379905670881271 class loss 0.00533625902608037 dist loss 0.07846279442310333\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.0816313698887825 class loss 0.00324798421934247 dist loss 0.07838338613510132\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.0809909999370575 class loss 0.002871557604521513 dist loss 0.0781194418668747\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.0835026204586029 class loss 0.00378480926156044 dist loss 0.07971780747175217\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08100248128175735 class loss 0.002961711958050728 dist loss 0.07804077118635178\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.0809401273727417 class loss 0.003082695184275508 dist loss 0.07785743474960327\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08101203292608261 class loss 0.0027563327457755804 dist loss 0.07825569808483124\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08390036970376968 class loss 0.004351797979325056 dist loss 0.07954857498407364\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08414831757545471 class loss 0.0033470240887254477 dist loss 0.08080129325389862\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08158095180988312 class loss 0.0026804099325090647 dist loss 0.0789005383849144\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08212573826313019 class loss 0.0034487543161958456 dist loss 0.0786769837141037\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.08214502036571503 class loss 0.0031454283744096756 dist loss 0.0789995938539505\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07916594296693802 class loss 0.002812558552250266 dist loss 0.07635338604450226\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08068136125802994 class loss 0.0029346724040806293 dist loss 0.0777466893196106\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08083626627922058 class loss 0.0031164546962827444 dist loss 0.07771981507539749\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.08082237839698792 class loss 0.003090701764449477 dist loss 0.0777316763997078\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08086752146482468 class loss 0.0035543020348995924 dist loss 0.07731322199106216\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07629986852407455 class loss 0.001581425778567791 dist loss 0.07471844553947449\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07656598091125488 class loss 0.0014424317050725222 dist loss 0.07512354850769043\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07755981385707855 class loss 0.001547830761410296 dist loss 0.07601198554039001\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07679624855518341 class loss 0.0011874708579853177 dist loss 0.07560877501964569\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07514192163944244 class loss 0.0014459810918197036 dist loss 0.07369594275951385\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.0736747533082962 class loss 0.001078808563761413 dist loss 0.07259594649076462\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07468221336603165 class loss 0.0012426587054505944 dist loss 0.07343955338001251\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.0743122324347496 class loss 0.0008821570663712919 dist loss 0.07343007624149323\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07483826577663422 class loss 0.0008753791335038841 dist loss 0.07396288961172104\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07648047804832458 class loss 0.0009275234187953174 dist loss 0.07555295526981354\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07495277374982834 class loss 0.001681611523963511 dist loss 0.07327116280794144\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07602866739034653 class loss 0.0015817597741261125 dist loss 0.0744469091296196\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07669639587402344 class loss 0.000898771861102432 dist loss 0.07579762488603592\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07729259133338928 class loss 0.0009596335585229099 dist loss 0.0763329565525055\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07544912397861481 class loss 0.0010230769403278828 dist loss 0.07442604750394821\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07719416916370392 class loss 0.0012020356953144073 dist loss 0.07599212974309921\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07515527307987213 class loss 0.0007778636645525694 dist loss 0.07437741011381149\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07545839250087738 class loss 0.0005833330214954913 dist loss 0.07487505674362183\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.0745806023478508 class loss 0.0010837690206244588 dist loss 0.07349683344364166\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07686011493206024 class loss 0.0007989066652953625 dist loss 0.07606121152639389\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07568216323852539 class loss 0.0012396815000101924 dist loss 0.07444248348474503\n",
            "Reducing each exemplar set to size: 20\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 91.74\n",
            "\n",
            "Test Accuracy (all groups seen so far): 43.98\n",
            "\n",
            "the model knows 100 classes:\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d78e6dd-21b7-455b-8d16-7e476a0858b0"
      },
      "source": [
        "if herding:\n",
        "  method = 'iCaRL_{}_herding'.format(classifier)\n",
        "else:\n",
        "  method = 'iCaRL_{}_random'.format(classifier)\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics iCaRL for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJcCAYAAAAb0rWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde7gcVZnv8e+bBIzKJYZbkMAERchoxkSJGG8QwUtQNMwcRJRbAI2CigIOF5kj6ugMKl5gRmCiIKBAEOXmCBEGjRzOMWCCAUFQLm4kIZEEiAjKJeQ9f1Rt6K6u3bV67arq3r1/n+fZT3Z1V621qrq6d6X6fddr7o6IiIhIvxnT7QGIiIiIVEEXOSIiItKXdJEjIiIifUkXOSIiItKXdJEjIiIifUkXOSIiItKXdJEjw2ZmLzSzH5vZn83s0mG0c6CZXVvm2LrBzK4xs0MjtuuL/S+bmU0xMzezcXW0H/v6jRRmdp6ZfbHb4xCpgy5yRhEz+6CZLTWzx81sVfph/uYSmt4P2AbYwt3fF9uIu1/o7u8oYTxNzGx2+kfs8szj09PHFwe28zkz+37Reu6+t7uf3+k4s/tviaPN7HYze8LMVpjZpWb2DwFjHfzD/Xj6M2BmJ2bWGTCzt3UyRjP7jJn9IW1zhZldkj5+tpldkLP+dDN7yswmpss7p/uwNr0ovs3MjjWzsZ2Mo0qxr1+/MrOtzOyi9PV61MwuzDz/NjO7peEc3b9bYxXJ0kXOKGFmxwLfBP6N5IJkB+BMYG4Jzf8d8Ht3X19CW1VZA7zBzLZoeOxQ4PdldZBelJT5njod+CRwNDAR2Bm4Anh3B21McPdNSC5E/7eZvT12MOndjYOBt6VtzgSuT58+H/gnM3txZrODgf9290fM7OXATcADwD+4++bA+9J2No0dV4f7UMndoD53GbCa5DNja+C0wSfM7JXARcDJwObAdGBZF8Yoks/d9dPnPyQfPo8D72uzzgtILoIeTH++CbwgfW42sAI4DngIWAUclj73eeBp4Jm0jyOAzwHfb2h7CuDAuHR5HnAf8BfgD8CBDY/f2LDdG4FfAX9O/31jw3OLgX8F/m/azrXAlkPs2+D4zwY+lj42FlgJfBZY3LDu6SR/hB8j+bB+S/r4nMx+3towji+l4/gbsFP62IfS588CftTQ/pdJLgwsZ5zP7T/wCuBZYLc2r9m7gV+nY30A+NxQxzx97GbgnxuWB0guWELPo/8Evtnm+d8BhzQsj03Ppbnp8veBn0Scv4P7cijwR2AtcHLD82OAE4F7gYeBHwATM9sekW57Qzqu09J27gM+RvP52fj6zQNuTNd/lOR83buh7x3TNv8C/A/wLRrO/Q7304BvkLzHHgN+A0xreH+elu7Dn0jO5Rc2bLsPsBxYB/w/4NUNz70GuCUd4yXAQuCLgWN6R3qejB3i+YuAf43ZX/3op44f3ckZHd4AjAcub7POycAsYAbJ/8Z2A/6l4flJJBdL25H8wfiWmb3E3U8huTt0ibtv4u7ntBtI+j/9M0j+UGxKciGzPGe9icBP0nW3AL4O/CRzJ+aDwGEk/7vcGPh0u76BC4BD0t/fCdxO8ke40a9IjsFEkg/wS81svLsvyuzn9IZtDgbmk9yNuD/T3nHAP5jZPDN7C8mxO9Tdi+qp7AWscPeb26zzRLo/E0gueI40s33zVjSzWcA04J6CfttZAhxiZv9sZjNzvmJqPL4AbwM2Aq5uWP7hMPp/M7ALybH5rJn9ffr4J4B9gT2Al5JcjHwrs+0ewN+TvO4fJrkoeA3JXaT9Cvp9PckF3JbAV4BzzMzS5y4iuXjcguTi/uC4XQOSC4rdSe7YbQ7sT3LRBnBq+vgMkgvp7Ugu0DGz1wDnAh9Jx/FfwFVm9gIz25jk7t/3SM7pS4H/1dipma1r87X1LJJ9P9/MHjazX5nZHpnnMbPfpF+Bf3/wq0mRXqCLnNFhC2Ctt/866UDgC+7+kLuvIblD0/iB/Uz6/DPufjXJ3YxdIsezAZhmZi9091XufkfOOu8G7nb377n7ene/GLgLeE/DOt9199+7+99I/vc+o12n7v7/gIlmtgvJH+OWGBJ3/767P5z2+TWS/0EX7ed57n5Hus0zmfb+SnIcv05yJ+MT7r6ioD1IXrNVBfuz2N1/4+4b3P024GKSP+aN1prZ34Bfknw9eUVA30P1932SC4p3Ar8AHjKzExpW+R6wh5lNTpcPAS5qOCaF+1Tg8+7+N3e/FbiV5GIc4KMkd3ZWuPtTJBcb+2W+mvqcuz+Rniv7k9yResDdHwH+vaDf+9392+7+LMnXctsC25jZDsDrgM+6+9PufiNw1TD27xmSC+WpJHf67nT3VekF1XzgGHd/xN3/QnLBfUC63Xzgv9z9Jnd/1pN4oqdILkBmkVxofjN97/6Q5EL+Oe4+IR17nskkF18/J/mPzteAK81sy4bnDya5cHoF8ELgP4ZxDERKpYuc0eFhYMuCeISX0nwX4v70sefayFwk/RXYpNOBuPsTwPtJ/jCtMrOfmNnUgPEMjmm7huXVEeP5HvBx4K3k3Nkys0+b2Z1pkOU6kv9Rb5ldL+OBdk+6+00kX4sYycVYiIdJ/pgOycxeb2Y/N7M1ZvZnkmOaHeuWJMflOJKv7TYK7D+XJ8HRbyO5e/RR4F/N7J3pc4NfBx1kZpuQ3F1pvJAs3KcCQ73efwdcnt6RWAfcSfJV3zYN6ze+Ri/NLGfPsyH7TS9aSft+KfBIw2PZfpqkgf6DgeAHZp9395+RfCX4LZILyAVmthmwFfAiYFnDPi5KH4dk/48bfC59fvt0fC8FVmbuHBbtb6O/AQPufk56kbQw3cc3NTw/+J+Nx0kuvt7VQfsildJFzujwS5L/2eV+lZF6kOTDctAOtH6VE+oJkg/lQZMan3T3n7r720n+4N0FfDtgPINjWhk5pkHfA44Crs78cSL9Oul4kv/pv8TdJ5DEAw1+NTHUV0xtv3oys4+R3BF6MG0/xPXAZDOb2Wadi0juHGzvSRDv2Q1jfX5wyf/uvw48SbLvw5b+wbsUuI3ka7BB5/P8/+z/4O6NQaj/Q+arkpI8QPL154SGn/Hu3niuNL5Gq0guAgbtENnvKpI7g43n+vZDrexJ1tYm6c+FQ6xzhrvvCryS5OupfyaJHfob8KqG/dvck+BvSPb/S5n9f1F693MVsF3D12ud7u9ttJ7f3ub5oq9hRWqli5xRwN3/TPL9/bfMbF8ze5GZbWRme5vZV9LVLgb+JU0X3TJdvzBdegjLgd3NbAcz2xw4afAJM9vGzOamsTlPkXzttSGnjauBnS1Jex9nZu8n+eD/78gxAeDufyD5SufknKc3BdaTZGKNM7PPAps1PP8nYEonGVRmtjPwReAgkj/+x5tZ26/V0nHeTfL10sWWpMBvbGbjzewAez4VfFOSOwlPmtluJDFK7Zya9j++4bGN0nYHf4a825fGFb3bzDY1szFmtjfwKpKMqUE/Ivkj+nmSC55GpwBvNLOvmtmktM2d0jiOCQVjb+ds4Etm9ndpm1uZWbuswR8AR5vZZDN7CUnQcsfc/X5gKfC59PV5A81fp3bEzF6X3p3biOQ/Ck8CG9x9A8l/BL5hZlun6243eActfe6j6bZmZi8efJ1I/oOzPt3fjczsn0ji7UJdDrzEzA41s7Fmth/JV1T/N33+u8BhZvay9GLvRIb5HhUpky5yRok0vuRYkmDiNST/+/s4z8dofJHkA/s2kqyOW9LHYvq6jiSL4zaSDKXGD70x6TgeBB4hueA4MqeNh0mCQ48j+ZrjeGAfd18bM6ZM2ze6e95dqp+SfA3we5Jb+k/S/PXD4ESHD5vZLUX9pBcM3we+7O63phcunwG+Z2YvCBjq0Tz/9cU6kuyhfwR+nD5/FPAFM/sLyUVp0VdhPyEJyv1ww2NXk9wlGPz5XJvtH0vH/8d0PF8BjmyM50i/jvwRyR/CprsV7n4vSRD8FOCO9Cu2H5Gcd38pGHs7p5Pc0bo2PRZLSIKFh/Jtktf6VpLz/LJh9H0gyT49TPJ+uYTk4j3GZunYHiU5/x4Gvpo+dwJJ0PgSM3uM5K7YLgDuvpTkNf3PdNt7SLLCcPengX9Klx8h+aq4aX/Tr8/ekjegNGbpvSRB/X8muYiZO/g+dPdzSb6SvCkd81Mk561IT7DiJA8REQlhyeSId3mSdSgiXaY7OSIikdKvmF6efn03h2RyzegMNhEply5yROQ5lpRteDzn55qK+z1wiH7zphfoJZNIJg98nGROpyPd/dddHZGIPEdfV4mIiEhf0p0cERER6UtdKVaXfnd9OkkNme+4+6nt1r/qO2c13W767HnNNQDHPflkyzbrx48vXKcqj0+a1PLYJqtX56zZ/7KvA9T7Wki5Ql7PdVOmtKwzYWCg476y7cS0keeeOXNaHttp0aJS2o4R81k1Et9X3fxMDtFrx3Tp0vktc15VrM6vdWrbt9rv5FhS7+ZbwN4k8558wJJKtiIiIiKl6cbXVbsB97j7fekcDgtJMhJEREREStONi5ztaJ5gbQXN9YgAMLP5ZrbUzJb+9IahaseJiIjIcG149tnafurUs4HH7r7A3We6+8x37v7mbg9HRERERphuBB6vpLmI3WQKii5+4eyxTcv/ecpmTcuf+nxrcFg3A8bGr1vX8lheUFujXguerlLMsaiin9D+YwMSi16/bgc6xpxfIeuUFSBcVjtZ3QwyLktZ506V5+CTE5rLkWU/F+s8/2M/C/r1MzjPhg15JQSrMWbs2OKVyuqrtp6e9yvgFWa2o5ltDBxAUndGRET6QPYCR6Rbar+T4+7rzezjJAXyxgLnunuvz2oqIiLSt2qNldloo9q66so8Oe5+NUn1YxEREZFKdOUiR0RERHrHhg31Zj3VpWezq0RERESGo/Y7OWa2PXABsA3JNNIL3P30dttkg9g+9fmHmpaXHb1vyzbTFzRnUNQ5VXreNlde8uem5bnv37zjdrstJtMgZIr/qjIWymo3tp2i7arMIss+lpfxV1U2Tl5Zk6zseKrMWgnJrOlm5ltZnzEx8gKEy3htQjJMY/chpJ2qSmXEnBexmV112/BsfdlVderG11XrgePc/RYz2xRYZmbXuftvuzAWERER6VPdyK5aBaxKf/+Lmd1JMuOxLnJERES6QDE5FTCzKcBrgJtynnuurMMjK/+n7qGJiIjICNe1ixwz2wT4EfApd38s+3xjWYeJ272t/gGKiIjIiGbuXn+nZhsB/w381N2/XrT+zJkLOh7kpRc80rT8j/Nf2rJOncGG2WDMTVav7riNssaXFxiaHU+d08bHzI6aN5bVM2Y0LU9esqTjdvPE7kNe8GWnqmo3Vlnj6bXp8rP7lTeeul7zPNkA/rySF2unTi1cp6pg9zrLMfRaeZQYIcdv6dL5Vtd4AB575OHaLgY2m7hFbftW+50cMzPgHODOkAscERERkRjdyK56E3Aw8BszW54+9pl0FmQRERGpWZ0FOuvUjeyqG4Fab8OJiIjI6KOyDiIiIqNcrQU6a9S3FzmvueiDTcubTG0N9J20fHnTcpWzIpcRkFjnDL4xs8TmbZe3TjaIMu/YFB2vvPFtedddTct5gaHZ7fICwGNmKC1r9taqZmqNFRLwGmPFrFktj01ZvLiUtmNk9zN7LuWpcybbkESF+9/RHHi85RnF+zAwe3bLY9mA/W4G8VY5U3HIrODZz5CQz/GQ8fR6YHQ/6dpFjpmNBZYCK919n26NQ3pPN7OH6jRSpnuvQzcvcESqNFIuaPr1Tk43JwP8JHBnF/sXERGRPtaVOzlmNhl4N/Al4NhujEFEREQS/Zpd1a07Od8EjgeGPKqNZR3WrLmhvpGJiIhIX6j9To6Z7QM85O7LzGz2UOu5+wJgAcTNeCwiIiJh+jUmp/ayDmb27ySTAa4HxgObAZe5+0FDbVPVRc4Hf/KnpuWL3r1N4TaxmSxF08bnlVooK5MlxD1z5jQt77RoUeE2IdPaz1+0tmWdBXO27HB0rUbiVO5liSlBIMPXS6Uoeu38jxlP7D6ElMip6rWq6rjntbv8xkNqnU9u9f0DtV0MTPq7KbXtWzcmAzwJOAkgvZPz6XYXOCIiMrLoInvk2bChP+/kdDO7SkRERKSFmR1jZneY2e1mdrGZjTezHc3sJjO7x8wuMbONi9rp6kWOuy/WHDkiIiLdteHZZ2v7KWJm2wFHAzPdfRowFjgA+DLwDXffCXgUOKKoLd3JERERkV4zDnihmY0DXgSsAvYEfpg+fz6wb0gjI05I4FnIdNzZQOMzf/FEyzqHz92usJ0QRdvlBRmvnZqZpj1gqvlYMTPO5u1T9rVZMKdllaDXL6usQMLYchXdkheQnj1e2fMEqj1XYhS95nUG0mZLOEDr+y8kqL7XhLxHqipZkrfO7Qcc0LQ8beHCjvvOkz13qnytYso6ZKnMQyszmw/Mb3hoQZpRDYC7rzSz04A/An8DrgWWAevcfX262gqg+Q90jm5NBjgB+A4wDXDgcHf/ZTfGIiIi5cpe4Ejvq3MywMYpYvKY2UuAucCOwDrgUiDnv8zFunUn53RgkbvvlwYOvahL4xAREZHe8jbgD+6+BsDMLgPeBEwws3Hp3ZzJwMqihroxGeDmwO7APAB3fxp4uu5xiIiISKLHJgP8IzDLzF5E8nXVXiQFvX8O7AcsBA4FrixqqBuBxzsCa4Dvmtmvzew7Zvbi7Eoq6yAiIjL6uPtNJAHGtwC/IblWWQCcABxrZvcAWwDnFLXVja+rxgGvBT7h7jeZ2enAicD/blxJZR1ERETq0WN3cnD3U4BTMg/fB+zWSTvduMhZAaxIr9QguVo7sd0G2Qj3kGycGNlMKoD9v3dn0/Jl79uxZZ2qIuNDMiFiMirKykzKaydbHmLGeee1rBPz+lV1jKvMaig6XiFZISEZbLHvhzrLFMS0XdW5HfK+qiqTqqwssrxtslljeRmbIedg9vwKaSdr2sKFLfta1nmblfdalXVul3EejLbMqV7TjbIOq83sATPbxd1/R/Jd22/rHoeIiFQjJlVduqvO7Ko6dSu76hPAhWlm1X3AYV0ah4iIiPSprlzkuPtyYGY3+hYREZFmvRaTUxaVdRAREZG+ZO71Jy6Z2THAh0hmO/4NcJi7DxmdVVV2VUxw2j9O/1nLY5ffumfHfZcxXfhIUOd0/SHqDLYtkhf0GTPde+y09nft21z2ZeoVVxRu0w9GYsmGEHUmGPS6ZR89vGl517PP7dJI4i1dOt/q7O+uZTfXdjEwddfdatu32u/ktKkuKiIiIlKabgUeD1YXfYakpMODXRqHiIjIqLfh2f7Mrqr9To67rwQGq4uuAv7s7tdm19OMxyIiIjIc3fi6qrG66EuBF5vZQdn13H2Bu89095lbbbV73cMUEREZNTZseLa2nzp1I7vqueqi7v4McBnwxi6MQ0RERPpYN2JyhqouGq3ODJ68TKoj3vHLpuVzrn1DyzrZKdezU5rHZtqUlaUVMkNpWdPPj1Zrp05tWs6b1j7meMW+5jstWtTxNnVmJlWV5dNrGXVlHb9ePz5lfcaElIfY5YqrwweWCvk7Utbfmn7IYBspulHW4SYzG6wuuh74NWkhThEREalfv04G2K0Zj/Oqi4qIiIiUplsp5CIiItIj+rVAp8o6iIiISF+q7E6OmZ0L7AM8lM5sjJlNBC4BpgADwP7u/minbYcEsIUoK9grG2j83h+valnnsve1DzQrawr2srYp69gMzJ7d8tik5cublrOBl6tnzCjcJkRskGB2u7xg0bxgxyITBgaGPZa8x+osSRDSVzbAGmDLu+5qWg4JvMwe95hjDq3lK/ICrusKBM1rN6bvst7Deed2tu3Y417GMc3bz5DxxIy5rNcmpq+y/qYNR7/G5FR5J+c8YE7msROB6939FcD16bKIiPSRXvijLQIV3slx9xvMbErm4bnA7PT384HFwAlVjUFERESK6U5OObZx98HvclYD2wy1oso6iIiIyHB0LbvK3d3Mhizt7u4LSOfPmTlzQW0l4EVEREabfs2uqvsi509mtq27rzKzbYGHYhtqDNxaP358UIBYnbNMXvWebZsfGA9f/9TGzy0ef+qTTeMZ9+STuePLrpOnjCC79ePHtzxW1mzKUxYvbmm3qK2YQGMoL4gzJvgy5HgVBRzmjTdvm2zbee00BnAPdTyreg9kA41jxQa9Zk294ornfs871/Meq0pZfZfVTvZcenzSpGEf9zKPZdFnedWvW8x7NoRmPa5H3Rc5VwGHAqem/14Z00hMZHqdgXAtFzg0X+BA63hC/tDmKSuLoOgCJ1b2AgeKL5b64QInT8x5m7cPRRc4kJ+hFtJ2GXr5AgfCzv+qlNV3We3kvRfLOu5lCPnPap39l9V3L17gKCanQ2Z2MfBLYBczW2FmR5Bc3LzdzO4mKdR5alX9i4iIyOhWZXbVB4Z4aq+q+hQREZHObdigOzkiIiIiI4ZqV4mIiIxyG55VdlVHhijr8FXgPcDTwL3AYe4+7Dnp84LnqgrsygswDck6Ov7Ux5qW/+lV1zctX3VHOd/ixZYyKGObPHnHq6ySFkXt5smWHMgLko2ZDj8m86zKrIzY4O1O+x6q/yIhAZ29EIzZTsiYH580qbCdkEDfXgxUbRTzuRjy2VCWvNehrKSNGL32+vWzuss6XAdMc/dXA78HTqqwfxER6YI666mJtFNrWQd3v7ZhcQmwX1X9i4iISBgFHpfvcOCaoZ5UWQcREREZjq4EHpvZycB64MKh1lFZBxERkXr062SAtV/kmNk8koDkvdy9souXqgK7yvqu+Qcr/1fzAxPgji/c3vTQrkdP7rjdXguMywsunDAwUErbMcqajXdg9uym5eysurFiyjzEygZjhgRiVvW+Cml34sV3tzz2yAdeUcVwgoKKQ8ac/byIPX69Hqh6z5xs+CXstGhR03L2WMR+lobMLp6d8Ttk5vBuHuM6Z3EebWq9yDGzOcDxwB7u/tc6++512QscEZGRKnuBI72vXwt01l3W4T+BTYHrzGy5mZ1dVf8iIiIyutVd1uGcqvoTERGROP0ak6OyDiIiItKXVNZBRERklOvXOzlWVYJTXlmHhueOA04DtnL3tUVtZVPIYyLRYyPns5H8sdkuMZksO1xyb9PyH9//8sJt8mQzDWJLGYRk/oSUO1g3ZUrTcvZY5LUbM619yHmSt05ds7Xm9Z19LGQsZU1ZX6WYMhhVlTLIlvaA1oy/2POrrgydvAyjkGOcfSzv3InJGstT9NlZ1jEu6/zPayckA6uq83Tp0vlWSkOBrr3ogtqmannHBw+pbd+qvJNzHkmg8QWND5rZ9sA7gD9W2LeIiHRJSJq39BZlV3XI3W8AHsl56hskaeSa4E9EREQqU/c8OXOBle5+q1n7u1VmNh+YD7DDDgey1Va71zBCERGR0adfY3Jqy64ysxcBnwE+G7K+uy9w95nuPlMXOCIiItKpOu/kvBzYERi8izMZuMXMdnP3jqLEQgK77tp336bl2Gn3ywpCjQlGywYaX3BjaxuHvLk56C0veG7S8uWF65Q1pX/2eJVV1iEm6DlPVUHF2eBuaD3uMSUbskHa0Hr88l67soIhs+3kHeOQc6fovMh7XaoK4g0p7VHVNPuxwcohiQsx53ZIuYM8RedXTBt57cQGQcec/2UF64ecO71YpqNfq5DXdpHj7r8Bth5cNrMBYGZIdpWIiIhIp+ou6yAiIiJSi7rLOjQ+P6WqvkVERCScAo9FRERERhCVdRARERnl+nUywMoucoYq62BmnwA+BjwL/MTdjx9uX3nR7DstWlS4XUyphTzZzJq8drLR9DER+NlMKoAzf/FE0/JRexTvQ0g2ToiQDIGyykWEZGuUlV0Sk8mVzaTKaydGt8szZPe9rPHUVToDwt7nVU3NnxXbblWZP7GZSTH7EVPaY2D27JbHsq/ntIULOx4LxL3mZZWQqOt8k5rLOpjZW4G5wHR3f8rMth5iWxEREamJYnI6NERZhyOBU939qXSdh6rqX0REREa3ugOPdwbeYmY3mdkvzOx1Q61oZvPNbKmZLV2z5oYahygiIjK6bHj22dp+6lT3Rc44YCIwC/hn4Ac2RBErlXUQERGR4ag7u2oFcJm7O3CzmW0AtgTWDKfR2KCtsoIf84JOi2SDlUOmms+b4v+oPQaalr928n0t6xz3pZcVtt3NwLc6+44pJ5AXQHzHEa9uWt71a613G2P2q6yAxJhA95B2uimvpETIe7iskiVZIWUJsmMuK4g3VlnJAjFiPm+nLF4c1Vf28zXvMzpmP8sKAO+l99Wgfs2uqvtOzhXAWwHMbGdgY0BlHURERAQAM9vFzJY3/DxmZp8ys4lmdp2Z3Z3++5KitqpMIb8YmA1saWYrgFOAc4Fzzex24Gng0PSujoiIiHRJL2VXufvvgBkAZjYWWAlcDpwIXO/up5rZienyCe3a6kZZh4Oq6lNERET6yl7Ave5+v5nNJbl5AnA+sJhuXeSIiIjIyLBhQ313csxsPjC/4aEF7r5giNUPAC5Of9/G3Velv68GtinqSxc5IiIiUpv0gmaoi5rnmNnGwHuBk3LacDMrDHeptayDmc0AzgbGA+uBo9z95k7bDomcD1FGqYVQIWUAslbMmtW0PHnJksJt8jKpls+b17Q847zzCtuJlT2GedOgTxgYaLtOldPuZ49pSPmPvL6y2VRrp05tWSebMVdWpk22r+zxzGunrGyO2AynMvoqax/y3ufZvso6B7PHJq/vmHZDMrtCtst7PUNKP2TlrZPNBq0qyy1P9vO1ztc8Zpu816FuG57tyeyqvYFb3P1P6fKfzGxbd19lZtsChRMKV5lddR4wJ/PYV4DPu/sM4LPpsoiI9JG86S5EInyA57+qArgKODT9/VDgyqIGqgw8vsHMpmQfBjZLf98ceLCq/kVERCRMnTE5IczsxcDbgY80PHwqySTCRwD3A/sXtVN3TM6ngJ+a2Wkkd5HeONSKjYFJO+xwIJr1WEREZHRw9yeALTKPPUySbRWs7skAjwSOcfftgWOAc4ZaUWUdREREZDisyrn40q+r/rsh8PjPwIQ0KtqAP7v7Zm2aAGDmzOC2V44AACAASURBVAUdD3LZGSualqcfv1PLOjFBbnUGXmZlA64hLMAuu59PX/DblnU2HN18U62sfQoJqCurr5BAx+yxyIsdyAvkLZIXYF0UaBkbPFqWkBIXWdnAbQgLiO8lMa9VWcp6zUPa6fb5VSTvdYj5LAhJGKnrMxrKS5JYunR+bl3Hqpz3pVNqm5h33smfr23f6r6T8yCwR/r7nsDdNfcvIiIio0TdZR0+DJxuZuOAJ2meDEhERES6oF8LdHajrMOuVfUpIiIiMkgzHouIiIxyvVSgs0x9e5Gz69GTM4+0Bn9lg05DAk5jg8hCgtGK1snOoBvabtbGh7yy5bHPfvThpuUvnD22sJ082fEMzJ7dsk7ILMMxYgKGY7bJExO4Ou7JJ4OCJqsSE4xZZ5BxWQGcWXUFGecpax9C2umlIOM8eedfzIzCsUHFMYH3IcHcvX7cR5sqY3K2By4gKaDlJAW4TjezicAlwBRgANjf3R+tahwjRTf/2El36DUXkV7Rr3dyqsyuWg8c5+6vBGYBHzOzVwInAte7+yuA69NlERERkVJVGXi8CliV/v4XM7sT2A6YS5J1BXA+sBg4oapxiIiISHv9ml1Vyzw56aSArwFuArZJL4AAVpN8nZW3zXwzW2pmS9esuSFvFREREZEhVR54bGabAD8CPuXujyUTHSfSmY9zZ1l09wXAAoib8VhERETC9GtMTqUXOWa2EckFzoXufln68J/MbFt3X2Vm2wIPVdF3tnTBmPmvbVknJrMmNnK+aLs6syXyAl6/cHZz2187+b6WdY770ssK286OceoVV3Q4unhlZLCFrlOGKrMwYjJHek2dWSoxr3k3y0PkqSpbqCqx772yymDEvCfKOjbZEj3Z8jxSniqzq4ykAOed7v71hqeuAg4lKZl+KHBlVWMQERGRYhs26E5Op94EHAz8xswGL1M/Q3Jx8wMzOwK4H9i/wjGIiIjIKFVldtWNwFCVRveqql8RERHpzIZnlV0lIiIiMmKMyLIOIcFpraULygkYywb35fW/durUlnWyJRmqCiIuK4A5L8h4h0vubVr+4/tfXthO3rHI9l9WaYWy9r2q1yYmqDJkVuQq28kKCVTNe49kgzzrCu4GuGfOnKblvLIiMf2XFcxd1rEIGU/ea1O0TmwwdcyM3tl9XzFrVuE6eeVuyiq1UNb7JitvzFKNbpR1+CrwHuBp4F7gMHcfeakfIiKSSyVLRp5+DTzuRlmH64Bp7v5q4PfASRWOQUREREap2ss6uPu1DastAfaragwiIiJSrF8nA+xGWYdGhwPXDLGNyjqIiIhItNrLOjQ8fjLJV1oX5m2nsg4iIiL16NcCnd0o64CZzQP2AfZy90ouYKrK3gjJYKhziu6qsmjyZLOprrzkzy3rzH3/5k3LeZlT2f6z0+OHZHOEZPCEbFdluYMyslSqKiOSJ6RMQUi7Ice0zpINedlUMULO06LPnZDzdtlnprWss+u/3d60fPsBB7SsM23hwpbHsqrKOiqr3azJS5aU0k6skVjSRZrVXtbBzOYAxwN7uPtfq+pfREREwvRrTE43yjqcAbwAuC6tSL7E3T9a4ThERERkFOpGWYerq+pTREREOtevd3JU1kFERET6Uu0zHjc8fxxwGrCVu6/tpO06g7ZWz5jRtJwXVBwS5FxGIHRISYk6j002yBjgH6f/rGn58lv3bFkneyxiAnJjA4arOj55Ad9F+xUS6BsSSJ53XsQc05BA2jxlHNOQchFlie2rroDqbJBxnrwg45iyIbHnYF2vTb8G6Pbifiq7qnODMx7fYmabAsvM7Dp3/216AfQO4I8V9i8iIiKjWO0zHgO/Bb5BkmF1ZVX9i4iISBjF5AxD44zHZjYXWOnutxZsoxmPRUREJFqtMx6TfIX1GZKvqtrSjMciIiL1UBXyCDkzHr8c2BG41cwGgMnALWbWGv0mIiIiMgy1znjs7r8Btm5YZwCY2Wl2VYiQaPV75sxpWs6b/j2kRENMZHxMdH2VJQjWTZnStJxXjiFENpvqyPf8qmWds378urZtVJnNUVZWQxnthGRAhbQbk0kVqoxzO6+dmPIa3T4vyuiryvdwzPiqzBgLycwr6isvczAmO7OszNTs5yTEfVb2QjbVaFH7jMfurskARUT6WMwFjnSXK4W8M21mPG5cZ0pV/YuIiMjoVnngsYiIiPS2MWP7swBCf+6ViIiIjHpdKetgZp8APgY8C/zE3Y/vpO2ygkfzAo077Tuv/7x1so+FBPxlg+WqCoaE6oJX84KMr7nyiablvee+uGm5rLIYVQaqltFOyJT6ebLnRUhZh7KOaV5fMcG0MduUFQCbd15kxxMTPJ0n5n0eG5wcM568YxrzuVPVey2vjZDXqqzPi+xjsQkZMX3XbczYttElI1btZR1ILnrmAtPd/Skz27ptKyIiMqIoe0h6RTfKOnwYONXdn0qfe6iqMYiIiEixMWP6805O7WUdgJ2Bt5jZTWb2CzPLnTRFZR1ERERkOGot6+Duj5nZOGAiMAt4HfADM3uZuzeVblBZBxERkXooJidCTlkHgBXAZelFzc1mtgHYElgT2m5eIGE2GC07M2VeQGfM98Yh24QE6oUEcFY1O2qVAblZa6dObXnsLR9p3q/fnbG0ZZ1dj57ctJw3vmzgbkhQZVkzO2fl7eeWd93VdptNVq9ueS1CZlRdPWNG0/LkJUtatok5T0POi6pmGC6z7aJ2ywrCLisgNyY4Oe/zI3uehAQihyjreJXVd8x2ee0snzevaXnGeedF9VWGcU8+WdrrJc1qLeuQugJ4K/BzM9sZ2BgYVlmHKqdKr0s/7ENZshc4efIyk0aakIyKqrI5es1oCVQdLfspnemFC5x+jcmpvawDcC5wrpndDjwNHJr9qkpERERkuLpV1uGgqvoVERGRzvRrTI5mPBYREZG+pNpVIiIio1y/xuRYVeEwQ5V1MLMZwNnAeJJZkY9y95vbtdXrKeQh06BnA2VjAkrzAlWzfZdVnqGq0hQh283f+96WxxZc8/LCvnpdNgMr7xzo9cDUsso61CmmrENdY4HuvuYhwe+9fk7WeUzr7Gvp0vm1XnV845MH1/Z39pjTv1fbvnWjrMNXgM+7+zVm9q50eXaF4xAREZFRqBtlHRzYLF1tc+DBqsYgIiIixRR4PAyZsg6fAr5qZg8ApwEnDbGNyjqIiIhItMovcrJlHYAjgWPcfXvgGJIJA1u4+wJ3n+nuM7faaveqhykiIjJqjRljtf3UqRtlHQ4FPpn+finwnU7bzQY/5gUNZgN9QwJyY4PwQoIWi8pOQOsYQwLaYgKNY4PnQvYzJDg5W5Zg0vLlTct5QcY3LdnQtPz6WcXX5yGB2nnjCzkW2bbzAnKzr022zEPI+RYyE2pIeYE8IVPzVzV9f53BwFWVpogJwo7tu6rXqteDimMVfcbEqjOgebQzswkk1wfTSMJcDgd+B1wCTAEGgP3d/dF27VR2J6dNWYcHgT3S3/cE7q5qDCNJWVlRIiIinRoz1mr7CXQ6sMjdpwLTgTuBE4Hr3f0VwPXpclvdKOvwYeD0tBr5k8D8CscgIiIiI4iZbQ7sDswDcPengafNbC7PZ2OfDywGTmjXVrfKOuxaVb8iIiLSmTqzq8xsPs03OBa4+4KG5R2BNcB3zWw6sIwkzGWbNHMbYDXJPHxtacZjERERqU16QbOgzSrjgNcCn3D3m8zsdDJfTbm7m1nhBIa6yBERERnleqyswwpghbvflC7/kOQi509mtq27rzKzbYGHihqq7CLHzMYDNwAvSPv5obufYmY7AguBLUhuQR2cft8WLCTrIiYKvspMg+yYY6Lp68yoqPJYxGQ6ZLOpstlWeevkicmyC8leigkcLyujLVa2/zqnrK+qHEm3FZ07ZR3jZWesaHls16MnF24XUoKmrMy8TtuIbSdvm5DPmJAM3JBszBWzZjUtT16ypLBvac/dV5vZA2a2i7v/DtgL+G36cyhwavrvlUVtVXkn5ylgT3d/PE0lv9HMrgGOBb7h7gvN7GzgCOCsCschIiIibfTgjMefAC40s42B+4DDSDLCf2BmRwD3A/sXNVJl4LEDj6eLG6U/TpI2/sH08fOBz6GLHBEREUm5+3JgZs5Te3XSTqUzHpvZ2DR9/CHgOuBeYJ27r09XWUFSzypvW5V1EBERqcGYMWNq+6l1v6ps3N2fdfcZwGRgN2BqB9uqrIOIiIhEqyW7yt3XmdnPgTcAE8xsXHo3ZzKwstP2QoLeHvraL5qWdzhiesftDtV2jJgyE1nZADdoDXIL2YeQ6ehjj0UZxyuv7+w07a+f1RrcN3/R2qblM/dtDcSMCSqusjxEzPiqktdX9rwtqwzGSAw0jjl3On0+VEiQcZ6Y0iyxwclFqjy3Q/Yh5BwMOV5FgcYhn6W9UF6jB2NySlFlWYet0toTmNkLgbeTTMv8c2C/dLWg6GgRERGRTlV5J2db4HwzG0saEe3u/21mvwUWmtkXgV8zRBVyERERkeGoMrvqNuA1OY/fRxKfIyIiIj2gxyYDLE29Yc4iIiIiNVFZBxERkVGuXwOPu1HW4UKSCX6eAW4GPuLuz3TS9tqpzZnoW951V8s6Wx+3R+aRckpBhJRjyGsnO+ZsZH9eu9nH8qL4s+tUOVV6Ud+xfYW0ETJV+oI5WzYtv/qHrefFb/ebUsp4sq/fuimt7U4YGGjbbpXZfCHvkZhzuc7SD1mxfYdk2pSVURSSdRcjJkMyRDZ7Drqb+VZWdmFIOwOzZzctT1m8uGWdMrILy/q7InG6UdbhQuCgdJ2LgA+hGY9FRES6RndyOjRUWQd3v3pwHTO7mWSuHBEREZFS1VrWoaFsOundnYOBRUNsq7IOIiIiNRgzxmr7qXW/qmw8W9bBzKY1PH0mcIO7/58htlVZBxEREYlWd1mHOcDtZnYKsBXwkZj2skGUVU07Hiqkr2xQW0igWUyphbyAxKJ2Y5UVnFxGyYs8eUHG8/a8o2n5vJ+9qpS+ioKM88QGwBZtA/mBxkX9x5b7CFFVQHpMeY0qA5jLem8Vie2n18trlPW5HfKahyQzxByfkIDmLJV1qE7dZR3uMrMPAe8EPuDuG6rqX0REREa3bpR1WA/cD/zSzAAuc/cvVDgOERERaaNfZzzuRlkHTUAoIiIildMFh4iIyCinmBwRERGREaT2sg4Nz58BHO7um1TR/+0HHNC0PG3hwsJtYks2hIiZ7j1mm7x1ypoqPUZsRkxWSAZWSLvZbKqJF9/dss4jH3hF0/LqGTNa1pm0fHnT8opZs1rWCcneyCoryydG3vEKmdY+RlnlD2K2C8kiiynhkLddVe+12M+qOjPN6pItYQKt48nLfqyqlEI2m6rXSmcMRTE5ncst6+DuS8xsJvCSCvsWERGRUa6yr6s80VLWIc22+ipwfFV9i4iIiHSjrMPHgavcfVXBtirrICIiUoMxY622n1r3q8rGc8o67A68D/iPgG1V1kFERESi1V3W4a3ATsA96USALzKze9x9p+G0v27KlJbHYgKNqwyeu/gLzdPsH3z8lMK+Y4Iqy9qH2IDEEDHBmTGBeiH7kA0yhtZA45ASCSFBxmWdbyHBo7d+bLem5V2/Fnc3tKwAybICjcsQu091BRWHCCkJEttOLwkJEs97f1aZRNKpvPMtm6gQk6RQtn4NPK67rMMyd5/k7lPcfQrw1+Fe4IiIiIjkqb2sQ4X9iYiISIQxY/tz2rzayzpk1qlkjhwRERERlXUQEREZ5fq1rMOIvMjJBhqHBJ6FzP5Zpb3PndO0POnJ5UOsWb2QgNyyjk3ebJ9l9JW3DzHBrHntZM+nZYv/2rLO62d1fms3Zj/zjl/IfsYEGucdi2wQdnamZwjbr6Jg6dhA2pC+Y2ZtDgl4jR1PjJDA7bLOr5DA7Lo+O2MD1HspoDrvXMp7H0k1ai/rYEla1RdJUsmfBc5y9zOqGoeIiIi016/ZVbWXdQD+HtgemOruG8xs6wrHICIiIqNUlYHHDrSUdQCOBD7o7hvS9R6qagwiIiJSrF9jcrpR1uHlwPvTkg3XmFnrbGyorIOIiIgMT91lHaaRxOg86e4zgW8D5w6xrco6iIiI1GDMGKvtp051l3WYA6wALkufuhz4bqfthUyzn1XlNN8hmSLZaPqQ0gEh0/fHrFNn6Ye8khtlTGFe5WuVlZdJ9e+fvrtp+aTTcm9Idiw7vrLKKoTIOxZlTTcf83plt8nLUgkRcwxDtgn5TKmqlEdZyjq/YsrA5L2eIZ9VIcc023be59CEgYGm5ZBMrpj97GYJE6k2u2or4Jn0AmewrMOXgStIalj9AdgD+H1VYxARkfr1Ugq3hOnXmJzayzqY2Y3AhWZ2DElg8ocqHIOIiIiMUrWXdXD3dcC7q+pXREREBEbojMciIiJSHk0G2KE2Mx7vBXyV5Cusx4F57n5P2f1nA3tDAihDAgnzhHz/nA18q2pa7yqnx4/ZJuS4r506tWk5LxgyJCAxO0V9SFBlyPT9eetkA43nL1rbss6COVsW9l8ktrxAWao6d2ICcqvc75j9zAtmzSYQ1BmbEvP5FbIPIX3F7GdZJRvy9jv7vsn7vI05XiH7mW037z1cZ0LBaNeNGY/PAua6+51mdhTwL8C8CschIiIibSjwuENtZjx2YLP08c2BB6sag4iIiIxelcbkpJlVy4CdgG+5+01m9iHgajP7G/AYMGuIbecD8wF22OFANCGgiIhINfo1JqcbMx4fA7zL3SeTTAT49SG21YzHIiIiEq3uGY/3BqanNawALgEW1TEGERERyaeYnA61mfF4czPb2d1/nz52ZxX9h2T1ZKPeQ6L9sxk8edvlReBnpxCPUeWU8DHZEiFTnN+1774t60y94oqm5Zhsjjwx07KHbJO3TvbcWTCndbszf/FE0/JRe7y4kvFVqarsoKrOybKE9FXGezpPSEZdNnsUwjI2s8c0772XzXYsq+RMTBZl7Ps+JHsp5DWO+RuRbTdkLHl/V6Qc3Zjx+MPAj8xsA/AocHiFYxARkZqprMPIozs5HWoz4/HlJIU5RURERCqjGY9FRERGOWVXiYiIiIwgld/JSWNylgIr3X0fM9sRWAhsQTKHzsHu/nQnbcYEg+XJfm8cEvCXF0SWDajLC0iMKTlQVWBcWUJKSIQEFWflBVWGBHmWVRogJOA1pK9soPHEi+9uWn7kA82lIULHF6KsoN1sO3mBoNkxhwShZoUE9JclZnxDbdepkM+YkP2OeV+FyrZdVmmPbLshr3le32WVyIkJaq6qzEovxDD1a0xOHXdyPklzBtWXgW+4+04kgcdH1DAGERERGWUqvcgxs8nAu4HvpMsG7An8MF3lfKA1x1hERERqM2bMmNp+at2vitv/JnA8sCFd3gJY5+7r0+UVwHZ5G5rZfDNbamZL16y5oeJhioiISL+p7CLHzPYBHnL3ZTHbq6yDiIjI6GRmA2b2GzNbbmZL08cmmtl1ZnZ3+u9LitqpMvD4TcB7zexdwHiSyuOnAxPMbFx6N2cysLLCMYiIiEiBHg08fqu7r21YPhG43t1PNbMT0+UT2jVg7l7lAJNOzGYDn06zqy4FfuTuC83sbOA2dz+z3fYzZy5oGmRV071XFTkfKyQjK+RYVFWyIWS7ssbTa8rYh3l73tHy2Hk/e1XH7cSet/3wOsQo630e+x6pS6+PL1bIeXv7AQc0LU9buDCqnbrkvVbLbzyk1quOW2/8fPUXA6npbz6lcN/MbACY2XiRY2a/A2a7+yoz2xZY7O67tGunG/PknAAca2b3kMTonNOFMYiIiEhqzBir7acx5jb9mZ8zJAeuNbNlDc9v4+6r0t9XA9sU7VddVcgXA4vT3+8DdqujXxEREekt7r4AWFCw2pvdfaWZbQ1cZ2ZNEy25u5tZ4d0nlXUQEREZ5XotJsfdV6b/PmRml5PcHPmTmW3b8HXVQ0XtqKyDiIiI9Awze7GZbTr4O/AO4HbgKuDQdLVDgSuL2upGWYcLgZnAM8DNwEfc/Zmy+40poxASfJg3FXlI21khQW8h4+n1QMK8IM+s7DTteVPWx5SvyHutQtrJvp4hAZx5fWXXyfaVF2T83h+valq+6j3btqyTVVZZkzzZfc97PWPO/7KEvJ7ZfchbJ/v6xb4fq1JnQH8vBeTmyY5nYPbslnV2WrSolL7qOha9cIx77E7ONsDlyfzBjAMucvdFZvYr4AdmdgRwP7B/UUN1fF01WNZhs3T5QuCg9PeLgA8BZ9UwDhEREelxaezu9JzHHwb26qStWss6ALj71Z4iuZMzucoxiIiISHt1ZlfVul8Vt58t6/AcM9sIOBjIva+osg4iIiIyHN0s63AmcIO7/5+8J1XWQUREpB5jxlptP3WqtayDmX3f3Q8ys1OArYCPVNX52qlTm5bLCo6MbScmgC27TuwMptl1QmZ8jQ2EiwlmzQs0zooJ+sx7rWJev5B9yhtfzDHMBhp/8n23tKxz+qWvLWynqiDUbgYZ5ykrOD8k2DwrL9h8wsBA4XYxypptPCs2kSKkr5hkgRh5xzzk9Yv57IxV17GQVpVd5Lj7ScBJ0FTW4SAz+xDwTmAvd2/5GktEREa2kKzK0WKkHIu6Y2Xq0o15cs4mSQ/7ZVpd9LNdGIOIiIj0uW6UddAsyyIiIj2kx+bJKY1mPBYREZG+pIscERER6Uu1l3VoePwM4HB336TTNrOBXHnZANmMndho++x2ee3ElBgIycIoKyI/ZFr7smT7yma55fW/esaMpuXJS5YU9hOTbQJh505ZQs65rGyJi9MvbV1n2ede2rS86+cebFmnrKyQstopOhZ1TmtfVuZNGeVcQvvKbpc9TyAsSzGk75DPvKJ26sweynsdQsac/dyZtHx5yzplfAaPlEwqBR7HGyzr8Bwzmwm8pIa+RUREZJSqvaxDemfnqyQzIYuIiEiX9etkgN0o6/Bx4Cp3X5W/SUJlHURERGQ4KovJaSzrkE4GiJm9FHgfMLtoe3dfACwAmDlzgVc1ThERkdGuX2Nyai3rANwBPAXcY2YALzKze9x9p04aDgn4KyugNBtQFxsgWVU5gbLaKasMQDZQLy8Y8tYTJzYt73J25wGTsUICj8sowREiJHg0L4DyVd/8a9PyxIvvblnnkQ+8orD/kP2MKQkSG1ybFRL0GfNaxSYPZJX1vgrZz6oCe/PGk02SyHuPxLyeZb1WIULazgYa553bde2nVKfusg77NK5jZo93eoEjIiIi5dJkgCIiIiIjSO1lHTKPdzxHjoiIiJRrzNj+vOfRn3slIiIio56KZYqIiIxy/ZpdZe752dlm9h/AkKnb7n50UAeZsg6WpFV9kSSV/FngLHc/o10bMSnkVWVmxLrykj83Lc99/+aF25SVjRCzX9kMC6i2BEKjsvYhpO2Qae1j2oWwrKOs7HhCjkXeOrd+5Z6m5V2PnlzYd5XKeK9VeV7EKGs8dX4O9UPmT8x7OHadqmTL3eSV+Vl+4yG1XnWsWfmN2qZq2Wq7Y2rbt3Z3cpaW1MdgWYfN0uV5wPbAVHffYGZbl9SPiIiIROjX7KohL3Lc/fzGZTN7kbv/daj18zSUdfgScGz68JHAB919Q9rPQx2NWERERCRAYeCxmb3BzH4L3JUuTzezMwPbzyvr8HLg/WnJhmvMLHfWMpV1EBERqceYMVbbT637FbDON4F3Ag8DuPutwO5FGzWWdcg89QLgSXefCXwbODdve3df4O4z3X3mVlsVdiciIiLSJCi7yt0fSMswDHo2YLOWsg5m9n1gBXBZus7lwHfDhxuuqqn4Y4UEGmfFjC92n7JBeFUGGRcF/FUZJJgN+MtO7R7adnY8VQW2xx6LbKDxI+fd3rLOZh+d2XH/ecG2x1/1YNPy198xsWWdon0PCeLNa2Ng9uym5SmLF7ftp0xlfX6ElBopS1WfeXUGhZdVTqOqcjf3zJnTtLzTokUt6+SVu5FqhFzkPGBmbwTczDbi+UDitoYo63CQmZ0KvBX4A7AH8PvIsYuISA8aiVlbo92oCzxu8FHgdGA74EHgp8DHhtHnqcCFZnYM8DjwoWG0JSIiIpKr8CLH3dcCBw6nk8ayDu6+jiTjSkRERHpAv04GGJJd9TIz+7GZrTGzh8zsSjN7WR2DExEREYkV8nXVRcC3gH9Mlw8ALgZeX9WgREREpD79GpMzZFmH51Ywu83dX5157FZ3nx7UQWtZh72Ar5LcRXocmOfu97RrI6asQ4xslgO0BtCtmzKlZR1FykuMbGYQxGUHxZbgeP9uv2havuTmPTruu05578+8rLYiZWUClTWefhVSWqdI7Gt1+wEHNC1PW7iw4767benS+bVedTzx2H/WVtbhxZt9vPtlHcxsMA/0GjM7EVhIUsvq/cDVHfSRLetwFjDX3e80s6OAfyEp9SAiIn0g7wJQelu/xuS0+7pqGclFzeCef6ThOSdND29niLIOzvMXPJuTZGyJiIiIlKpd7aodS2h/sKzDpg2PfQi42sz+BjwGzMrb0MzmA/MBdtjhQDTrsYiISDX6NSYnpKwDZjbNzPY3s0MGfwK2GaqswzHAu9x9Mslsx1/P215lHURERGQ4QgKPTwFmA68kicXZG7jR3fcr2O7fgYOB9aRlHYCfA1Pd/eXpOjsAi9z9le3aygYelxHQBnFTdscGwmW3CykLECNbtgCqC4wOCXitarr32GDb7HZVTqFfl5BjHLLOv3/67pZ1Tjott35uzyh6X0F3g4HL+qzKtrN6xoyWdUKC1rOJExMGBqLGM9JUWXYir+0y+qk78PiZp8+qLfB4o42PrG3fQu7k7AfsBax298OA6SSxNG25+0nuPtndp5Cknf8MmAtsbmY7p6u9nYASESIiIiKdCpkn52/uvsHM1pvZZsBDwPYxnbn7ejP7MPAjM9sAPAocHtOWiIiIlGM0ZlcNWmpmE4Bvk2RcPQ78m5S2XQAAIABJREFUspNOMmUdLiepPi4iIiJSmZDaVUelv55tZouAzdz9tmqHJSIiInXp1+yqdpMBvrbdc+5+SzVDEhERERm+IbOrzOznbbZzd9+zsHGzAeAvwLPAenefmc6kfAkwBRgA9nf3R9u1k82uymbI5GUslJW9VBQ5D3HZVWVF9scoK0Msb5vsOnnZXtmMjmw7dWU9hPaVV8qjjKyUvPHFHOMqs4eWnbGiaXn68Tu1rBObldhpG2UJOe4hx7TOTMaQc/nWeR9sWt717HMLt8nbh2zGYchr083Ps5ByPLHji8nGzL5WseU/lt94SK23VjZQT/kkgDHUlznWbjLAt5bUx1vdfW3D8onA9e5+alou4kTghJL6EhEREQECJwMs2Vzg/PT384F9uzAGERER6XNVX+Q4cK2ZLUvLNABs4+6r0t9XA9vkbWhm881sqZktXbPmhoqHKSIiMnqtd6/tp04hKeTD8WZ3X2lmWwPXmVnTF9bu7maWu8fuvgBYAK0xOSIiIiJFCi9yzMyAA4GXufsX0lIMk9z95qJt3X1l+u9DZnY5sBvwJzPb1t1Xmdm2JJMLtlUUFJgXVBYSnBwiJmAtNqC0LrF9xwQg5gXoZl+LbGBeSLBtbDDkwOzZhePLPhYSZBwTzJ33fEgZgLLOyZB2soHGv7iydTx7vbPzAO+QQNqqxJYEyaqzJELIa/WqhZc1LYcEvOa1GxK0W8brV1bAcMg+hPSf916LOS+yfY+U0jF13mHZuMaQ6pCvq84E3gB8IF3+C/Ctoo3M7MVmtung78A7gNuBq4BD09UOBa7scMwiIiIihUK+rnq9u7/WzH4N4O6PmtnGAdttA1ye3AhiHHCRuy8ys18BPzCzI4D7gf0jxy4iIiIlqDtWpi4hFznPmNlYkiBizGwrYEPRRu5+H0kxz+zjD5MU/BQRERGpTMhFzhkktaa2NrMvkVQl/5dKRyUiIiK16dc7OUPOeNy0ktlUkrsvRjKR351VD6yRsqtEetfSpfOblmfOXNBxG7Gzwq6eMaNpedLy5R33ndd/WbNIV9XuaNZLSRx5ynrNly6tb1ZggEfWn1nb39mJ447q/ozHg9Jsqr8CP258zN3/GLDtAK1lHb4KvAd4GrgXOMzd9c4XERHpkvXdHkBFQr6u+glJPI4B44Edgd8BrwrsI1vW4TrgJHdfb2ZfBk5CZR1ERESkZIUXOe7+D43LaXXyo2I7dPdrGxaXkMT4iIiISJf0YkxOmvS0FFjp7vuY2Y7AQmALYBlwsLs/3a6Njss6uPstwOtDV6e1rEOjw4Fr8jZUWQcREZFR7ZNAYwzwl4FvuPtOwKPAEUUNhMTkHNuwOAZ4LfBg4ABbyjq4+w1puyeTfA14Yd6GKusgIiJSj167k2Nmk4F3A18Cjk2rL+wJfDBd5Xzgc8BZ7doJicnZtOH39SQxOj8KGeQQZR1uMLN5wD7AXh6Q3hUTTd/NCPzYKfR7SZX70OvZEVn98HqWJS8LKptNdcGZr25Z55Cjbmvbbl4GSsh5EpJNlW1nxaxZLetMWby4sJ0YozmbqqhcS5XlZbr5GTOaX/NQ6Tc7jd/uLEhvbDT6JnA8z1+DbAGsc/fBGOkVwHZFfbW9yEm/D9vU3T8dMvDMti8Gxrj7XxrKOnzBzOakA9/D3f/aabsiItLb8i6GpbfVeSen8ZuaPGa2D/CQuy8zs9nD6WvIixwzG5dmQL0psu2hyjrcA7yA5OsrgCXu/tHIPkRERKS/vAl4r5m9iySrezPgdGDC4LUJMBlYWdRQuzs5N5PE3yw3s6uAS4EnBp9098uG2jB9fqiyDjvlrC4iIiKCu59EMr0M6Z2cT7v7gWZ2KUlG9kICC3yHxOSMBx4mCfgZnC/HgbYXOSIiIjIy9Frg8RBOABaa2ReBXwPnFG3Q7iJn6zSz6naev7gZFHQ08mY8bnjuOOA0YKvMZIGFQqbNjgk0e3zSpJbHNlm9uuN28vqOCYQra3rwmL6rDNTL7lfMMQ6RFzCcfaysIMGQ4OS8dYq2Kavv0O2K2gnZJi/I+IIbm9s55M3F7ZQlZh9ClBWQPjB7dtNyVUHQoco4T8evWzfiEgxuP+CAlsemLVw47HaVuDA87r4YWJz+fh9JAlOwdhc5Y4FNaL64ea7fDvrIzniMmW1PEohcWBpCRERGlrIuJKU+o7Gswyp3/0JF/X6DJMOq8Ps0ERERkRjtZjwuo0poy4zHZjaXZIrmW9tt2Djj8cOrf17CUERERCTPevfafurU7k7OXiW03zLjMfAZkq+q2mrMo5/x5gtGRESUiIiI9I4hL3Lc/ZHhNp4z4/EeJFXMb03nyJkM3GJmu7l7NdGnIiIi0tYIya7qWEgKeZShZjx2960b1hkAZhZlV8VMB77sM9Oalnf9t9sLt4nN8gnJIojJKMpm/sRG6deZsVM0lTu07nt2mv2YqfohLFOqqinX8zLzJgwMNC2XlW1SVjtlnRchstlUy47bvWl5+rdubtmmqgyUnRYtKqWdkNc8K+/4ZbOp8tYJmUE4JHMw+1heiYvs+y/k8yzbbmx2X0gmXFmfedm2YzOpyng/KlC7OpVd5DDEjMcV9iciIiIRdCenQ0PNeJxZZ0pV/YuIiMjoVuWdHBERERkB+vVOTrsUchEREZERy7zCq7ehyjqY2SeAj6WP/8Tdj2/XzsyZC5oGGRLoVdY6ZSkKLCur76pKB/Sa2P28a999m5anXnFFKf3XeUx7bbr8MsYzf+97Wx5bcM3LC7crq/RJjG5O11/W+7ybnxdllSPp9vmfVdb4li6dX8ZcdcFuePzrtd3K2X2TY2vbtzq+rmoq62BmbwXmAtPd/al0Dh0RERGRUnUjJudI4FR3fwqSOXS6MAYRERFJKSYnTktZB2Bn4C1mdpOZ/cLMXpe3YWNZhzVrbqh4mCIiItJvqr6Tk1fWYRwwEZgFvA74gZm9zDPBQY1lHbIxOSIiIiJFKr3IySnrsBuwArgsvai52cw2AFsCa6oci4iIiOTr16+rai/rADwOvBX4uZntDGwMtC3rsHbq1KblLe+6q2k5dkrssqYQz07vnleyIWTK9SIh4wvJIqtT3nT0ZWTAxGYsxEzpHzv9fNE2Zakzyyekr2VH79uyzvQF7Y97XibV9T9tbnevd8aV8sjKfp5A62dKiDrLpZSxTZ6q3p9QX0ZpWco6Fr22X6Nd7WUdzGxj4Fwzux14Gjg0+1WViIiMXKrFNPLoTk6Hhirr4O5PAwdV1a+IiIgIqKyDiIjIqNevd3JU1kFERET6UqV3cvLKOpjZDOBsYDywHjjK3W9u105RUGBZgV6x7eQFGmcVBbCtnjGj5bFJy5c3LceOr5uBcP3w3Xze8Vs3ZUrT8oSBgXoGQ1zQc512PSOuVEZWNtB43p53tKxz3s9e1XG7MUHGsXq9BEHIZ1eM2P0MOV4hbd9+wAFNy1MWL25ZJ/uZXGdJkF60vtsDqEjtZR2ArwCfd/drzOxd6fLsGsYhIiIio0g3YnIc2Cz9fXPgwS6MQURERFKKyYmTV9bhU8BXzewB4DTgpLwNVdZBREREhqMbZR32A45x9x+Z2f7AOcDbshuqrIOIiEg9+vVOTjfKOhwKfDJd5VLgO1WOoVflzZJcRhBgWbN2ljUza1mBjWXNKHzXvs2z8U5buDBqPNn9qnPW4W71M5RHzru9aXnivGkdtzEwe3bLY9lg0bwg42OvfaRp+evvmNhx3xAXIBw7A3kv6eZ5m6esvrPnTuznUMx5kU1KgHoTE6RZN8o6PAjsASwG9gTurmoMI0lVWQ4iItI9I+UCR3dyOjdUWYfHgdPNbBzwJDC/TRsiIiIiUbpR1uFGYNeq+hUREZHO9OudHM14LCIiIn1JFzkiIiLSl8wrvEVlZhNIsqemkcyZczjwO+ASYAowAOzv7o+2ayebQt7rU6V3U1lZSCFCsgiyWWTQ3SBrnTtD67VMmxjvfdX1LY9ddcdeXRhJYu3UqU3LISUlln308JbHdj373KblsrIoq1TVe63X38PZ1yb2dVm6dL6VMZ5QC9b+W23fV83f8jO17VvVd3JOBxa5+1SS+Jw7gROB6939FcD16bKIiPSJXqunJqNXlSnkmwO7A/MA3P1p4Gkzm8vztarOJ0klP6GqcYiIiEh7Cjzu3I7AGuC7ZvZrM/tOOl/ONu6+Kl1nNUmqeQuVdRAREZHhqPIiZxzwWuAsd38N8ASZr6Y8CQjKvXx09wXuPtPdZ2611e4VDlNERGR0W+9e20+dqpwMcAWwwt1vSpd/SHKR8ycz29bdV5nZtsBDnTacDTSrM2AypK8Vs2a1rDN5yZJaxlNlEF52v0L2KS/IOCYYsywxx6eq4OluB/qWFSCZ1c39ygsyPvlDzWUnPv/9mYXtxI43u+8x53Y2yDhPWa9VXgBz9nyP2Yeyjl9eO3W+R2I+X2Nem7zPGClHlZMBrjazB8xsF3f/HbAX8Nv051Dg1PTfK6sag4iIiBTr15icqquQfwK40Mw2Bu4DDiP5iuwHZnYEcD+wf8VjEBERkVGo6irky4G8e8Pdm7hCREREmvTrnRzNeCwiIiJ9qeqvq0RERKTHre/2ACpS6UXOEGUd/gl4D/A0cC9wmLsPK1WgqunC89oO6ausTKo6Mw1C+iprv7KlH3pdN8tQVCkmCySknEDeudPNqfi/9J1pTcsXnblzyzqHHHVbKX3F7Fc3sw3zzoGyMreyGUPZY5PXTzdLNORlONVVKqNfP2N6QTfKOlwHTHP3VwO/B06qeAwiIlIjpUSPPP06T05lFzkNZR3OgaSsg7uvc/dr3X3wztgSYHJVYxAREZHRqxtlHRodDlyTt7HKOoiIiNRDd3I617asg5mdTBLrdGHexirrICIiIsPRjbIOmNk8YJ//396dx8tRlfkf/zyyySIgEBYJ/C4KGBk0UQITkSUkLhEdgzOAKMg+EdyAcUTQGQRndCKgjI4/ZSIgoCgGwcCIQBAJy/wMEDDBsCoSMUAgbKKskjy/P05d0l1d3XW6qKru2/f7fr3uK6nqOnVOVZ+uW7f6OecBpib5q7oSE8S4ZPLkpuWsoNkiQcV1qqo9McGjZaUyyArmzmtPTLBf1n7T+8lqby8DYNOy6k6f914HJKbPV1WpH+p8H7KCjK+49Jmm5fdOTz90bm1z1mekSFB9TKBxum9nna+nhoa63m866Blaj6FIIHmd/TbmepYlfQwx1wsZeWpP62Bm04DjgT3d/dmq6hcREZE4gzoZYC/SOtwCrAVcbWYA8939qIrbISIiIqNML9I6bFtlnSIiItKdQX2So7QOIiIiMpCU1kFERGSUU1qHArLSOrj7r5LXPgOcDoxx98e62W/MyIzNFy7M3aZORUbNpEdLlDUFe8y5KGsUTVZd6dEQ6fcza0RDkVE+ZY26yGpPej/p9wqKjbRJn6+io9zKGr20bMKEpuWiqT3yZsCtM9VH1vu58+eaRxmtPtR6vtJtLGt0VYyYkaBFPrNFUyvEjGSU4NajDm9Zt9OZ5/SgJaNT1U9yhtM67JsEH68DYGZbAe8GHqi4fhEREcnRTzE5ZvZq4HrCIKXVgZ+4+xfNbBvgQmBj4Fbgo+7+Yqd91Z7WIXn5DMIw8v45qyIiItIPXgCmuPt4YAIwzcwmAV8FznD3bYEngSPydlR7Wgczmw486O6LOhVWWgcREZF69FNaBw/+kiyukfw4MIUwsTDAecA+efuqO63DycDngZPyCiutg4iIyOBpfIiR/MzI2GY1M1sIPApcDdwHPNWQ4HspsGVeXXWndTiZ8IRnUTIR4FjgNjPbxd2jI9fSgXFZgYTpgMmhefNatimSTqCoIoGf6faUFTydFZCbDhzMOqdF6o8JyC0SJJvVvvQxZB1nzDTt6fqXTprUsk26P5UVhFokMDrrfJXVV9IB/EWDuXuZQiUmJUJMCoR0PygSAJ4l5lzEBKSnP2sxwclZn8+Y97Ou4PeY61C/XbfT/W38uT8sqzmVqjMmx91nAbNytlkBTEgGMP0UaM1BEqGyJznJTcsfzeyNyaqpwG3uvqm7D7n7EOFG6G3d3OCIiEh/U84nKUsSy3st8HZgQzMbfjgzFngwr3wv0jqIiIhIH+mz0VVjgL+6+1NmtjbwLkLQ8bXAvoQRVocAl+btqxdpHRpfH6qyfhERERlxtgDOM7PVCN84zXb3n5nZncCFZvbvwK9JRm93ohmPRURERrl+mvHY3W8H3pqx/vfALt3sa2BuctIBYjGBhEUC1soKvIyRNfNuGTOLxuyjrMDQmODbrLrKCNgsGpCYrjsraD29TdZMwGUFVBeZ7TlGkfYVrbuXM+IWmR07Jjj5kO1bn5T/eMmeufspQ9b5TB9nVnByepus62QZbY7dR14fLNqWmBnmq3pvsvqbYpR6p9IEnWa2oZn9xMzuNrO7zOztyfpPJevuMLNTX2k9vU7ZUJfRMnV6WSNSBoEujjKoRkvfHi3H2a9qT+tgZnsB04Hx7v6CmW1acRtERESkg34KPC5TZTc5DWkdDoWQ1gF40cyOBma6+wvJ+keraoOIiIiMXrWndQC2B3Y3s5vM7Doz2zmrsNI6iIiI1KOf0jqUqe60Dick6zcCJgGfBWZbMv1xI6V1EBERkVei7rQOJyTrL3F3B242s5XAJoSnPqXp5RTiZaVE6GVwbdGRQDEpN/L2E5OmoOg5jknlUWSa/aLbpFU1kipLGVPWQzmjFKs8zpiRNjHnPX2+zrt3ess2dx/anC9wwrnndqwna78x5s5vHfA7ZXLzcszIxl4GxVZ5fSsr7URMP31sXHO2gfSItZjjzBoJV7dBjcmpO63DncAcYC8AM9seWBN4rKp2iIiIyOjUi7QOzwDnmNli4EXgkOSpjoiIiPTAoD7J6VVah4OqrFdERERkYGY8FhERkWL6Ka1DmUbkTU5V08+XFQhXVtBukWMoK7AxRi8DdLPEBBL2++yj/TaTc1nT7KdVFWicFRgdE4RaZDbxrGNIBxrPu2b/puXd3ndZ7n5jgrunTF6vZZsi153RMot60YD5mG1iUgjlGS3vQy9UepNjZhsCZwE7Ag4cDjwHnAm8mnDz+HF3v7nKdowE/fbLTURERg/F5BTTktYBmA2c4u5XmNnewKnA5IrbISIiIqNML9I6OLB+stkGwENVtUFERETyDeqTnF6kdTgWOM3M/gicDpyYVVhpHUREROSV6EVah6OB49x9K+A44OyswkrrICIiUo9BzV3Vi7QOuwHHJOsuIgQml66skTYxAcF503pneWpoqOsyMeoMYC5rJFdZ70PMNlWNYigrxUWRETJVpuAYaQHxVaaHSIs5X5Onzm5aPmyPG1vKfP/63ZqWq0wdk04fEJP6oU5F+naMsvpF1iitMupK/z6Q8vQircNDwJ7JuinAb6tqg4iIiIxevUjrcCnwDTNbHXgemFFxG0RERKSDQQ087kVahxuBnaqsV0RERGREzngsIiIi5VFahy4lsTg/blj1euAk4Pxk/RCwBNjf3Z/stK+8gM2s4LQ60wmkg4az2psOWCsr0LiXYoK5i5zjKt+7qoJ26+xvVQUMx5SJmR4/qz1LJ01qWh47f37XdZfl29c907Lu43uu2/V+ivSddJAxwEY/ag5LfOLD2+Xut2ggbTrQuMpg8yKftaoC72P2kw7KhtbzVVVge50B86NNlYHH97j7BHefQPh66lngp4QRVte4+3bANcmyiIgMiH7PESetBnUIeZXz5DSaCtzn7n8ApgPnJevPA/apqQ0iIiIyitQVk3MA8KPk/5u5+8PJ/5cBm2UVMLMZJCOvxr7hUDbefK+q2ygiIjIqDeroqsqf5CTDxz9AmPivibs7ITt5i8YZj3WDIyIiIt2q40nOe4Hb3P2RZPkRM9vC3R82sy2AR2tog4iIiLQxqE9y6rjJ+TCrvqoCuAw4BJiZ/Htp3g6qGnmRHilSVoR7VnvT6QSqqrtOWVORF5kmvshoiaKjfIro5WiTGGWNWskaXZI+pzH9NKuuoXnzcsvliRkJdOunW0P8dvrmnKblIiOpikq3L+sz89Jhzcf15U8ubNnmi99qTh0T0/+LtK8sVY6WSx97WalaepniYiRe/0eKSm9ykqzj7wI+1rB6JjDbzI4A/gDsX2UbREREpDM9ySnA3Z8BNk6te5ww2kpERESkMprxWEREZJQb1BmP65onR0RERKRWvUjrsCXwd8CLwH3AYe5eetTVE+cublre6NAdW7YpEuyVFfBXxhT/MUGVZU3BXrSutJhAvaoChOtMoxBzDDHbFGlP1n7TyjoXWQGcZdWflg7AjelLMfWkg4whLuC7SFB4TKB2ej8xx5kOMgZYMnly03LWexVzPUsf5++mTWvZZtyc5nNYNIA/7xwW3W+RQOOidcX00zIGFPTDDNGDGpPTi7QOVwM7uvtbgHuBE6tqg4iI1K/OXGQindSe1sHd57r78Nd/84GxNbVBRERERpG6bnIa0zo0Ohy4IquAmc0wswVmtmD58usrbZyIiMhopgSdBbVL62BmXyAEdF+QVa4xrcOYMXtU3UwREREZML1I64CZHQq8H5ia5K8SERGRHhnUwOPa0zqY2TTgeGBPd3+2yA5jotnXOXa31JpyBnBVmfqhSJkikf1Vjkwq0p6yRrmVNb17WtFUBnXVXeXIjKqmm69zCv2q+ntWf1s6aVLT8uYLW1M0FKk7nRbjg+N/2bLNRa/eu2k5q1+k38+i6TbK6O9lXc+Kiqkrpv4y2qxA7er0Iq3Dt4C1gKvNDGC+ux9VZTtERESkPT3JKaBNWodtq6xTREREBJTWQUREZNRTWgcRERGREaT2tA7u/p/J658BTgfGuPtj3ew7JkgrHZgaE4SXpazA3qoC6uoKjIsVkx4iPVV6kfQHWUGfZU3fHxPI28tAwTrfz/R7lXXey0hXkfVZ7OVxZrUn5noxdv78sprU0U8XTWlZd91Vze2b+p5iAelVpcGIUVYqmzo/53n9oqrUNmVTTE6X3P0eYAKAma0GPEhI64CZbQW8G3igqvpFRERkdKsrJufltA7J8hmEYeSX1lS/iIiItDGoT3JqT+tgZtOBB919UacCSusgIiIir0TlT3Ia0jqcaGbrAJ8nfFXVkbvPAmYBTJw4azBvMUVERPrAoD7JqTWtg5m9GdgGWJRMBDgWuM3MdnH3UqerTQeMlTUbbtEAtiLKCu4rUq6sgL8s6dluswLz8vZbdHbSqma/LXq+iihrvzHnNOa9KtKeXs4inaXIDMwxAaVliflcTX1P8/JJR61o2eZLZ67WtJzV3vRx9TLIvug1J6Zvx7xXZbyf/dbXR5ta0zq4+2+ATYdfMLMlwMRuR1eJiIhIedzX6HUTKlFpTE5DWodLqqxHREREJK32tA6p14eqrF9ERERGL6V1EBERGe1WrtnrFlRCaR1ERERkIPUkrYOZfQr4BLACuNzdjy+7/ipHUxXZJh3tn55GfpO77y5lv0XTVxSpu6xydY4+KGvEWno/ZaUlqHNK/XS5v2y+ecs26c9RVaOHRqIi52LZhAkt6zZfuLDr/WS9V+kRYiedu27LNge87Zqm5Z/c9o6WbfppNFDR1At1pkmoM/1IpfroSU6SFeF8YDPAgVnu/g0z24hwXzEELAH2d/cnO+2r9rQOZrYXMB0Y7+4vmNmmHXYjIiIio8tLwGfc/TYzew1wq5ldDRwKXOPuM83sBOAE4HOddlR7WgczOw2Y6e4vALj7ozW1QURERLL00ZMcd38YeDj5/5/N7C5gS8IDksnJZucB88i5yak9rQOwPbC7md1kZteZ2c5ZBZTWQUREZPA0/n5PfmZ02HYIeCtwE7BZcgMEsIzwdVZHtaZ1aKhzI2ASsDMw28xe7948p7TSOoiIiNSkxic5jb/fOzGz9YCLgWPd/ekkU8LwPtzMcu8Nak3rkCwvBS5JbmpuNrOVwCbA8tgdFg1GK0PRqdzTbc4KNE5LBxdmBVOnj7PKoLcyAmmz1hUJ4IxJo1B0mxhVpYcoK3VGkbpjgvUfGzeuZV1MX64qCLsqZaWvSJcpEmScpejAinSg8ReOXNyyzb/+ZLem5aLB5lW9fzH7iWlz+j0uepxlpIEZscHKFTKzNQg3OBe4+/CEwo+Y2Rbu/rCZbQHkhrvUmtYhMQfYC7jWzLYH1gSU1kFEZEDUObpJStJHMTkWHtmcDdzl7l9veOky4BBgZvLvpXn7qvQmpyGtw8caVp8DnGNmi4EXgUPSX1WJiIjIqPUO4KPAb8xs+PHn5wk3N7PN7AjgD8D+eTuqPa2Du78IHFRlvSIiItKFPnqS4+43Atbm5and7EszHouIiMhAUu4qERGR0a6PnuSUqfa0DoTJe84EXk2Y1fDj7n5zN/tOjzpKT2cOxQLfyppCP6vuIqMh+i2VQVmjhX43bVrT8tC8eU3LZU0rX9UIqFhFznt6xEfRKeur6jtZn7UYVY00ixEzSjHmvUqvKzrSsogiowJjzt8pP5jYsm72L5rPz8G7FbuWlhF8HJO+IkuR9Ch1Srdn6aRJPWrJ4Ks9rQPwXeAUd7/CzPYGTmXVDIYiIjLCaXTVCDSgT3Lqisl5Oa0DIdnW+sn6DYCHamqDiIiIjCK9SOtwLHCamf0ROJ1VMyE3UVoHEREReSUqv8lpSOtwUbLqaOA4d98KOI4w4U8Ld5/l7hPdfeKYMXtU3UwREZHRa+Wa9f3UqBdpHQ4Bjkn+fxFwVrc7jAniLRK0mBXklvbU0FDLurHz57/iurP0235ixAT8pdMAlDEtepas/abTEmQFMVaVWiFmGvki09EXDQaOUVfwb9bnqqzjKut6kX7Pi8SdZF1jigxKiAl6jjmmrPO+//ub27P3nNbUD7/cZ2zH/a7+/PNRQfR5igQZx9ZVNDVGFbLTfbyl9nYMol6kdXgI2JMwymoK8Nsa2iAiIjXJugmTPrdiMAOPe5HW4R9CpP3aAAAgAElEQVSBb5jZ6sDzQNsU6yIiIiJF9SKtw43ATlXWKyIiIl3wwXySo7QOIiIiMpCU1kFERGS0G9DJAKuOyTkOOJIwAeBvgMOALYALCV9j3Qp8NMlMHi0mcn7xAQc0Le944YUt26SD42Ki7cuatj1mpE2d0/eXJaY96RET6RFP2SMN8utJn6+skSwx0+Ont0m3D1pHiC2ZPLllmyLpKmJG8KTrrlJ69E3WeY/53KS3KStdRFnqmqG36IiedF/O6ksx6SvS71/WZyTdv7JGUt366X2alnf65pzc9lWl366BaTEj6vr9GEayyr6uMrMtgU8DE919R2A1wqSAXwXOcPdtgSeBI6pqg4iIiEQY0Hlyqo7JWR1YOxlJtQ7wMGHY+E+S188D9mlTVkRERKSwKhN0PmhmpwMPAM8BcwlfTz3l7i8lmy0Ftswqb2YzSIaXb731gWjWYxERkYoMaExOlV9XvRaYDmwDvA5YF5gWW15pHUREROSVMHevZsdm+wHT3P2IZPlg4O3AfsDm7v6Smb0dONnd39NpXxMnzqqmkT2UDuhcOmlSyzZZwdJ5ik5xXpV+b0/RtiybMKFpOSZYOq8tRdtT5zmOSSdQlSpTP1SlrP7W7/5p7hMt677+7o06lsnqtzHnK+YcpgcCPD753pZtxs9sbnPMYIYsVb2nCxbMsEp23IZd8v9q+z3rf79rbcdWZUzOA8AkM1vHzAyYCtwJXAvsm2xzCHBphW0QEZGa5d3giNSlypicm8zsJ8BtwEvAr4FZwOXAhWb278m6zCzkIiIiUpMBjcmpOq3DF4Evplb/HtilynpFRERENOOxiIjIaDegT3IqCzwuUzrwOCY4rUjA30gMku23wMZ+a89IFxPoW7TfVvVeZbU5ve+y2lfkGLKC/MfOn59bTrpz6gnrNy0fP/PpUvYb857HzP7c72oPPJ59e32Bx/u/ZSACjzGz48zsDjNbbGY/MrNXm9kFZnZPsu4cM1ujyjaIiEi90jc4Ir3Si7QOFwDjgDcDaxNyW4mIiEivrFizvp8aVR2TM5zW4a+EtA4Pufvc4RfN7GagNfubiIiIyCtU2ZMcd38QGE7r8DDwp9QNzhrAR4Ers8qb2QwzW2BmC5Yvv76qZoqIiIivWd9PjWpN62BmBzVs8m3gene/Iau80jqIiIjIK1Hl11XvBO539+UAZnYJsCvwAzP7IjAG+FjMjm79dHOi8vGzMh/+NCkyUqTfRpeUNQIl3Z4qp+FP119V6oKRMGor77zHnJuY9yo9kgTi0h1UdQ7TU+oDjJszp+v9pEdBDc2b17JNkWN4ZJfW8zW2h4Or0seZlSKkqvcqJrVCkevF8TOfbtnPoq80L+/0+WLHFHMuirQ5Jm1IzGd2JF6rgIEdQl7lTc7LaR0IWcinAgvM7EjgPcBUd19ZYf0iItIDMTmfROrQi7QOzwB/AH4VUlpxibt/qap2iIiISA49yelem7QOmmVZREREKqcbDhERkdGu5vlr6jIi0jpM2O38pkaWFcgVE9BcpK6swOMYRYKTy1LVFPq3fqZ1ZNxOX+s8JUBZgdtlKSt4etmECU3LWQGmRduTVtX5iQnOrEovj7tORYJbs7apU5H3ZsqcpS3bzD1g245lIG4gRcw2/Z76p/a0Dt97qr60DodtWNuxVfokx8yOI8xo7MBvgMPc/fnktW8Ch7v7elW2YaQYhIuziIiMUAMak9OLtA6Y2UTgtVXVLSIiIlJ7WgczWw04DfgI8MGK6xcREZE8Nc9EXJdepHX4JHCZuz/cqXxjWofHl11bVTNFRERkQFX2JCeV1uEp4CIzOxjYD5icV97dZxHm1WkJPBYREZESrVit1y2oRGWjq8xsP2Caux+RLB8MnAKsDQxH2W4N/N7dt83eSzBx4qyuG9nvkfPpqfizov/7bbREGekYYhQ97pE2nXrWKLIqU26kFRkR08s0HVWOuuv360VZMwjX2b4idX32o4ualk/7/vhC7bn1qMOblsef+8PcMmWNKC1L7aOr/ru+odb+Mavt2Cr7uoqGtA4WpjaeCnzd3Td39yF3HwKezbvBERGRkaXf/8CQ0aMXaR1ERESkj7xqZZ2pJOv7aqwXaR0aX9ccOSIiIlIJpXUQEREZ5WzFihprG5AnOVkzHgMvAP9OGGW1AviOu3+zm/3GTNldRNHvkYtMM77esmWF6kpLT7NfdIr9qs7pY+PGtazLS2dQNIix36a+TweXp+vudeBxkXNR1vkrsp86z02MrPcv73NdVt/ud0WPMx1ofMO8v7Zs89ozP9q0vOOFF7Zskw40Lnr+0uWyrmfpa25dAzQkTpVDyIdnPN7B3Z8zs9mEGY8N2AoY5+4rzWzTqtogIiIi+ep9klOf2mc8JjzF+Yi7rwRw90crboOIiIiMQr2Y8fgNwIeS2YyvMLPtsso3zni8fHnnrNUiIiJS3KtWrqztp9bjqmrHqRmPXwesa2YHAWsBz7v7ROC7wDlZ5d19lrtPdPeJY8bsUVUzRUREZEBV+XXVO4H73X05gJldAuwKLAUuSbb5KfC9CtsgIiIiORST072XZzwGniPMeLwAeBrYC7gf2BO4t9sdx4yyqHM0Qkx78kYvxYy0yYrAL2uUVlkjV9JtTI/+gvzRVVWO4ImZpr2sqdzz3pt+Gy0Uo9cjwqpS5D0u8tnr9SipOtMU5O07ZgTW7pPXaNnmB4ee3bQ8k3Vz91PWSMus97yMc9jrfjHIejHj8drABcnw8r8QhpiLiMiA0C/tkUdPcgpoM+PxC8D7qqxXRERERDMei4iIjHJ1j3qqS5VZyEVERES6ZmbnmNmjZra4Yd1GZna1mf02+fe1efvpRVqHdwCnEW6w/gIc6u6/62a/McFz6Sn1ywrQLSrvO+qiwdRFpgOvMt1Bej/j5sxp2SYdvJouE9OWotOgp/tFliJ9Jas96XUx7/GyCRNyt9nk7rublqsMnk4bhCDjomL6XFWxKGW9nzGpRdKy3vM6A5jTZp7bHGh8zVWtdf/tIUNNy1mf6SLHUKT/9zq9TKw+jMk5F/gWcH7DuhOAa9x9ppmdkCx/rtNOqpwnZzitw0R335GQkesA4DvAge4+Afgh8C9VtUFERERGHne/HngitXo6cF7y//OAffL2U/XXVcNpHVZnVVoHB9ZPXt8gWSciIiKjQGNGg+RnRmTRzdz94eT/y4DN8gpUOYT8QTMbTuvwHDDX3eea2ZHAz83sOcKcOZOyyicHPQNg660PRLMei4iIVKPOr6vcfRZhSplXsg83M8/brhdpHY4D9nb3sYTZjr+eVV5pHURERKTBI2a2BUDyb26C77rTOrwDGO/uNyXb/Bi4ssI2iIiISI4RMoT8MuAQYGby76V5BXqR1mE/M9ve3e8F3gXc1e2OYyLTqxpNlZWmYMMlS3LL5bUna5RDzHEWidLvdWR/L6dBT78PVaaQKLLvmJFTaUVHb6T7ckw/7qVej1Ipo66ix1DVcRYdLVfVZzgv/U2Wqe9pPac/O2frpuUDDq8mHUOMXl9vRyoz+xEwGdjEzJYSJhaeCcw2syOAPwD75+2nF2kdlgIXm9lK4Eng8KraICIiIvn6bQi5u3+4zUtTu9lPL9I6/DT5EREREamM0jqIiIiMcv32JKcsSusgIiIiA6nqtA7HAP8IGPBdd/9PM9uIMKpqCFgC7O/uT3az3yLBaWUpKzjzsXHjcrdJB6HGKCs4MysQush5zkqjkA7+LZKioWj7ygoCLCNtSFnvVVlB2DHt6WXwb9Z73ut0LXl6ea3KUlU6hpjPeV5bYtsTcww7zn190/KiWZe3bHPgjDG5dY0mI2R0VdeqnCdnR8INzi7AeOD9ZrYtq3JPbAdckyyLiMiA6PebTxk9qnyS8ybgJnd/FsDMrgP+njBB4ORkm/OAeeQk2BIREZHqKCane4uB3c1s42SunL2BrYjMPdGY22L58usrbKaIiIgMoirnybnLzL4KzAWeARYCK1LbtM090ZjbYuLEWbn5KURERKSYQX2SU/U8OWcDZwOY2VcIEwE+YmZbuPvDsbknqlJWEF6R/cQEFRfZb1mBhEUDJGPanN4mJjgzJgC2rPbFKCPmoNczofa6/m5lnfOqAmljxATOVhVoHDPzetHZ2Yuc0yKfh5gZj4vOJD5uzpym5QMvbA0ynvHe+5qWZ13xhtz9yshT9eiqTd39UTPbmhCPM4mQsLOr3BMiIiJSnUEdXVX1ZIAXm9nGwF+BT7j7U2bWde4JERERkW5V/XXV7hnrHqfL3BMiIiIi3VJaBxERkVFuUAOPldZBREREBlIv0jqcBvwd8CJwH3CYu3ccgpCO9i9rxEJZU9ZXNaJjpI1+yVLVyIyio5tiRmVVdd57OZqvrP0U/YwsmzChaXnzhQtzy8Qocuw3zPtry7rdJ69RS91Zlk6a1LQ8dv783DIxo6SKpqCpKtVDzMizqq7tWdKjqV48/86WbdY8eIdS2jMS6ElOlzqkdbga2NHd3wLcC5xYVRtERKR+WbmsRHqh9rQO7n5qwzbzgX0rbIOIiIjkGNQh5L1I69DocOCKrMKNaR0eX3Zthc0UERGRQdSztA5m9gXgJeCCNuVfTuswYbfzldZBRESkIoMak9OLtA6Y2aHA+4Gp7p57A5MXIBwzPXhMQFuVgZdFFDmGLL2c+j5Luj3p6eezUl7EfMefDkZOnz9oPfayzkWdAc39tp8Y6UDjXvbJrCDjdP8qGtie95nN6icxgcbpz8h9H1mvZZstf958OY8J7n5s3LiWdTEpZ/Kst2xZofe4rLQOMdfOdPuygowfOHtR0/LWR4zPrTuvLVntKZqmRvLVntbBzKYBxwN7DsfriIjI4NAv7ZFHT3KKyUrr8C1gLeBqMwOY7+5HVdwOERERGWV6kdZh2yrrFBERke5odJWIiIjICKLcVSIiIqPcoMbkWMTgpuI7z0jr0PDaZ4DTgTHu/lin/aSHkJc1EqOs0Uu9VNYolarORXo6fyhvSv8i+m2kWZ46R/PJ4Btp/T9LeqRZ0fQVRfzs4ne3rHv/P8ytpK4FC2ZYJTtuY+ig62ubqmXJD/ao7dgqe5KTSuvwInClmf3M3X9nZlsB7wYeqKp+ERERiTOoT3KqjMl5Oa2Du78EXEcYRg5wBmEYuSb5ExERkUrUntbBzKYDD7r7ok6FldZBREREXom60zqsBXye8FVVXnmldRAREanBoA4hrzutwyPAPsCiZCLAscBtZraLu7edR71IcFzMNO29DDQuK9C3rMDBss5FOrAxZor4IsGQtx51eMu6nc48p2k5Jq1DlfKmqM8KKk6/D1ntLSt4tM5UFGnpgPReBqPXKR00C8UCZ2/9Sut7t9Pn89+rkRhonFY05UYR6c9wVpDxqdf+uWn5+L1eU2mbpDu1p3Vw9280vL4EmJg3ukpERESqM6iBx7Wndai4PhERERGgB2kdUq8PVVm/iIiI5BvUJzlK6yAiIiIDSWkdRERERrlBHV3Vk7QOZvYp4BPACuBydz++034mTpylIeSR6kwD0G8pB2JGHb14/p1Ny2sevEOhuqpKg1HVtPsxo3piRqNlteeJcxc3LW906I5dt69oX+rlFP9Zirx/g5BqIUsv0+bEjK4tIqaflnWNqTutwxunX17b79l7Ln3f4KZ1ALYCpgPj3f0FM9u0qjaIiEj9sm6Ypb8NakxOlV9XvZzWAcDMhtM6TARmuvsLAO7+aIVtEBERkVGq9rQOwPbJ+pvM7Doz2zmrcGNah+XLr6+wmSIiIqObrVhR20+dKrvJcfe7gOG0DlcS0jqsIDw92giYBHwWmG3J9Mep8rPcfaK7TxwzZo+qmikiIiIDqtLA46aKQlqHpcAHgK+6+7XJ+vsIMyEvb1c2HXgcE6gXE5BYJOAv67vmmIC6MoIL+y3QtyxFAhTrTEkQc97TaQqgnFQFRftbWcoK4OxlEGpVlk6a1LJu7Pz5ldQVc/0o673qZSB0TN1VpTUp6zjTff2MI29p2eaz33pzbt11Bx7/zXsuri3w+I6r/mHkBx5DdloHYCWwF3CtmW0PrAkorYOIiIiUqva0DmZ2DnCOmS0mjLo6xOt6nCQiIiItNLqqgKy0Du7+InBQlfWKiIiIKK2DiIiIDKSBTesQE9hYJNAsa78xQZXf+EzzLJjHfK11Fsx+CiKOmSE3RkzQbsx7VWfgajqIOCaAOGabIkHERY/z7n32aVoeN2dOof2kg1eLBr8XOY5+D1YuI7Ac4gJg0+uy3od+Oz95ej2Qoqq60u/Diadv17LNU+OGmpY3ufvuStrSjUH9uqrSJzlmdoyZLTazO8zs2GTdBDObb2YLk3lwdqmyDSNFP93gSD1G2i8lEZGRphdpHU4FTnH3K8xs72R5clXtEBERkc4GNUFnL9I6OLB+ss0GwEMVtkFERERGqSpvchYDX06GkD9HSOuwADgWuMrMTid8XbZrVmEzmwHMANh66wPRrMciIiLVUExOlzqkdTgaOM7dtwKOA85uU15pHURERKSwXqR1+A9gQ3f3JGfVn9x9/U5lJ+x2flMjq5quP2YK8SwxIx/yyhTV71OcPzZuXMu6vOnmi44u6bfg7SLntKxp7Xs5NX+/+d20aU3L2155ZW6Zop/hvPMe029jtolJKREzmi+rrnR6iCKjKmOUdS6KSp+fmOtOVW6Y99eWdWuv94la0zq89e3fq21S3l//6rDajq3q0VWbJv8Op3X4ISEGZ89kkynAb6tsg4iIiIxOvUjr8I/AN8xsdeB5krgbERER6Q2NriqgTVqHG4GdqqxXRERERi4zmwZ8A1gNOMvdZxbZz8DOeCwiIiJx+ml0lZmtBvxf4F2EWN5bzOwyd7+zc8lWyl0lIiIi/WQX4Hfu/vskqfeFwPRCe3L3EfMDzKirXF1lBrUutW/k1NXv7dO50LnodV393r6R9kOIxV3Q8DMj9fq+hK+ohpc/CnyrUF29PtguT8yCusrVVWZQ61L7Rk5d/d4+nQudi17X1e/tG7SfMm9y9HWViIiI9JMHga0alscm67qmmxwRERHpJ7cA25nZNma2JnAAcFmRHY200VWzaixXV5lBrUvtGzl19Xv76qyr39tXZ1393r466+r39g0Ud3/JzD4JXEUYQn6Ou99RZF+1pXUQERERqZO+rhIREZGBpJscERERGUy9HirWxZCyacA9wO+AEyK2fzVwM7AIuAM4pYu6NgR+AtwN3AW8PaLMMcDipK5jO2x3DvAosLhh3WlJXbcDPyVkac8rczIh2nxh8rN3RJkJwPxk+wXALqkyWwHXAncmx3FMsn6/ZHklMDHjmDLLNbz+GcCBTSLq+nHDMS0BFsa8r8A2wE1J//gxsGZEmbOTdbcn7/d6EWUM+DJwb9I3Ph3ZvinAbUkfOQ9YPeM8rgb8GvhZsnwBoc8vTt7PNSLKnAvc33AOJ7Tph+lyU5P2LQRuBLZNbb8E+M1w34npF+3KdeoXHerK6xctn1lgI+BqQhLgq4HXxnzWgX9L+sRCYC7wupjrA/CpZN0dwKmRdY0HfpUc7/8A6zds/8aGY14IPA0cS4frRYcyJ9P5etGuXN4147jkeBcDPyL0/08SPoct722ncg2vfRP4S0wZ4IaGNj8EzMm7Lkf2i6xyef0i83dAp37Rpp62fUI/3f/0vAFRjQwX5PuA1wNrEn6B7JBTxkh+aQFrEH4BToqs7zzgyOT/a5K66cjYfseko65DCOb+BalfEg3b7gG8jeabj3eT/NIDvgp8NaLMycA/d2hTVpm5wHuT/+8NzEuV2QJ4W/L/1xB+ke8AvIlwEZxH9k1OZrlkeStC8NgfaL7JaVumYZuvASfFvK/AbOCAZP2ZwNERZRp/oXydhpvnDmUOA84HXpW8tmlE+3YF/ghsn6z/EnBExnn8J+CHrLrx2DvZnxEu6kdHlDkX2Deij6fL3Qu8Kfn/x4FzU9svofVmpGO/aFeuU7/oVCanX7R8ZoFTh99T4ARSn6sO5Rr7xaeBMyPK7EX43K+V1S86lLsF2DNZdzjwb22OeTVgGfB/yLletClzMh2uFx3Ktb1mAFsSbqjXTpZnA4cCbwWGOrz3meWS/08Evk/qJqdTmYZtLgYObljOvC7n9YsO5dr2iw5l2vaLDmWi+oR+4n5GytdVXU/x7MFfksU1kh/Pq8jMNiDcIJyd7OdFd38qp9ibgJvc/Vl3fwm4Dvj7Nu26HngitW5uUg7CX01j88rkaVPGgfWT/29A+MunsczD7n5b8v8/E/7a3NLd73L3ezrUlVkuefkM4HhS5z6nDGZmwP6EX+6N5dq9r1MIfyVD+GWyT14Zd3+6oa61G9vYoZ6jgS+5+8pku0cj2rcCeNHd703WXw38Q2M5MxsLvA84q2FfP0/254SnQ2PzysRoU65j38iS1y9yZPaLPFn9osNndjqhL0CqT3QqN9wvEus2trFDXUcDM939hWR9U7/oUG574Ppks5Z+0WAqcJ+7/yHvepFVps3r7TSWy+sXqwNrm9nqhF/WD7n7r919SU4dLeWSfEWnEfpFVJnhF8xsfcI1YE7D9u2uyx37RbtynfpFh7o69Yt2ZWL7hEQYKTc5WxL+Eh62lIZfiO2Y2WpmtpDwtc3V7n5TRF3bAMuB75nZr83sLDNbN6fMYmB3M9vYzNYh/MWzVU6Zdg4Hrojc9pNmdruZnWNmr43Y/ljgNDP7I3A6cGK7Dc1siPAXWcw5yyxnZtOBB919UWyZhtW7A4+4+28ztm96XwlP+Z5quPC39I92fcHMvkf4q3Uc8F8RZd4AfMjMFpjZFWa2XUT7bgZWN7OJySb70to//pNwcV+Zsb81CDN+XhlZ5stJvzjDzNZK769NuSOBn5vZ0qSudMZfB+aa2a1mNiNjn+20lIvoF53qyuoX7T6zm7n7w8k2y4DNUvtq+1k3sy8nn5MDgZMiymxPuAbcZGbXmdnOkXXdwao/2Paj/XXjAFI3/IlO14t0mdjrRWO5ttcMd38wWfcA8DDwJ3ef22G/eeU+CVzW8J7FlBm2D3BN6kak3XU5r1+0vZ536BftynTqF+3KxPYJiTBSbnIKcfcV7j6B8JfOLma2Y0Sx1Qlf83zH3d8KPEN4pNmpnrsIj43nEn4RLST89d4VM/sC8BIhFiPPdwi/cCcQPvRfiyhzNHCcu29F+H777DbtWI/w6PfY1EWjo8ZyhOP4PM0Xgm7q+jDZF/WW95Vwg9JRu77g7ocBryM8SfpQRJm1gOfdfSLwXUKsTF77/obwi+MMM7sZ+DMN/cPM3g886u63tmn+t4Hr3f2GiDInJudjZ0LswecaX+xQ7jhCnMZY4HuEr+8a7ebubwPeC3zCzPZo09a0rHJ5/aJTXVn9IvczmzwNSz81alvO3b+QfE4uIPzyzSuzOuF8TwI+C8xOnjrllTsc+LiZ3Ur42vbF9MlIJkP7AHBRan3b60VGmajrRUa5tteM5EZpOuEG7nXAumZ2UNZ+U3VklTuY8Av9v7oo01hXS7+IuS5n9YtO5dr1iw5l2vaLDmVy+4R0wfvgO7O8H0KA3lUNyycCJ3a5j5OI+E4a2BxY0rC8O3B5l3V9Bfh4h9eHaIiVSdYdSgg2Wye2TN5r6fXAn1g1N5IBT2eUWYMQK/FPGa/No33sRVM54M2EJxlLkp+XCH+FbZ5XF+HC8AgwNvJ9/SzwGKviFJr6S0xfIHyV8LO8MoQAwm0azuGfuu13hJiK2Q3L/0F4+rSE8Jfls8APkte+SHgE/6rUPtqWadhmcvqY2pS7nPD1xPA2WwN3djimkxuPqVO/yCj3r3n9ol1d7foFbT6zhKDtLZJ1WwD3xJRLbbM1zZ+hdnVdCezVsP4+YEyXdW0P3JxxDqYDc1PrDqXz9aKlTMNrQ7S/ljSVo8M1g3BTcnbD8sHAtxuWl5Adk5NV7v6kPw73i5WEEIXcuoBNgMdpCF5uc2xfIcSbdewX7cp16hcd6urYLyLqyewT+on/GSlPcrqe4tnMxpjZhsn/1wbeRfgF1ZG7LwP+aGZvTFZNJYwA6sjMNk3+3ZrwveoP88o0lJ1G+PrgA+7+bGSZLRoWP0h49JnnIWDP5P9TCKMLGvdphL/U7nL39F/yndrSUs7df+Pum7r7kLsPEX6xvi05v3l1vRO4292XZtSV9b7eRRiptW+y2SHApTll7jGzbRva8gEa+keH/jOHEEwI4VwOx9l0LNfQP9YiPF05c7iMu5/o7mOT83QA8Et3P8jMjgTeA3zYkxigiDJbNBzTPqT6RVY5wi+2Dcxs+2Sz4XM6fEzrmtlrhv9PuEnL7W9tyt2S0y861ZXZLzp8Zi8j9AVI9YlO5az5K8jpNPSLDnW93C+S87gm4cY7r67hfvEq4F9o6BcNmp5SRF4v0mVirxfpJyKdrhkPAJPMbJ2kv02lod90kFXu6+6+eUO/eNbdt42sa1/Czfzz6YraXJc79ot25Tr1iw51dewXbeqJ6RMSq9d3WbE/hO8r7yXcCX8hYvu3EIbI3k74QJ/URV0TCMMlbyd00pYhhhllbiBc7BYBUzts9yPC4+K/Ei7wRxCGW/6RVUMh06M5ssp8nzDE8HbCh3aLiDK7AbcmbbwJ2ClVZjfCo9vbG9qyN+GiuBR4gfCX9FUx5VLbLKF5dFXbMoQRQkd1874SRt7dnJzLi0hGM7QrQ/iq9n+Tc7iY8Ph5/Yh6NiT85f4bwl/S4yPbdxrhonwPnacYmMyqEU8vEfr78PnJ7MOpMr9sOKYf0DAsPqfcB5NyiwhPZl7fsN3rk/XDw+K/0FCmU7/ILJfTL9qWyekXLZ9ZYGPgGsIv5l8AG0WWuzg5f7cThvBuGVFmzeR8LyYMxZ8SWdcxhOvavYQ4KEuVWZfwlGKDhnV514usMh2vFx3K5V0zTiH8sl+c1LEWYeTRUkL/fYiGbNKdyqVezxpCnlmG0F+nxV6XI/tFVrm8fpFVpmO/aFo6WDQAAAOASURBVFOmY5/QT3c/SusgIiIiA2mkfF0lIiIi0hXd5IiIiMhA0k2OiIiIDCTd5IiIiMhA0k2OiIiIDCTd5Ij0ETNbYWYLzWyxmV2UTPdedF/nmtm+yf/PMrMdOmw72cx2LVDHEjPbJHZ9apu/dHo9Y/uTzeyfu22jiIxeuskR6S/PufsEd9+RMJ37UY0vWkhO2DV3P9LdO01qOZmQLV1EZGDoJkekf90AbJs8ZbnBzC4jzJK7mpmdZma3WEi4+DEIsxyb2bfM7B4z+wWw6fCOzGyeJQlCzWyamd1mZovM7BoLCVKPAo5LniLtnszcfHFSxy1m9o6k7MZmNtfM7jCzswhT/XdkZnMsJNu8w1IJNy0kEb0jaceYZN0bzOzKpMwNZpabm0xEJEuhvwpFpFrJE5v3sirz+NuAHd39/uRG4U/uvnOSJuJ/zWwuIZP7G4EdCJmV7ySVQDS5kfgusEeyr43c/QkzO5Mwy+zpyXY/BM5w9xuTKeevAt5EyKV1o7t/yczeR5hJO8/hSR1rA7eY2cXu/jhhht0F7n6cmZ2U7PuTwCzCzMa/NbO/JSQonVLgNIrIKKebHJH+sraZLUz+fwMhv9euhCR99yfr3w28ZTjeBtgA2I6QZPRH7r4CeMjMfpmx/0mEjOb3A7j7E23a8U5gB1uVSHt9Cxnj9yDk2MHdLzezJyOO6dNm9sHk/1slbX2ckITxx8n6HwCXJHXsClzUUPdaEXWIiLTQTY5If3nO3Sc0rkh+2T/TuAr4lLtfldpu7xLb8SpgkqeSHjbceEQxs8mEG6a3u/uzZjYPeHWbzT2p96n0ORARKUIxOSIjz1XA0Wa2BoTsxknG7uuBDyUxO1uwKlt6o/nAHma2TVJ2o2T9n4HXNGw3F/jU8IKZDd90XA98JFn3XkKSyU42AJ5MbnDGEZ4kDXsVqzLHf4TwNdjTwP1mtl9Sh5nZ+Jw6REQy6SZHZOQ5ixBvc5uZLQb+m/BU9qeEzMp3AucTsqQ3cfflwAzCV0OLWPV10f8AHxwOPCZkkp6YBDbfyapRXqcQbpLuIHxt9UBOW68EVjezuwgZlec3vPYMsEtyDFOALyXrDwSOSNp3BzA94pyIiLRQFnIREREZSHqSIyIiIgNJNzkiIiIykHSTIyIiIgNJNzkiIiIykHSTIyIiIgNJNzkiIiIykHSTIyIiIgPp/wN+HgFCPMTAbAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGxWw4fyzyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}