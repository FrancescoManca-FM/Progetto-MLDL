{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_icarl_point_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/classifiers/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1504715f-84dd-4a78-d937-ae74f0936deb"
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3437a9b7-b140-4e07-c340-d2959e981033"
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f1c2d8e4-b31a-47af-c7bb-a3775feda8ea"
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 159, done.\u001b[K\n",
            "remote: Counting objects: 100% (159/159), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 2278 (delta 85), reused 107 (delta 38), pack-reused 2119\u001b[K\n",
            "Receiving objects: 100% (2278/2278), 27.53 MiB | 34.09 MiB/s, done.\n",
            "Resolving deltas: 100% (1351/1351), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "be6a372f-4595-4253-b01a-4fda84d37863"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}/cifar-100-python'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-24 21:02:13--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz.1’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  97.1MB/s    in 1.7s    \n",
            "\n",
            "2020-06-24 21:02:14 (97.1 MB/s) - ‘cifar-100-python.tar.gz.1’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d011cd32-9270-4c4f-fa5f-aa8f79afc808"
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = 66\n",
        "\n",
        "# icarl params\n",
        "herding = True # if false random exemplars, if true nme (herding)\n",
        "classifier = \"NCM\" # NCM, FCC, KNN, SVC, COS"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 66, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c63c23b-282a-4c7f-da76-efcd9b9e90b6"
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "     \n",
        "        # add other classifiers\n",
        "        if classifier == 'NCM':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.classify(images)\n",
        "        elif classifier == 'FCC':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.FCC_classify(images)\n",
        "        elif classifier == 'KNN' or classifier == 'SVC':\n",
        "          preds = net.KNN_SVC_classify(images)\n",
        "          preds = preds.to(DEVICE)\n",
        "        elif classifier == 'COS':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.COS_classify(images)\n",
        "\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "        if method == 'test':\n",
        "          all_preds_cm.extend(preds.tolist())\n",
        "          all_labels_cm.extend(labels.data.tolist())\n",
        "  accuracy = correct/len(dataset)\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        all_preds_cm = []\n",
        "        all_labels_cm = []\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "          train_set_big = train_subset\n",
        "        else:\n",
        "          test_set = utils.joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, train_dataset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(math.ceil(K/icarl.n_classes))\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          icarl.construct_exemplar_set(class_train_subset,m,y)\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier\n",
        "        icarl.computeMeans()\n",
        "\n",
        "        # common training on exemplars for KNN and SVC classifier\n",
        "        if classifier == 'KNN':\n",
        "          K_nn = 5\n",
        "          icarl.modelTrain(classifier, K_nn)\n",
        "        elif classifier == 'SVC':\n",
        "          icarl.modelTrain(classifier)\n",
        "\n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAn-c_mMrMB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This class implements the main model of iCaRL \n",
        "    and all the methods regarding the exemplars\n",
        "    from delivery: iCaRL is made up of 2 components\n",
        "    - feature extractor (a convolutional NN) => resnet32 optimized on cifar100\n",
        "    - classifier => a FC layer OR a non-parametric classifier (NME)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "import gc #extensive use in order to manage memory issues\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToPILImage \n",
        "\n",
        "from Cifar100 import utils\n",
        "from Cifar100.resnet import resnet32\n",
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# new classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def auto_loss_rebalancing(n_known, n_classes, loss_type):\n",
        "  alpha = n_known/n_classes \n",
        "\n",
        "  if loss_type == 'class':\n",
        "    return 1-alpha\n",
        "  return alpha\n",
        "\n",
        "def get_rebalancing(rebalancing=None):\n",
        "  if rebalancing is None:\n",
        "    return lambda n_known, n_classes, loss_type: 1\n",
        "  if rebalancing in ['auto', 'AUTO']:\n",
        "    return auto_loss_rebalancing\n",
        "  if callable(rebalancing):\n",
        "    return rebalancing\n",
        "\n",
        "# feature_size: 2048, why?\n",
        "# n_classes: 10 => 100\n",
        "class ICaRL(nn.Module):\n",
        "  def __init__(self, feature_size, n_classes,\\\n",
        "      BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE, MILESTONES, MOMENTUM, K,\\\n",
        "      herding, reverse_index = None, class_loss_criterion='bce', dist_loss_criterion='bce', loss_rebalancing='auto', lambda0=1):\n",
        "    super(ICaRL, self).__init__()\n",
        "    self.net = resnet32()\n",
        "    self.net.fc = nn.Linear(self.net.fc.in_features, n_classes)\n",
        "\n",
        "    self.feature_extractor = resnet32()\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    self.n_classes = n_classes\n",
        "    self.n_known = 0\n",
        "\n",
        "    # Hyper-parameters from iCaRL\n",
        "    self.BATCH_SIZE = BATCH_SIZE\n",
        "    self.WEIGHT_DECAY  = WEIGHT_DECAY\n",
        "    self.LR = LR\n",
        "    self.GAMMA = GAMMA # this allow LR to become 1/5 LR after MILESTONES epochs\n",
        "    self.NUM_EPOCHS = NUM_EPOCHS\n",
        "    self.DEVICE = DEVICE\n",
        "    self.MILESTONES = MILESTONES # when the LR decreases, according to icarl\n",
        "    self.MOMENTUM = MOMENTUM\n",
        "    self.K = K\n",
        "    \n",
        "    self.reverse_index=reverse_index\n",
        "\n",
        "    self.optimizer, self.scheduler = utils.getOptimizerScheduler(self.LR, self.MOMENTUM, self.WEIGHT_DECAY, self.MILESTONES, self.GAMMA, self.parameters())\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    # List containing exemplar_sets\n",
        "    # Each exemplar_set is a np.array of N images\n",
        "    self.exemplar_sets = []\n",
        "    self.exemplar_sets_indices = []\n",
        "\n",
        "    \n",
        "    # for the classification/distillation loss we have two alternatives\n",
        "    # 1- BCE loss with Logits (reduction could be mean or sum)\n",
        "    # 2- BCE loss + sigmoid\n",
        "    # actually we use just one loss as explained on the forum\n",
        "\n",
        "    self.class_loss, self.dist_loss = self.build_loss(class_loss_criterion, dist_loss_criterion, loss_rebalancing, lambda0=lambda0)\n",
        "\n",
        "    # Means of exemplars (cntroids)\n",
        "    self.compute_means = True\n",
        "    self.exemplar_means = []\n",
        "    self.exemplar_mean_nn = [] # means not normalized\n",
        "\n",
        "    self.herding = herding # random choice of exemplars or icarl exemplars strategy?\n",
        "\n",
        "    # this is used as explained in the forum to compute the exemplar mean in a more accurate way\n",
        "    # populated during construct exemplar set and used in the classify step\n",
        "    self.data_from_classes = []\n",
        "    self.means_from_classes = []\n",
        "\n",
        "    # Knn, svc classification\n",
        "    self.model = None\n",
        "\n",
        "    # QUA! #\n",
        "    ###############################################################################################################################################################################################\n",
        "    self.oldNet= None\n",
        "    self.moreOldNet= None\n",
        "\n",
        "    ###############################################################################################################################################################################################\n",
        "  \n",
        "  # increment the number of classes considered by the net\n",
        "  # incremental learning approach, 0,10..100\n",
        "  def increment_classes(self, n):\n",
        "        gc.collect()\n",
        "\n",
        "        in_features = self.net.fc.in_features\n",
        "        out_features = self.net.fc.out_features\n",
        "        weights = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features + n) #add 10 classes to the fc last layer\n",
        "        self.net.fc.weight.data[:out_features] = weights\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "        self.n_classes += n #icrement #classes considered\n",
        "\n",
        "  # computes the mean of each exemplar set\n",
        "  def computeMeans(self):\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # new mean mgmt\n",
        "    tensors_mean = []\n",
        "    exemplar_mean_nn=[]\n",
        "    with torch.no_grad():\n",
        "      for tensor_set in self.data_from_classes:\n",
        "        features = []\n",
        "        for tensor, _ in tensor_set:\n",
        "          \n",
        "          tensor = tensor.to(self.DEVICE)\n",
        "          feature = feature_extractor(tensor)\n",
        "\n",
        "          feature.data = feature.data / feature.data.norm() # Normalize\n",
        "          features.append(feature)\n",
        "\n",
        "          # cleaning \n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        features = torch.stack(features) #(num_exemplars,num_features)\n",
        "        mean_tensor = features.mean(0) \n",
        "        exemplar_mean_nn.append(mean_tensor.to('cpu'))\n",
        "        mean_tensor.data = mean_tensor.data / mean_tensor.data.norm() # Re-normalize\n",
        "        mean_tensor = mean_tensor.to('cpu')\n",
        "        tensors_mean.append(mean_tensor)\n",
        "\n",
        "    self.exemplar_means = tensors_mean  # nb the mean is computed over all the imgs\n",
        "    self.exemplar_mean_nn= exemplar_mean_nn # exemplars means not normalized\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # train procedure common for KNN and SVC classifier (save a lot of training time)\n",
        "  def modelTrain(self, method, K_nn = None):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # -- train a SVC classifier\n",
        "    X_train, y_train = [], []\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets:\n",
        "          for exemplar, label in  exemplar_set:\n",
        "            exemplar = exemplar.to(self.DEVICE)\n",
        "            feature = feature_extractor(exemplar)\n",
        "            feature = feature.squeeze()\n",
        "            feature.data = feature.data / feature.data.norm() # Normalize\n",
        "            X_train.append(feature.cpu().detach().numpy())\n",
        "            y_train.append(label)\n",
        "    \n",
        "    if method == 'KNN':\n",
        "      model = KNeighborsClassifier(n_neighbors = K_nn)\n",
        "    elif method == 'SVC':\n",
        "      model = LinearSVC()\n",
        "    self.model = model.fit(X_train, y_train)\n",
        "\n",
        "  # common classify function\n",
        "  def KNN_SVC_classify(self, images):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # --- prediction\n",
        "    X_pred = []\n",
        "    images = images.to(self.DEVICE)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    features = feature_extractor(images)\n",
        "    for feature in features:\n",
        "      feature = feature.squeeze()\n",
        "      feature.data = feature.data / feature.data.norm() # Normalize\n",
        "      X_pred.append(feature.cpu().detach().numpy())\n",
        "    \n",
        "    preds = self.model.predict(X_pred)\n",
        "    # --- end prediction\n",
        "    return torch.tensor(preds)\n",
        "    \n",
        "  # classify base on cosine similarity\n",
        "  def COS_classify(self, batch_imgs):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_imgs_size = batch_imgs.size(0)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    means_exemplars = torch.cat(self.exemplar_mean_nn, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "    means_exemplars = means_exemplars.transpose(1, 2) # means no normalized\n",
        "\n",
        "    feature = feature_extractor(batch_imgs) # features no normalized\n",
        "    \n",
        "    feature=feature.to('cpu')\n",
        "    means_exemplars = means_exemplars.to('cpu')\n",
        "\n",
        "    preds=[]\n",
        "    for a in feature:\n",
        "      a=a.detach().numpy()\n",
        "      aa=np.linalg.norm(a)\n",
        "      res=[]\n",
        "      for b in means_exemplars:\n",
        "        b=b.detach().numpy()\n",
        "        bb=np.linalg.norm(b)\n",
        "        dot = np.dot(a, b)\n",
        "        cos = dot / (aa * bb)\n",
        "        res.append(cos)\n",
        "      preds.append(np.argmax(np.array(res)))\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return torch.FloatTensor(preds).to(self.DEVICE)\n",
        "\n",
        "  # classification via fc layer (similar to lwf approach)\n",
        "  def FCC_classify(self, images):\n",
        "    _, preds = torch.max(torch.softmax(self.net(images), dim=1), dim=1, keepdim=False)\n",
        "    return preds\n",
        "  # NME classification from iCaRL paper\n",
        "  def classify(self, batch_imgs):\n",
        "      \"\"\"Classify images by nearest-mean-of-exemplars\n",
        "      Args:\n",
        "          batch_imgs: input image batch\n",
        "      Returns:\n",
        "          preds: Tensor of size (batch_size,)\n",
        "      \"\"\"\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      batch_imgs_size = batch_imgs.size(0)\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # update exemplar_means with the mean\n",
        "      # of all the train data for a given class\n",
        "\n",
        "      means_exemplars = torch.cat(self.exemplar_means, dim=0)\n",
        "      means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "      means_exemplars = means_exemplars.transpose(1, 2) \n",
        "\n",
        "      feature = feature_extractor(batch_imgs) \n",
        "      aus_normalized_features = []\n",
        "      for el in feature: # Normalize\n",
        "          el.data = el.data / el.data.norm()\n",
        "          aus_normalized_features.append(el)\n",
        "\n",
        "      feature = torch.stack(aus_normalized_features,dim=0)\n",
        "\n",
        "      feature = feature.unsqueeze(2) \n",
        "      feature = feature.expand_as(means_exemplars) \n",
        "\n",
        "      means_exemplars = means_exemplars.to(self.DEVICE)\n",
        "\n",
        "      # Nearest prototype\n",
        "      preds = torch.argmin((feature - means_exemplars).pow(2).sum(1),dim=1)\n",
        "\n",
        "      # cleaning\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      return preds\n",
        "\n",
        "  # implementation of alg. 4 of icarl paper\n",
        "  # iCaRL ConstructExemplarSet\n",
        "  def construct_exemplar_set(self, tensors, m, label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "          tensors: train_subset containing a single label\n",
        "          m: number of exemplars allowed/exemplar set (class)\n",
        "          label: considered class\n",
        "    \"\"\"\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    exemplar_set_indices = set()\n",
        "    exemplar_list_indices = []\n",
        "    exemplar_set = []\n",
        "    if self.herding:\n",
        "\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # Compute and cache features for each example\n",
        "      features = []\n",
        "\n",
        "      loader = DataLoader(tensors,batch_size=self.BATCH_SIZE,shuffle=True,drop_last=False,num_workers = 4)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for _, images, labels in loader:\n",
        "          images = images.to(self.DEVICE)\n",
        "          labels = labels.to(self.DEVICE)\n",
        "          feature = feature_extractor(images) \n",
        "\n",
        "          feature = feature / np.linalg.norm(feature.cpu()) # Normalize\n",
        "          \n",
        "          features.append(feature)\n",
        "\n",
        "      features_s = torch.cat(features)\n",
        "      \n",
        "      class_mean = features_s.mean(0)\n",
        "      class_mean = class_mean / np.linalg.norm(class_mean.cpu()) # Normalize\n",
        "      class_mean = torch.stack([class_mean]*features_s.size()[0])\n",
        "\n",
        "      summon = torch.zeros(1,features_s.size()[1]).to(self.DEVICE) #(1,num_features)\n",
        "      for k in range(1, (m + 1)):\n",
        "          S = torch.cat([summon]*features_s.size()[0]) # second addend, features in the exemplar set\n",
        "          results = pd.DataFrame((class_mean-(1/k)*(features_s + S)).pow(2).sum(1).cpu(), columns=['result']).sort_values('result')\n",
        "          results['index'] = results.index\n",
        "          results = results.to_numpy()\n",
        "\n",
        "          # select argmin not included in exemplar_set_indices\n",
        "          for i in range(results.shape[0]):\n",
        "            index = results[i, 1]\n",
        "            exemplar_k_index = tensors[index][0]\n",
        "            if exemplar_k_index not in exemplar_set_indices:\n",
        "              exemplar_k = tensors[index][1].unsqueeze(dim = 0) # take the image from the tuple (index, img, label)\n",
        "              exemplar_set.append((exemplar_k, label))\n",
        "              exemplar_k_index = tensors[index][0] # index of the img on the real dataset\n",
        "              \n",
        "              exemplar_list_indices.append(exemplar_k_index)\n",
        "              exemplar_set_indices.add(exemplar_k_index)\n",
        "              break\n",
        "\n",
        "          # features of the exemplar k\n",
        "          phi = feature_extractor(exemplar_k.to(self.DEVICE)) #feature_extractor(exemplar_k.to(self.DEVICE))\n",
        "          summon += phi # update sum of features\n",
        "    else:\n",
        "      tensors_size = len(tensors)\n",
        "      unique_random_indexes = random.sample(range(0, tensors_size), m) # random sample without replacement k exemplars\n",
        "      i = 0\n",
        "      for k in range(1, (m + 1)):\n",
        "        index = unique_random_indexes[i]\n",
        "        exemplar_k = tensors[index][1].unsqueeze(dim = 0)\n",
        "        exemplar_k_index = tensors[index][0]\n",
        "        exemplar_set.append((exemplar_k, label))\n",
        "        exemplar_set_indices.add(exemplar_k_index)\n",
        "        i = i + 1\n",
        "\n",
        "    # --- new ---\n",
        "    tensor_set = []\n",
        "    for i in range(0, len(tensors)):\n",
        "      t = tensors[i][1].unsqueeze(dim = 0)\n",
        "      tensor_set.append((t, label))\n",
        "    \n",
        "    self.exemplar_sets.append(exemplar_set) #update exemplar sets with the updated exemplars images\n",
        "    self.exemplar_sets_indices.append(exemplar_list_indices)\n",
        "\n",
        "    # this is used to compute more accurately the means of the exemplar (see also computeMeans and classify)\n",
        "    self.data_from_classes.append(tensor_set)\n",
        "\n",
        "    # cleaning\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # build a exemplar dataset as a subset of the train dataset\n",
        "  def build_exemplars_dataset(self, train_dataset): #complete train dataset\n",
        "    all_exemplars_indices = []\n",
        "    for exemplar_set_indices in self.exemplar_sets_indices:\n",
        "        all_exemplars_indices.extend(exemplar_set_indices)\n",
        "\n",
        "    exemplars_dataset = Subset(train_dataset, all_exemplars_indices)\n",
        "    return exemplars_dataset\n",
        "\n",
        "  def update_representation(self, dataset, train_dataset_big, new_classes):\n",
        "    # 1 - retrieve the classes from the dataset (which is the current train_subset)\n",
        "    # 2 - retrieve the new classes\n",
        "    # 1,2 are done in the main_icarl\n",
        "    #gc.collect()\n",
        "\n",
        "    # 3 - increment classes\n",
        "    #          (add output nodes)\n",
        "    #          (update n_classes)\n",
        "    # 5        store network outputs with pre-update parameters\n",
        "    self.increment_classes(len(new_classes))\n",
        "\n",
        "    # 4 - combine current train_subset (dataset) with exemplars\n",
        "    #     to form a new augmented train dataset\n",
        "    # join the datasets\n",
        "    exemplars_dataset = self.build_exemplars_dataset(train_dataset_big)\n",
        "    \n",
        "    ######################################################################################\n",
        "    new_classes_dataset=[]\n",
        "    if self.n_known==0:\n",
        "      cut=0\n",
        "    else:\n",
        "      cut=self.n_known/10\n",
        "    \n",
        "    new_dataset_size = 5000 - int(math.ceil(450*cut))\n",
        "    unique_random_indexes = random.sample(range(0, len(dataset)), new_dataset_size) # random sample without replacement\n",
        "\n",
        "    new_dataset=torch.utils.data.Subset(dataset, unique_random_indexes)\n",
        "    print(\"NEW CLASS SAMPLES: \",len(new_dataset))\n",
        "    ######################################################################################\n",
        "    #\n",
        "    if len(exemplars_dataset) > 0:\n",
        "      join_dataset = ConcatDataset(new_dataset, exemplars_dataset)\n",
        "      augmented_dataset = ConcatDataset(join_dataset, exemplars_dataset)\n",
        "      # load_exemplars = DataLoader(exemplars_dataset, batch_size=self.BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "      # new_image=[]\n",
        "      # old_image=[]\n",
        "      # for _, images, labels in load_exemplars:\n",
        "      #   for img,lab in zip(images,labels):\n",
        "      #     tran=transforms.Compose([transforms.ToPILImage(),transforms.RandomHorizontalFlip(p=1),transforms.ToTensor()])\n",
        "      #     new_img=tran(img)\n",
        "      #     new_image.append((0,new_img,lab))\n",
        "      #     old_image.append((0,img,lab))\n",
        "      # new_join=new_dataset+old_image+new_image\n",
        "      # augmented_dataset=new_join\n",
        "      #augmented_dataset = ConcatDataset(dataset, exemplars_dataset)\n",
        "      #augmented_dataset = utils.joinSubsets(train_dataset_big, [dataset, exemplars_dataset])\n",
        "    else: \n",
        "      augmented_dataset = new_dataset # first iteration\n",
        "\n",
        "    print(\"ALL DB: \",len(augmented_dataset))\n",
        "    # 6 - run network training, with loss function\n",
        "\n",
        "    net = self.net\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=self.LR, weight_decay=self.WEIGHT_DECAY, momentum=self.MOMENTUM)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.MILESTONES, gamma=self.GAMMA, last_epoch=-1)\n",
        "\n",
        "    criterion = utils.getLossCriterion()\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    net = net.to(self.DEVICE)\n",
        "\n",
        "    # define the loader for the augmented_dataset\n",
        "    loader = DataLoader(augmented_dataset, batch_size=self.BATCH_SIZE,shuffle=True, num_workers=4, drop_last = True)\n",
        "\n",
        "    if len(self.exemplar_sets) > 0:\n",
        "      # QUA! #\n",
        "      #########################################################################################\n",
        "\n",
        "      #   print(self.oldNetTeachers[1].size())\n",
        "      if self.oldNet!=None:\n",
        "        self.moreOldNet=self.oldNet\n",
        "      self.oldNet= copy.deepcopy(net)\n",
        "\n",
        "      #########################################################################################\n",
        "    for epoch in range(self.NUM_EPOCHS):\n",
        "        print(\"NUM_EPOCHS: \",epoch,\"/\", self.NUM_EPOCHS,\" LR: \",scheduler.get_lr())\n",
        "        for _, images, labels in loader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(self.DEVICE)\n",
        "            labels = labels.to(self.DEVICE)\n",
        "            net.train()\n",
        "            # PyTorch, by default, accumulates gradients after each backward pass\n",
        "            # We need to manually set the gradients to zero before starting a new iteration\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            \n",
        " \n",
        "            # QUA! #\n",
        "            #############################################################################################################\n",
        "            # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "            # Loss = only classification on new classes\n",
        "            loss = self.class_loss(outputs, labels, col_start=self.n_known)\n",
        "            class_loss = loss.item() # Used for logging for debugging purposes\n",
        "            \n",
        "            # Distilation loss for old classes, class loss on new classes\n",
        "            dist_loss = None\n",
        "            older_dist_loss=None\n",
        "            if len(self.exemplar_sets) > 0:\n",
        "              old_net=self.oldNet\n",
        "              out_old = torch.sigmoid(old_net(images))\n",
        "              dist_loss = self.dist_loss(outputs, out_old, col_end=self.n_known)\n",
        "\n",
        "              if self.moreOldNet!=None:\n",
        "                older_net=self.moreOldNet # old old net\n",
        "                older_out = torch.sigmoid(older_net(images))\n",
        "                older_dist_loss = self.double_dist_loss(outputs[0:128,0:-20], older_out[0:128,0:-10], col_end=self.n_known)\n",
        "                loss += older_dist_loss\n",
        "\n",
        "              loss += dist_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ##############################################################################################################\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LOSS: \", loss.item(), 'class loss', class_loss, 'dist loss', dist_loss.item() if dist_loss is not None else dist_loss, 'older dist loss',older_dist_loss.item() if older_dist_loss is not None else older_dist_loss)\n",
        "\n",
        "    self.net = copy.deepcopy(net)\n",
        "    self.feature_extractor = copy.deepcopy(net)\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    #cleaning\n",
        "    del net\n",
        "    torch.cuda.empty_cache()\n",
        " # QUA! #\n",
        " ###################################################################################################################\n",
        "  def double_dist_loss(self,outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    rebalancing=None\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    dist_loss_func = self.bce_dist_loss\n",
        "    alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "    return 0.5*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        " ##################################################################################################################\n",
        "\n",
        "  def build_loss(self, class_loss_criterion, dist_loss_criterion, rebalancing=None, lambda0=1):\n",
        "    class_loss_func = None\n",
        "    dist_loss_func = None\n",
        "\n",
        "    if class_loss_criterion in ['l2', 'L2']:\n",
        "      class_loss_func = self.l2_class_loss\n",
        "    elif class_loss_criterion in ['bce', 'BCE']:\n",
        "      class_loss_func = self.bce_class_loss\n",
        "    elif class_loss_criterion in ['ce', 'CE']:\n",
        "      class_loss_func = self.ce_class_loss\n",
        "\n",
        "    if dist_loss_criterion in ['l2', 'L2']:\n",
        "      dist_loss_func = self.l2_dist_loss\n",
        "    elif dist_loss_criterion in ['bce', 'BCE']:\n",
        "      dist_loss_func = self.bce_dist_loss\n",
        "    elif dist_loss_criterion in ['ce', 'CE']:\n",
        "      dist_loss_func = self.ce_dist_loss\n",
        "\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    \n",
        "    def class_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'class')\n",
        "      return alpha*class_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    def dist_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "      return lambda0*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    return class_loss, dist_loss\n",
        "\n",
        "  def bce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def bce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def ce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, self.reverse_index.getNodes(labels), decode=False, row_start=row_start, row_end=row_end, col_start=None, col_end=col_end)\n",
        "    \n",
        "  def ce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, labels, decode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "\n",
        "  def bce_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
        "\n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "\n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  def ce_loss(self, outputs, labels, decode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if decode:\n",
        "      labels = torch.argmax(labels, dim=1)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end])\n",
        "\n",
        "\n",
        "  def l2_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.MSELoss(reduction = 'mean')\n",
        "    \n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  # implementation of alg. 5 of icarl paper\n",
        "  # iCaRL ReduceExemplarSet\n",
        "  def reduce_exemplar_sets(self, m):\n",
        "  \t    # i keep only the first m exemplar images\n",
        "        # where m is the UPDATED K/number_classes_seen\n",
        "        # the number of images per each exemplar set (class) progressively decreases\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:m] \n",
        "        for x, P_x in enumerate(self.exemplar_sets_indices):\n",
        "            self.exemplar_sets_indices[x] = P_x[:m] \n",
        "\n",
        "\n",
        "# ---------- \n",
        "from torch.utils.data import Dataset\n",
        "\"\"\"\n",
        "  Merge two different datasets (train and exemplars in our case)\n",
        "  format:\n",
        "  train\n",
        "  --------\n",
        "  exemplars\n",
        "  train leans on cifar100\n",
        "  exemplars is managed here (exemplar_transform is performed) => changed\n",
        "\"\"\"\n",
        "class ConcatDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, dataset1, dataset2):\n",
        "        self.dataset1 = dataset1\n",
        "        self.dataset2 = dataset2\n",
        "        self.l1 = len(dataset1)\n",
        "        self.l2 = len(dataset2)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        if index < self.l1:\n",
        "            _, image,label = self.dataset1[index] #here it leans on cifar100 get item\n",
        "            return _, image,label\n",
        "        else:\n",
        "            _, image, label = self.dataset2[index - self.l1]\n",
        "            return _, image,label\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.l1 + self.l2)\n",
        "#------------"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "93012424-ffe7-4681-d740-5b5f75c3f609"
      },
      "source": [
        "#from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, herding, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca8c1273-92d3-44ca-b794-add8689eafef"
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NEW CLASS SAMPLES:  5000\n",
            "ALL DB:  5000\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LOSS:  0.32516908645629883 class loss 0.32516908645629883 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.3214259445667267 class loss 0.3214259445667267 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.3047736585140228 class loss 0.3047736585140228 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.3144136965274811 class loss 0.3144136965274811 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.3040864169597626 class loss 0.3040864169597626 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.29410848021507263 class loss 0.29410848021507263 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.2803092300891876 class loss 0.2803092300891876 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.28446754813194275 class loss 0.28446754813194275 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.27457818388938904 class loss 0.27457818388938904 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.2654457688331604 class loss 0.2654457688331604 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.27895909547805786 class loss 0.27895909547805786 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.22741177678108215 class loss 0.22741177678108215 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.2557610869407654 class loss 0.2557610869407654 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.23771674931049347 class loss 0.23771674931049347 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.22627870738506317 class loss 0.22627870738506317 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.2376108020544052 class loss 0.2376108020544052 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.23839636147022247 class loss 0.23839636147022247 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.2353455275297165 class loss 0.2353455275297165 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.21240897476673126 class loss 0.21240897476673126 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.22972141206264496 class loss 0.22972141206264496 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.24493388831615448 class loss 0.24493388831615448 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.21963845193386078 class loss 0.21963845193386078 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.2213878184556961 class loss 0.2213878184556961 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.23720479011535645 class loss 0.23720479011535645 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.19627033174037933 class loss 0.19627033174037933 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.20685167610645294 class loss 0.20685167610645294 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.2132130116224289 class loss 0.2132130116224289 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.20903067290782928 class loss 0.20903067290782928 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.21109946072101593 class loss 0.21109946072101593 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.2082868069410324 class loss 0.2082868069410324 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.1835770457983017 class loss 0.1835770457983017 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.2077004760503769 class loss 0.2077004760503769 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.19230403006076813 class loss 0.19230403006076813 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.20104900002479553 class loss 0.20104900002479553 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.1953074187040329 class loss 0.1953074187040329 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.1752648502588272 class loss 0.1752648502588272 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.18834547698497772 class loss 0.18834547698497772 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.15030281245708466 class loss 0.15030281245708466 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.16732794046401978 class loss 0.16732794046401978 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.15690551698207855 class loss 0.15690551698207855 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.15692661702632904 class loss 0.15692661702632904 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.15917456150054932 class loss 0.15917456150054932 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.1718737930059433 class loss 0.1718737930059433 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.156198188662529 class loss 0.156198188662529 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.15749143064022064 class loss 0.15749143064022064 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.14606444537639618 class loss 0.14606444537639618 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.15520401298999786 class loss 0.15520401298999786 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.13139748573303223 class loss 0.13139748573303223 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.13029776513576508 class loss 0.13029776513576508 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.11598117649555206 class loss 0.11598117649555206 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.13038776814937592 class loss 0.13038776814937592 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.11835302412509918 class loss 0.11835302412509918 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.10153638571500778 class loss 0.10153638571500778 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.10534393787384033 class loss 0.10534393787384033 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.12283747643232346 class loss 0.12283747643232346 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.10363388061523438 class loss 0.10363388061523438 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.11095725744962692 class loss 0.11095725744962692 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.10828697681427002 class loss 0.10828697681427002 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.10920359939336777 class loss 0.10920359939336777 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.09760453552007675 class loss 0.09760453552007675 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.11227848380804062 class loss 0.11227848380804062 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.10930635035037994 class loss 0.10930635035037994 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.09277409315109253 class loss 0.09277409315109253 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.06844105571508408 class loss 0.06844105571508408 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08662912994623184 class loss 0.08662912994623184 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08429479598999023 class loss 0.08429479598999023 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08326274156570435 class loss 0.08326274156570435 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08470004796981812 class loss 0.08470004796981812 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.058732952922582626 class loss 0.058732952922582626 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.07790923118591309 class loss 0.07790923118591309 dist loss None older dist loss None\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 85.26\n",
            "\n",
            "Test Accuracy (all groups seen so far): 74.40\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NEW CLASS SAMPLES:  4550\n",
            "ALL DB:  8550\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.1517767310142517 class loss 0.08649016916751862 dist loss 0.06528656929731369 older dist loss None\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.13100247085094452 class loss 0.07315362989902496 dist loss 0.05784883722662926 older dist loss None\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.13593533635139465 class loss 0.07060785591602325 dist loss 0.0653274804353714 older dist loss None\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.11682017147541046 class loss 0.059403639286756516 dist loss 0.05741652846336365 older dist loss None\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.12935924530029297 class loss 0.062436725944280624 dist loss 0.06692252308130264 older dist loss None\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.12666958570480347 class loss 0.06197093799710274 dist loss 0.06469865143299103 older dist loss None\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.11580078303813934 class loss 0.04935350641608238 dist loss 0.06644728034734726 older dist loss None\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.1185222640633583 class loss 0.055114567279815674 dist loss 0.06340769678354263 older dist loss None\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.11320708692073822 class loss 0.04775781184434891 dist loss 0.06544927507638931 older dist loss None\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.12345501780509949 class loss 0.05390520021319389 dist loss 0.0695498138666153 older dist loss None\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.12216830253601074 class loss 0.0536700077354908 dist loss 0.06849829852581024 older dist loss None\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.11245233565568924 class loss 0.048479124903678894 dist loss 0.06397321075201035 older dist loss None\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.11967428028583527 class loss 0.052999187260866165 dist loss 0.0666750892996788 older dist loss None\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.10161975026130676 class loss 0.035725682973861694 dist loss 0.06589406728744507 older dist loss None\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.10462607443332672 class loss 0.038863655179739 dist loss 0.06576241552829742 older dist loss None\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.09865821897983551 class loss 0.03458082303404808 dist loss 0.06407739967107773 older dist loss None\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.10182623565196991 class loss 0.0354357473552227 dist loss 0.06639048457145691 older dist loss None\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.12280640006065369 class loss 0.049134183675050735 dist loss 0.07367222011089325 older dist loss None\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.1008719652891159 class loss 0.033753901720047 dist loss 0.06711806356906891 older dist loss None\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.10697250813245773 class loss 0.03915838897228241 dist loss 0.06781411916017532 older dist loss None\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.10072894394397736 class loss 0.03732287511229515 dist loss 0.0634060651063919 older dist loss None\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.11473686993122101 class loss 0.04157258942723274 dist loss 0.07316427677869797 older dist loss None\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.11621767282485962 class loss 0.04442232847213745 dist loss 0.07179534435272217 older dist loss None\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.10262033343315125 class loss 0.030578438192605972 dist loss 0.07204189896583557 older dist loss None\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.10725268721580505 class loss 0.0381055548787117 dist loss 0.06914713233709335 older dist loss None\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.09956568479537964 class loss 0.028655314818024635 dist loss 0.07091037184000015 older dist loss None\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.09656010568141937 class loss 0.030179673805832863 dist loss 0.06638043373823166 older dist loss None\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.10307106375694275 class loss 0.036751050502061844 dist loss 0.0663200169801712 older dist loss None\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.09281207621097565 class loss 0.029922474175691605 dist loss 0.06288960576057434 older dist loss None\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.10218127071857452 class loss 0.03406790643930435 dist loss 0.06811336427927017 older dist loss None\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.10754266381263733 class loss 0.03768777474761009 dist loss 0.06985489279031754 older dist loss None\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.10433928668498993 class loss 0.03269585967063904 dist loss 0.07164342701435089 older dist loss None\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.09073008596897125 class loss 0.02373006008565426 dist loss 0.06700002402067184 older dist loss None\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.09616225957870483 class loss 0.028237003833055496 dist loss 0.06792525947093964 older dist loss None\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.09736098349094391 class loss 0.02634311094880104 dist loss 0.07101786881685257 older dist loss None\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.09123191982507706 class loss 0.02286672592163086 dist loss 0.0683651939034462 older dist loss None\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.09869925677776337 class loss 0.028820563107728958 dist loss 0.06987869739532471 older dist loss None\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.10359238088130951 class loss 0.033348213881254196 dist loss 0.07024417072534561 older dist loss None\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.09331516921520233 class loss 0.02623331919312477 dist loss 0.06708185374736786 older dist loss None\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.091977559030056 class loss 0.02375972829759121 dist loss 0.06821782886981964 older dist loss None\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.09744830429553986 class loss 0.03135301545262337 dist loss 0.06609528511762619 older dist loss None\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.0815521776676178 class loss 0.017065206542611122 dist loss 0.06448697298765182 older dist loss None\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.09564816206693649 class loss 0.02475149929523468 dist loss 0.07089666277170181 older dist loss None\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.09185485541820526 class loss 0.024247489869594574 dist loss 0.06760736554861069 older dist loss None\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.08598755300045013 class loss 0.021772151812911034 dist loss 0.06421539932489395 older dist loss None\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.07824573665857315 class loss 0.014663534238934517 dist loss 0.06358220428228378 older dist loss None\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.08919873833656311 class loss 0.020474541932344437 dist loss 0.06872419267892838 older dist loss None\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.0891028344631195 class loss 0.019035879522562027 dist loss 0.07006695121526718 older dist loss None\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.0917646735906601 class loss 0.018733760342001915 dist loss 0.07303091138601303 older dist loss None\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.07921887934207916 class loss 0.012430062517523766 dist loss 0.06678881496191025 older dist loss None\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.07792586833238602 class loss 0.012116513215005398 dist loss 0.06580935418605804 older dist loss None\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.07033330947160721 class loss 0.008964280597865582 dist loss 0.06136902794241905 older dist loss None\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.06588917225599289 class loss 0.006807033903896809 dist loss 0.059082139283418655 older dist loss None\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.06917540729045868 class loss 0.006265513598918915 dist loss 0.06290989369153976 older dist loss None\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.0719161182641983 class loss 0.008744291961193085 dist loss 0.06317182630300522 older dist loss None\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.0698353722691536 class loss 0.01062945555895567 dist loss 0.05920591577887535 older dist loss None\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.07075989991426468 class loss 0.007922452874481678 dist loss 0.06283744424581528 older dist loss None\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.06676139682531357 class loss 0.006079815328121185 dist loss 0.06068158149719238 older dist loss None\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.06894229352474213 class loss 0.0075392089784145355 dist loss 0.06140308454632759 older dist loss None\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.06577365100383759 class loss 0.005757729988545179 dist loss 0.06001592427492142 older dist loss None\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.06801962107419968 class loss 0.005192267242819071 dist loss 0.06282735615968704 older dist loss None\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.06253863871097565 class loss 0.004395173396915197 dist loss 0.05814346298575401 older dist loss None\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.06557463109493256 class loss 0.006439443677663803 dist loss 0.05913518741726875 older dist loss None\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.06356631219387054 class loss 0.002353366930037737 dist loss 0.061212945729494095 older dist loss None\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06543838232755661 class loss 0.0046015153639018536 dist loss 0.06083687022328377 older dist loss None\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06624902039766312 class loss 0.0035093219485133886 dist loss 0.06273970007896423 older dist loss None\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06393437832593918 class loss 0.003086524549871683 dist loss 0.06084785610437393 older dist loss None\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06579146534204483 class loss 0.002463674172759056 dist loss 0.06332778930664062 older dist loss None\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.061898183077573776 class loss 0.002996802097186446 dist loss 0.058901380747556686 older dist loss None\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06155002489686012 class loss 0.0020179525017738342 dist loss 0.05953207239508629 older dist loss None\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 91.72\n",
            "\n",
            "Test Accuracy (all groups seen so far): 70.95\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "NEW CLASS SAMPLES:  4100\n",
            "ALL DB:  8100\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.17921572923660278 class loss 0.04531046748161316 dist loss 0.06345924735069275 older dist loss 0.07044601440429688\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.18177922070026398 class loss 0.050461702048778534 dist loss 0.06285979598760605 older dist loss 0.0684577226638794\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.16647912561893463 class loss 0.03600294142961502 dist loss 0.06230286881327629 older dist loss 0.06817331165075302\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.16163009405136108 class loss 0.03450814634561539 dist loss 0.06177413463592529 older dist loss 0.06534780561923981\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.1515367031097412 class loss 0.028476176783442497 dist loss 0.05721946060657501 older dist loss 0.06584107130765915\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.1604432612657547 class loss 0.03483719378709793 dist loss 0.05998130142688751 older dist loss 0.06562476605176926\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.1598730981349945 class loss 0.027850469574332237 dist loss 0.06382334232330322 older dist loss 0.0681992918252945\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.152915820479393 class loss 0.025653589516878128 dist loss 0.05917799472808838 older dist loss 0.0680842399597168\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.15579822659492493 class loss 0.026219230145215988 dist loss 0.061237215995788574 older dist loss 0.06834178417921066\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.15690068900585175 class loss 0.02845834754407406 dist loss 0.06162332743406296 older dist loss 0.06681901216506958\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.16412079334259033 class loss 0.02980387583374977 dist loss 0.06526423245668411 older dist loss 0.06905267387628555\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.15289190411567688 class loss 0.02491060271859169 dist loss 0.062426552176475525 older dist loss 0.06555475294589996\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.15788772702217102 class loss 0.02901005558669567 dist loss 0.06283707916736603 older dist loss 0.06604058295488358\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.14718906581401825 class loss 0.020978061482310295 dist loss 0.06209885701537132 older dist loss 0.06411214917898178\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.1602613925933838 class loss 0.024563897401094437 dist loss 0.06511297821998596 older dist loss 0.07058451324701309\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.15460138022899628 class loss 0.021838340908288956 dist loss 0.06181934103369713 older dist loss 0.0709436908364296\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.1504657119512558 class loss 0.019106131047010422 dist loss 0.06338204443454742 older dist loss 0.06797754019498825\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.1518653929233551 class loss 0.022596020251512527 dist loss 0.06276603043079376 older dist loss 0.06650333851575851\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.15134906768798828 class loss 0.01838386431336403 dist loss 0.06285709887742996 older dist loss 0.0701080933213234\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.15218250453472137 class loss 0.013489807024598122 dist loss 0.06624329835176468 older dist loss 0.07244940102100372\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.16241174936294556 class loss 0.017930656671524048 dist loss 0.06833197176456451 older dist loss 0.07614912837743759\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.1593477725982666 class loss 0.028008878231048584 dist loss 0.06402792781591415 older dist loss 0.06731096655130386\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.15650302171707153 class loss 0.021611912176012993 dist loss 0.06612629443407059 older dist loss 0.06876480579376221\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.14792999625205994 class loss 0.01753024011850357 dist loss 0.06294380873441696 older dist loss 0.0674559473991394\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.14579646289348602 class loss 0.01603330858051777 dist loss 0.06443347036838531 older dist loss 0.06532968580722809\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.1456843912601471 class loss 0.020433736965060234 dist loss 0.06192092224955559 older dist loss 0.06332973390817642\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.1468620002269745 class loss 0.018872737884521484 dist loss 0.06152480095624924 older dist loss 0.06646446883678436\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.14998960494995117 class loss 0.015977121889591217 dist loss 0.0655198022723198 older dist loss 0.06849268823862076\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.14994874596595764 class loss 0.01616751030087471 dist loss 0.06476875394582748 older dist loss 0.06901247799396515\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.14431166648864746 class loss 0.013532118871808052 dist loss 0.06534484028816223 older dist loss 0.06543470174074173\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.1470869928598404 class loss 0.011622591875493526 dist loss 0.0667983815073967 older dist loss 0.0686660185456276\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.14305008947849274 class loss 0.016364440321922302 dist loss 0.06427311897277832 older dist loss 0.062412526458501816\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.13143543899059296 class loss 0.013764318078756332 dist loss 0.05799011141061783 older dist loss 0.059681009501218796\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.14694632589817047 class loss 0.015081742778420448 dist loss 0.06505681574344635 older dist loss 0.06680776923894882\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.1459425985813141 class loss 0.01396634429693222 dist loss 0.0634748563170433 older dist loss 0.06850139051675797\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.13656117022037506 class loss 0.012973833829164505 dist loss 0.06059729680418968 older dist loss 0.06299003213644028\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.1464032679796219 class loss 0.01246112585067749 dist loss 0.06579667329788208 older dist loss 0.06814546883106232\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.15022960305213928 class loss 0.01459509041160345 dist loss 0.06592191755771637 older dist loss 0.06971260160207748\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.14365297555923462 class loss 0.011551356874406338 dist loss 0.06346569210290909 older dist loss 0.06863593310117722\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.14200198650360107 class loss 0.01129991840571165 dist loss 0.06294368207454681 older dist loss 0.06775838136672974\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.13522681593894958 class loss 0.009812012314796448 dist loss 0.0618487112224102 older dist loss 0.06356608867645264\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.13850821554660797 class loss 0.011429737322032452 dist loss 0.06213432177901268 older dist loss 0.06494416296482086\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.13985306024551392 class loss 0.013999257236719131 dist loss 0.06215661019086838 older dist loss 0.06369718164205551\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.14675799012184143 class loss 0.013806510716676712 dist loss 0.06519947201013565 older dist loss 0.06775199621915817\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.14891164004802704 class loss 0.01888507977128029 dist loss 0.062410395592451096 older dist loss 0.06761615723371506\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.13456134498119354 class loss 0.011808729730546474 dist loss 0.060016196221113205 older dist loss 0.06273641437292099\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.14098581671714783 class loss 0.007650194689631462 dist loss 0.06419692188501358 older dist loss 0.06913870573043823\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.13775953650474548 class loss 0.005704482086002827 dist loss 0.06372539699077606 older dist loss 0.06832964718341827\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.13557597994804382 class loss 0.00896210502833128 dist loss 0.06183681637048721 older dist loss 0.06477705389261246\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.12550614774227142 class loss 0.005016901530325413 dist loss 0.05754293128848076 older dist loss 0.06294631212949753\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.12241997569799423 class loss 0.0038690618239343166 dist loss 0.05541832745075226 older dist loss 0.06313258409500122\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.12220533192157745 class loss 0.00644730543717742 dist loss 0.05604400858283043 older dist loss 0.05971401557326317\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.12429635971784592 class loss 0.0032252268865704536 dist loss 0.06000026315450668 older dist loss 0.061070870608091354\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.12645727396011353 class loss 0.004648216068744659 dist loss 0.059417955577373505 older dist loss 0.06239111348986626\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.11948739737272263 class loss 0.003071497194468975 dist loss 0.055381372570991516 older dist loss 0.06103452667593956\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.12268248200416565 class loss 0.0040304833091795444 dist loss 0.05753936991095543 older dist loss 0.06111263111233711\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.12731747329235077 class loss 0.00388524541631341 dist loss 0.0586843304336071 older dist loss 0.0647478923201561\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.12416794896125793 class loss 0.0027645155787467957 dist loss 0.05864938348531723 older dist loss 0.06275404989719391\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.12557052075862885 class loss 0.002020673593506217 dist loss 0.05833456665277481 older dist loss 0.06521528214216232\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.12671276926994324 class loss 0.004113960079848766 dist loss 0.05842432379722595 older dist loss 0.06417449563741684\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.12502935528755188 class loss 0.0020233229734003544 dist loss 0.0616007000207901 older dist loss 0.06140533834695816\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.12043379247188568 class loss 0.001532005495391786 dist loss 0.05651436746120453 older dist loss 0.06238741800189018\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.12322936952114105 class loss 0.003497237106785178 dist loss 0.057297587394714355 older dist loss 0.062434542924165726\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.12417256087064743 class loss 0.0026375201996415854 dist loss 0.05724169313907623 older dist loss 0.06429334729909897\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.11592437326908112 class loss 0.0015762121183797717 dist loss 0.05541855841875076 older dist loss 0.058929599821567535\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.12396354228258133 class loss 0.002687939442694187 dist loss 0.058691538870334625 older dist loss 0.06258406490087509\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.12208514660596848 class loss 0.0023382932413369417 dist loss 0.060206860303878784 older dist loss 0.05953999236226082\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.12629126012325287 class loss 0.0018937726272270083 dist loss 0.0592392161488533 older dist loss 0.06515827029943466\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.12354572117328644 class loss 0.0024632872082293034 dist loss 0.0590231791138649 older dist loss 0.062059253454208374\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.11751598864793777 class loss 0.0013382936595007777 dist loss 0.0542917400598526 older dist loss 0.061885952949523926\n",
            "Reducing each exemplar set to size: 67\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 84.84\n",
            "\n",
            "Test Accuracy (all groups seen so far): 64.80\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "NEW CLASS SAMPLES:  3650\n",
            "ALL DB:  7670\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.14231158792972565 class loss 0.03134208545088768 dist loss 0.06024332344532013 older dist loss 0.050726182758808136\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.14264577627182007 class loss 0.030656367540359497 dist loss 0.06018158048391342 older dist loss 0.051807839423418045\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.12993255257606506 class loss 0.02595043182373047 dist loss 0.056436970829963684 older dist loss 0.04754514619708061\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.13672298192977905 class loss 0.03078458085656166 dist loss 0.0571015328168869 older dist loss 0.04883686825633049\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.13605093955993652 class loss 0.029624050483107567 dist loss 0.05863372981548309 older dist loss 0.04779314994812012\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.1228119507431984 class loss 0.02154991216957569 dist loss 0.05583862215280533 older dist loss 0.045423414558172226\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.13252073526382446 class loss 0.02315557561814785 dist loss 0.061948664486408234 older dist loss 0.04741648584604263\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.14024081826210022 class loss 0.028618205338716507 dist loss 0.06256987899541855 older dist loss 0.04905272647738457\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.12705717980861664 class loss 0.022627828642725945 dist loss 0.05637926235795021 older dist loss 0.048050086945295334\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.12948279082775116 class loss 0.02265368029475212 dist loss 0.05857570469379425 older dist loss 0.04825340583920479\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.13185006380081177 class loss 0.02380361407995224 dist loss 0.060437560081481934 older dist loss 0.04760889336466789\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.13394297659397125 class loss 0.024307113140821457 dist loss 0.05920702964067459 older dist loss 0.050428833812475204\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.127174511551857 class loss 0.018972083926200867 dist loss 0.06024696305394173 older dist loss 0.047955472022295\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.12945561110973358 class loss 0.02172187529504299 dist loss 0.059166885912418365 older dist loss 0.048566848039627075\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.1253536194562912 class loss 0.018141578882932663 dist loss 0.058823827654123306 older dist loss 0.048388220369815826\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.11935088038444519 class loss 0.017173705622553825 dist loss 0.0567006841301918 older dist loss 0.04547648876905441\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.12449822574853897 class loss 0.015772422775626183 dist loss 0.059598080813884735 older dist loss 0.0491277240216732\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.12544900178909302 class loss 0.01954847387969494 dist loss 0.05862480774521828 older dist loss 0.047275714576244354\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.12258957326412201 class loss 0.01593519188463688 dist loss 0.05876664072275162 older dist loss 0.04788774251937866\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.11603038012981415 class loss 0.014849485829472542 dist loss 0.056762807071208954 older dist loss 0.0444180853664875\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.12604045867919922 class loss 0.01594243012368679 dist loss 0.0600401908159256 older dist loss 0.05005783960223198\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.12712886929512024 class loss 0.019358014687895775 dist loss 0.06019803509116173 older dist loss 0.04757281765341759\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.12491342425346375 class loss 0.013899934478104115 dist loss 0.062363963574171066 older dist loss 0.04864951968193054\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.12991875410079956 class loss 0.017946040257811546 dist loss 0.06203413009643555 older dist loss 0.049938589334487915\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.1227852925658226 class loss 0.015888018533587456 dist loss 0.06022297590970993 older dist loss 0.046674296259880066\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.11816893517971039 class loss 0.012372978962957859 dist loss 0.059663474559783936 older dist loss 0.04613248631358147\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.1233963817358017 class loss 0.013875392265617847 dist loss 0.06098848208785057 older dist loss 0.048532504588365555\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.11219672858715057 class loss 0.011754591949284077 dist loss 0.0561419278383255 older dist loss 0.04430020973086357\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.11657018214464188 class loss 0.01307071652263403 dist loss 0.056884702295064926 older dist loss 0.046614762395620346\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.12770593166351318 class loss 0.01446508802473545 dist loss 0.06274519860744476 older dist loss 0.050495635718107224\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.11663500964641571 class loss 0.009770526550710201 dist loss 0.060390885919332504 older dist loss 0.04647359251976013\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.12111619114875793 class loss 0.011514946818351746 dist loss 0.06063409894704819 older dist loss 0.048967145383358\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.12581798434257507 class loss 0.013724468648433685 dist loss 0.061824776232242584 older dist loss 0.0502687469124794\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.12505841255187988 class loss 0.014894693158566952 dist loss 0.06073145940899849 older dist loss 0.04943225532770157\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.11672766506671906 class loss 0.010066158138215542 dist loss 0.059834226965904236 older dist loss 0.046827275305986404\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.11841987073421478 class loss 0.010186120867729187 dist loss 0.05984209477901459 older dist loss 0.04839165881276131\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.11779405176639557 class loss 0.009257002733647823 dist loss 0.059204522520303726 older dist loss 0.049332525581121445\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.11203319579362869 class loss 0.008922825567424297 dist loss 0.058575935661792755 older dist loss 0.044534433633089066\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.12374506890773773 class loss 0.01367498654872179 dist loss 0.06147051602602005 older dist loss 0.04859956353902817\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.12585535645484924 class loss 0.011313590221107006 dist loss 0.06445792317390442 older dist loss 0.050083842128515244\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.12077035009860992 class loss 0.01254013180732727 dist loss 0.05894584581255913 older dist loss 0.049284372478723526\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.12324392795562744 class loss 0.012712269090116024 dist loss 0.06171092391014099 older dist loss 0.04882073402404785\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.11454452574253082 class loss 0.00912255048751831 dist loss 0.059010446071624756 older dist loss 0.046411529183387756\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.11927513778209686 class loss 0.011367802508175373 dist loss 0.06016922369599342 older dist loss 0.04773810878396034\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.11876755952835083 class loss 0.012107709422707558 dist loss 0.06003619730472565 older dist loss 0.046623650938272476\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.12676018476486206 class loss 0.015018976293504238 dist loss 0.06270258128643036 older dist loss 0.049038637429475784\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.11839861422777176 class loss 0.009604492224752903 dist loss 0.06002085283398628 older dist loss 0.04877327010035515\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.1165200024843216 class loss 0.009006275795400143 dist loss 0.059680819511413574 older dist loss 0.0478329062461853\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.12082494050264359 class loss 0.011334771290421486 dist loss 0.061455026268959045 older dist loss 0.048035141080617905\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.10823912918567657 class loss 0.005456101149320602 dist loss 0.05756841599941254 older dist loss 0.045214612036943436\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.10536006838083267 class loss 0.004339729901403189 dist loss 0.05625935643911362 older dist loss 0.04476098343729973\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.10353145003318787 class loss 0.00446224445477128 dist loss 0.05467963591217995 older dist loss 0.044389571994543076\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.10235299170017242 class loss 0.002979888580739498 dist loss 0.05546769127249718 older dist loss 0.04390541464090347\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.10357456654310226 class loss 0.003812501672655344 dist loss 0.05370284616947174 older dist loss 0.04605921730399132\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.10163005441427231 class loss 0.00275237369351089 dist loss 0.05493084713816643 older dist loss 0.04394683241844177\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.10773991048336029 class loss 0.0025938113685697317 dist loss 0.05925608426332474 older dist loss 0.04589001461863518\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.10246901959180832 class loss 0.002749444218352437 dist loss 0.054713085293769836 older dist loss 0.045006491243839264\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.10744045674800873 class loss 0.00396340899169445 dist loss 0.058038096874952316 older dist loss 0.04543895646929741\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.100900799036026 class loss 0.0029504939448088408 dist loss 0.05461635813117027 older dist loss 0.04333394393324852\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.11204612255096436 class loss 0.004099484067410231 dist loss 0.060140274465084076 older dist loss 0.04780635982751846\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.10101227462291718 class loss 0.0023994147777557373 dist loss 0.05363694950938225 older dist loss 0.04497591033577919\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.10102810710668564 class loss 0.002683724043890834 dist loss 0.053997702896595 older dist loss 0.044346679002046585\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.10422100871801376 class loss 0.0025291370693594217 dist loss 0.056461673229932785 older dist loss 0.0452301986515522\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.09898468106985092 class loss 0.002258587395772338 dist loss 0.053717102855443954 older dist loss 0.043008990585803986\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.10229665040969849 class loss 0.001773394295014441 dist loss 0.05589914321899414 older dist loss 0.04462410882115364\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.10013726353645325 class loss 0.0015658943448215723 dist loss 0.05491996556520462 older dist loss 0.04365139827132225\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.10185021162033081 class loss 0.002275132806971669 dist loss 0.055152714252471924 older dist loss 0.044422369450330734\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09944187849760056 class loss 0.0011486875591799617 dist loss 0.05521578714251518 older dist loss 0.04307740554213524\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0986563116312027 class loss 0.002725990256294608 dist loss 0.05369151011109352 older dist loss 0.042238812893629074\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09868597239255905 class loss 0.0017322590574622154 dist loss 0.0521591454744339 older dist loss 0.04479456692934036\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 76.12\n",
            "\n",
            "Test Accuracy (all groups seen so far): 59.50\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "NEW CLASS SAMPLES:  3200\n",
            "ALL DB:  7200\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.12399012595415115 class loss 0.0248248390853405 dist loss 0.05763975530862808 older dist loss 0.04152553528547287\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.11561121791601181 class loss 0.02137685939669609 dist loss 0.05563642457127571 older dist loss 0.03859793394804001\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.1155252605676651 class loss 0.015961645171046257 dist loss 0.05800818279385567 older dist loss 0.041555438190698624\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.11684887111186981 class loss 0.015403538011014462 dist loss 0.05935990437865257 older dist loss 0.0420854315161705\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.11521190404891968 class loss 0.016039913520216942 dist loss 0.05838000401854515 older dist loss 0.04079199209809303\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.11226383596658707 class loss 0.015325616113841534 dist loss 0.057144541293382645 older dist loss 0.039793677628040314\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.11325816065073013 class loss 0.017218416556715965 dist loss 0.056688278913497925 older dist loss 0.03935146704316139\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.11461792141199112 class loss 0.01704079657793045 dist loss 0.05785829573869705 older dist loss 0.03971882909536362\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.10506336390972137 class loss 0.011888702400028706 dist loss 0.055276233702898026 older dist loss 0.03789842501282692\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.12170421332120895 class loss 0.017906775698065758 dist loss 0.060675449669361115 older dist loss 0.04312198609113693\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.11147215217351913 class loss 0.012308456934988499 dist loss 0.05888456106185913 older dist loss 0.04027913510799408\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.11510752141475677 class loss 0.01432136632502079 dist loss 0.0588940791785717 older dist loss 0.041892074048519135\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.12022671103477478 class loss 0.019636427983641624 dist loss 0.059564825147390366 older dist loss 0.04102545976638794\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.11165574193000793 class loss 0.011874969117343426 dist loss 0.0585702620446682 older dist loss 0.04121050983667374\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.1116458922624588 class loss 0.011093159206211567 dist loss 0.059729188680648804 older dist loss 0.04082354158163071\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.11641854792833328 class loss 0.012687874026596546 dist loss 0.06186502054333687 older dist loss 0.04186565428972244\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.11199844628572464 class loss 0.01155554037541151 dist loss 0.05976255610585213 older dist loss 0.04068034887313843\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.10933776199817657 class loss 0.010500004515051842 dist loss 0.058777160942554474 older dist loss 0.04006059840321541\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.10565564781427383 class loss 0.009560870938003063 dist loss 0.05770600587129593 older dist loss 0.03838877007365227\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.11328436434268951 class loss 0.011759418062865734 dist loss 0.05985217168927193 older dist loss 0.04167277365922928\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.11216991394758224 class loss 0.012097644619643688 dist loss 0.05968411639332771 older dist loss 0.04038815200328827\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.10599055886268616 class loss 0.0071149724535644054 dist loss 0.05853763967752457 older dist loss 0.0403379425406456\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.11023520678281784 class loss 0.008164349012076855 dist loss 0.060298122465610504 older dist loss 0.04177273437380791\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.11032187193632126 class loss 0.007843404076993465 dist loss 0.0620134174823761 older dist loss 0.04046504944562912\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.11154390871524811 class loss 0.009300285018980503 dist loss 0.06124062463641167 older dist loss 0.04100299999117851\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.10775625705718994 class loss 0.00872880406677723 dist loss 0.05844736844301224 older dist loss 0.040580082684755325\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.10991420596837997 class loss 0.009583894163370132 dist loss 0.05967983603477478 older dist loss 0.04065047577023506\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.11140526831150055 class loss 0.007139310240745544 dist loss 0.06194379925727844 older dist loss 0.04232216253876686\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.10896104574203491 class loss 0.005953347310423851 dist loss 0.06135484203696251 older dist loss 0.041652861982584\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.1071435958147049 class loss 0.007736505009233952 dist loss 0.05967511609196663 older dist loss 0.03973197564482689\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.1095084547996521 class loss 0.006957586854696274 dist loss 0.062123823910951614 older dist loss 0.04042704403400421\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.1121717244386673 class loss 0.010226568207144737 dist loss 0.061197441071271896 older dist loss 0.04074771702289581\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.1037549152970314 class loss 0.005702379159629345 dist loss 0.05812153220176697 older dist loss 0.039931003004312515\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.11228027939796448 class loss 0.00821762252599001 dist loss 0.06181513890624046 older dist loss 0.04224751889705658\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.10717970132827759 class loss 0.006799802184104919 dist loss 0.059134479612112045 older dist loss 0.04124542325735092\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.11106699705123901 class loss 0.009107115678489208 dist loss 0.06026698276400566 older dist loss 0.04169289395213127\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.1131192147731781 class loss 0.0080855218693614 dist loss 0.06255819648504257 older dist loss 0.042475491762161255\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.11395678669214249 class loss 0.0073722051456570625 dist loss 0.0636335089802742 older dist loss 0.0429510734975338\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.1052892804145813 class loss 0.0051583475433290005 dist loss 0.060096174478530884 older dist loss 0.04003475606441498\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.1073237806558609 class loss 0.0045796590857207775 dist loss 0.06091558560729027 older dist loss 0.04182853177189827\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.11159785091876984 class loss 0.006037147715687752 dist loss 0.06141364201903343 older dist loss 0.044147055596113205\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.11519865691661835 class loss 0.007323877420276403 dist loss 0.06409503519535065 older dist loss 0.04377974197268486\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.1049860417842865 class loss 0.0032060754019767046 dist loss 0.06061858683824539 older dist loss 0.04116138070821762\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.10775856673717499 class loss 0.004260387737303972 dist loss 0.061530839651823044 older dist loss 0.04196733608841896\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.10607700049877167 class loss 0.005951387342065573 dist loss 0.059724755585193634 older dist loss 0.040400855243206024\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.11096887290477753 class loss 0.008054014295339584 dist loss 0.060739316046237946 older dist loss 0.042175546288490295\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.1069616973400116 class loss 0.007067125290632248 dist loss 0.059531278908252716 older dist loss 0.04036329686641693\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.11329707503318787 class loss 0.009974895976483822 dist loss 0.06282597035169601 older dist loss 0.040496207773685455\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.10654991120100021 class loss 0.006979549769312143 dist loss 0.059392351657152176 older dist loss 0.04017800837755203\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0993013083934784 class loss 0.0028946108650416136 dist loss 0.05735442042350769 older dist loss 0.03905228152871132\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.09937310218811035 class loss 0.002792045008391142 dist loss 0.05698248744010925 older dist loss 0.03959856554865837\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.09642086923122406 class loss 0.002414013957604766 dist loss 0.055122531950473785 older dist loss 0.038884326815605164\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.10178114473819733 class loss 0.002132896101102233 dist loss 0.05989253148436546 older dist loss 0.039755720645189285\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.09538526833057404 class loss 0.0017503276467323303 dist loss 0.05530714988708496 older dist loss 0.038327787071466446\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.09559925645589828 class loss 0.0021430980414152145 dist loss 0.054904766380786896 older dist loss 0.038551393896341324\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.0951143205165863 class loss 0.0019751153886318207 dist loss 0.05548065900802612 older dist loss 0.03765854239463806\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.0961042195558548 class loss 0.002255804371088743 dist loss 0.055984824895858765 older dist loss 0.037863589823246\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.09497120976448059 class loss 0.0024890971835702658 dist loss 0.05496973916888237 older dist loss 0.03751237690448761\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.09930606186389923 class loss 0.0017437756760045886 dist loss 0.057756345719099045 older dist loss 0.03980593755841255\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.09598277509212494 class loss 0.0019076860044151545 dist loss 0.056060027331113815 older dist loss 0.038015056401491165\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.09409883618354797 class loss 0.001126679708249867 dist loss 0.055133432149887085 older dist loss 0.03783872351050377\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.0957617238163948 class loss 0.0016579816583544016 dist loss 0.056041985750198364 older dist loss 0.038061756640672684\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.09653034806251526 class loss 0.001241160905919969 dist loss 0.05581922084093094 older dist loss 0.03946996107697487\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.09448689222335815 class loss 0.002057807520031929 dist loss 0.05543952062726021 older dist loss 0.03698955848813057\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09302228689193726 class loss 0.0015508822398260236 dist loss 0.05506562814116478 older dist loss 0.0364057756960392\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09775044023990631 class loss 0.0011575637618079782 dist loss 0.057043105363845825 older dist loss 0.039549775421619415\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09719569236040115 class loss 0.001878448179922998 dist loss 0.056127894669771194 older dist loss 0.03918934985995293\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09627227485179901 class loss 0.0014395348262041807 dist loss 0.05672610551118851 older dist loss 0.038106631487607956\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09656891971826553 class loss 0.0008385069668292999 dist loss 0.05694068595767021 older dist loss 0.03878972679376602\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09782318770885468 class loss 0.001466353889554739 dist loss 0.05730978772044182 older dist loss 0.03904705122113228\n",
            "Reducing each exemplar set to size: 40\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 67.68\n",
            "\n",
            "Test Accuracy (all groups seen so far): 54.80\n",
            "\n",
            "the model knows 50 classes:\n",
            " \n",
            "GROUP:  6\n",
            "NEW CLASS SAMPLES:  2750\n",
            "ALL DB:  6750\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.11382260918617249 class loss 0.01944524049758911 dist loss 0.05618090182542801 older dist loss 0.03819647058844566\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.10980947315692902 class loss 0.017230434343218803 dist loss 0.05483168363571167 older dist loss 0.03774735704064369\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.11052954196929932 class loss 0.016466379165649414 dist loss 0.056581079959869385 older dist loss 0.037482086569070816\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.10277257859706879 class loss 0.010178190656006336 dist loss 0.056126922369003296 older dist loss 0.03646746650338173\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.10388501733541489 class loss 0.01208286639302969 dist loss 0.055555034428834915 older dist loss 0.03624711558222771\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.10306023061275482 class loss 0.011549855582416058 dist loss 0.054569050669670105 older dist loss 0.036941319704055786\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.1073840782046318 class loss 0.014438590966165066 dist loss 0.0562954880297184 older dist loss 0.036649998277425766\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.10577665269374847 class loss 0.01135255303233862 dist loss 0.05677632987499237 older dist loss 0.03764776512980461\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.10914624482393265 class loss 0.012625003233551979 dist loss 0.05764372646808624 older dist loss 0.03887751325964928\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.10736562311649323 class loss 0.010625001043081284 dist loss 0.0586886927485466 older dist loss 0.03805192932486534\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.10204974561929703 class loss 0.008792208507657051 dist loss 0.05657608062028885 older dist loss 0.03668145462870598\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.11200760304927826 class loss 0.01241398137062788 dist loss 0.059516578912734985 older dist loss 0.04007703810930252\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.10449887812137604 class loss 0.00996219553053379 dist loss 0.05718551576137543 older dist loss 0.03735116869211197\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.10304014384746552 class loss 0.008082249201834202 dist loss 0.05754907801747322 older dist loss 0.03740881755948067\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.10125011205673218 class loss 0.007789802737534046 dist loss 0.055918630212545395 older dist loss 0.03754167631268501\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.10299740731716156 class loss 0.007543554529547691 dist loss 0.05771418288350105 older dist loss 0.037739671766757965\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.10715876519680023 class loss 0.010995624586939812 dist loss 0.05824969336390495 older dist loss 0.03791344165802002\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.09862344712018967 class loss 0.005956128239631653 dist loss 0.05610451474785805 older dist loss 0.036562804132699966\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.10253289341926575 class loss 0.008739436976611614 dist loss 0.05615919083356857 older dist loss 0.03763426095247269\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.10562846064567566 class loss 0.007465284317731857 dist loss 0.05974460765719414 older dist loss 0.03841857239603996\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.10659341514110565 class loss 0.008705129846930504 dist loss 0.05907271429896355 older dist loss 0.038815565407276154\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.10025881975889206 class loss 0.006619342602789402 dist loss 0.05641724169254303 older dist loss 0.0372222363948822\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.10745786130428314 class loss 0.009132402017712593 dist loss 0.0600845105946064 older dist loss 0.0382409431040287\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.1004946231842041 class loss 0.007182707078754902 dist loss 0.05674886703491211 older dist loss 0.036563050001859665\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.1034487932920456 class loss 0.005989506375044584 dist loss 0.058788154274225235 older dist loss 0.03867113217711449\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.10528546571731567 class loss 0.004976589698344469 dist loss 0.06066177412867546 older dist loss 0.03964710235595703\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.10557614266872406 class loss 0.007149732671678066 dist loss 0.058954015374183655 older dist loss 0.03947239741683006\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.10369988530874252 class loss 0.004404439590871334 dist loss 0.05978064611554146 older dist loss 0.039514798671007156\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.10421496629714966 class loss 0.006682145409286022 dist loss 0.059037331491708755 older dist loss 0.038495492190122604\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.10279712080955505 class loss 0.004854289814829826 dist loss 0.059293344616889954 older dist loss 0.03864948824048042\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.10710080713033676 class loss 0.006942366249859333 dist loss 0.06068112701177597 older dist loss 0.03947731480002403\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.10503280162811279 class loss 0.006189526058733463 dist loss 0.0599675178527832 older dist loss 0.0388757586479187\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.09876143932342529 class loss 0.004488599486649036 dist loss 0.05703359842300415 older dist loss 0.03723924234509468\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.10574104636907578 class loss 0.005118630826473236 dist loss 0.06097370758652687 older dist loss 0.03964870795607567\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.10277241468429565 class loss 0.0054097967222332954 dist loss 0.05891742557287216 older dist loss 0.03844518959522247\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.10383277386426926 class loss 0.004898448474705219 dist loss 0.06031522527337074 older dist loss 0.03861910104751587\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.10243214666843414 class loss 0.004714103881269693 dist loss 0.05917096510529518 older dist loss 0.038547080010175705\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.10272300243377686 class loss 0.0058846240863204 dist loss 0.05800185725092888 older dist loss 0.0388365164399147\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.09890873730182648 class loss 0.0034963912330567837 dist loss 0.05791833624243736 older dist loss 0.03749401494860649\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.1013442650437355 class loss 0.003502804320305586 dist loss 0.05973883345723152 older dist loss 0.03810262680053711\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.10074861347675323 class loss 0.00360741070471704 dist loss 0.05926972255110741 older dist loss 0.03787148371338844\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.1011405661702156 class loss 0.005796641111373901 dist loss 0.057644929736852646 older dist loss 0.03769899532198906\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.0984719768166542 class loss 0.0042067887261509895 dist loss 0.057238176465034485 older dist loss 0.037027012556791306\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.10380914807319641 class loss 0.003638715483248234 dist loss 0.06007227301597595 older dist loss 0.0400981605052948\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.10298071056604385 class loss 0.003327547339722514 dist loss 0.060301464051008224 older dist loss 0.0393516980111599\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.09862115234136581 class loss 0.004706568084657192 dist loss 0.05688929930329323 older dist loss 0.03702528402209282\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.09951919317245483 class loss 0.002865514252334833 dist loss 0.05922599136829376 older dist loss 0.03742768242955208\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.0988040417432785 class loss 0.0033181244507431984 dist loss 0.05752220004796982 older dist loss 0.03796371445059776\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.10027599334716797 class loss 0.002754214685410261 dist loss 0.05952104926109314 older dist loss 0.03800073266029358\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09842886030673981 class loss 0.0023862742818892 dist loss 0.05793808028101921 older dist loss 0.038104504346847534\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.09345239400863647 class loss 0.0015033879317343235 dist loss 0.05531805008649826 older dist loss 0.03663095831871033\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.09386497735977173 class loss 0.0016572567401453853 dist loss 0.05576740950345993 older dist loss 0.03644030913710594\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.0937698632478714 class loss 0.0012431902578100562 dist loss 0.05611760541796684 older dist loss 0.03640906885266304\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.09688924252986908 class loss 0.002076612086966634 dist loss 0.056960973888635635 older dist loss 0.03785166144371033\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.0929902195930481 class loss 0.0011659965384751558 dist loss 0.05542778596282005 older dist loss 0.03639643266797066\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.09277790784835815 class loss 0.0019256843952462077 dist loss 0.05487358197569847 older dist loss 0.035978641360998154\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.09261472523212433 class loss 0.0017553505022078753 dist loss 0.054788365960121155 older dist loss 0.036071013659238815\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.09513455629348755 class loss 0.001269304077140987 dist loss 0.056540511548519135 older dist loss 0.03732474520802498\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.09582577645778656 class loss 0.0012801130069419742 dist loss 0.05663081258535385 older dist loss 0.03791485354304314\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.09386655688285828 class loss 0.001115570543333888 dist loss 0.05524509772658348 older dist loss 0.03750588372349739\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.0910942405462265 class loss 0.0012303004041314125 dist loss 0.05359797924757004 older dist loss 0.036265961825847626\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.09686916321516037 class loss 0.0013787377392873168 dist loss 0.057502709329128265 older dist loss 0.03798771649599075\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.0902048796415329 class loss 0.0008990091737359762 dist loss 0.05382669344544411 older dist loss 0.03547917678952217\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.09192413836717606 class loss 0.0013367662904784083 dist loss 0.05466331169009209 older dist loss 0.03592406213283539\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09124626219272614 class loss 0.0009900479344651103 dist loss 0.05463783070445061 older dist loss 0.035618383437395096\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09281817078590393 class loss 0.0010782269528135657 dist loss 0.05540299415588379 older dist loss 0.03633694723248482\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09056363999843597 class loss 0.0017267720540985465 dist loss 0.05366954952478409 older dist loss 0.03516731783747673\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09536337852478027 class loss 0.0027323642279952765 dist loss 0.056081078946590424 older dist loss 0.03654994070529938\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09049050509929657 class loss 0.0007913751760497689 dist loss 0.0542418397963047 older dist loss 0.035457294434309006\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09033140540122986 class loss 0.0011969080660492182 dist loss 0.05451151356101036 older dist loss 0.034622982144355774\n",
            "Reducing each exemplar set to size: 34\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 62.78\n",
            "\n",
            "Test Accuracy (all groups seen so far): 51.70\n",
            "\n",
            "the model knows 60 classes:\n",
            " \n",
            "GROUP:  7\n",
            "NEW CLASS SAMPLES:  2300\n",
            "ALL DB:  6380\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.10283121466636658 class loss 0.012467528693377972 dist loss 0.05452774465084076 older dist loss 0.03583593666553497\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.10454005002975464 class loss 0.012602024711668491 dist loss 0.0561593659222126 older dist loss 0.03577866032719612\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.098252072930336 class loss 0.008980737999081612 dist loss 0.05441156029701233 older dist loss 0.03485977277159691\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.09957708418369293 class loss 0.010165666230022907 dist loss 0.054712072014808655 older dist loss 0.034699343144893646\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.10051053762435913 class loss 0.009862866252660751 dist loss 0.05564315989613533 older dist loss 0.03500451520085335\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.10122666507959366 class loss 0.010192422196269035 dist loss 0.05550922453403473 older dist loss 0.03552502021193504\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.09954816102981567 class loss 0.008619191125035286 dist loss 0.055758554488420486 older dist loss 0.03517041727900505\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.10300610959529877 class loss 0.008510012179613113 dist loss 0.05772875249385834 older dist loss 0.03676734119653702\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.09950928390026093 class loss 0.007240575272589922 dist loss 0.05658262223005295 older dist loss 0.03568609058856964\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.09838555753231049 class loss 0.005502877291291952 dist loss 0.05669800937175751 older dist loss 0.03618466854095459\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.09562747180461884 class loss 0.006308501586318016 dist loss 0.05527179688215256 older dist loss 0.03404717519879341\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.09763360023498535 class loss 0.00545126385986805 dist loss 0.05677930638194084 older dist loss 0.03540303185582161\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.10028854012489319 class loss 0.006889883894473314 dist loss 0.05742139369249344 older dist loss 0.035977259278297424\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.10221875458955765 class loss 0.006062780506908894 dist loss 0.0590294674038887 older dist loss 0.037126507610082626\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.0989181250333786 class loss 0.006212152075022459 dist loss 0.056644830852746964 older dist loss 0.03606114163994789\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.10545482486486435 class loss 0.007951337844133377 dist loss 0.0595979169011116 older dist loss 0.03790557011961937\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.09763175249099731 class loss 0.004447028040885925 dist loss 0.057428568601608276 older dist loss 0.03575615957379341\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.1005779504776001 class loss 0.006703059189021587 dist loss 0.05714033916592598 older dist loss 0.03673454746603966\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.09886283427476883 class loss 0.006368000991642475 dist loss 0.05715462937951088 older dist loss 0.03534020483493805\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.09658341109752655 class loss 0.0042747026309370995 dist loss 0.05652765929698944 older dist loss 0.03578104451298714\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.0986136943101883 class loss 0.005574711598455906 dist loss 0.0575949028134346 older dist loss 0.03544408082962036\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.09935329854488373 class loss 0.00528573477640748 dist loss 0.05702809989452362 older dist loss 0.037039466202259064\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.09865979850292206 class loss 0.004871902521699667 dist loss 0.05760212987661362 older dist loss 0.03618576377630234\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.09867565333843231 class loss 0.005633191205561161 dist loss 0.05761817470192909 older dist loss 0.03542429208755493\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.09688645601272583 class loss 0.005332041997462511 dist loss 0.05617301166057587 older dist loss 0.03538140282034874\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.09853556752204895 class loss 0.003974013961851597 dist loss 0.05758172646164894 older dist loss 0.03697982802987099\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.09972633421421051 class loss 0.005107957404106855 dist loss 0.058507855981588364 older dist loss 0.0361105240881443\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.09894990921020508 class loss 0.0033788627479225397 dist loss 0.058590520173311234 older dist loss 0.0369805209338665\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.09943358600139618 class loss 0.0040890839882195 dist loss 0.05870937928557396 older dist loss 0.03663512319326401\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.09648603200912476 class loss 0.004254807718098164 dist loss 0.056389205157756805 older dist loss 0.03584201633930206\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.09981539845466614 class loss 0.004381635691970587 dist loss 0.05885019525885582 older dist loss 0.03658356890082359\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.09596888720989227 class loss 0.0029493968468159437 dist loss 0.05685891583561897 older dist loss 0.03616057336330414\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.09520989656448364 class loss 0.0018526563653722405 dist loss 0.057258717715740204 older dist loss 0.03609852492809296\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.09733642637729645 class loss 0.0033755365293473005 dist loss 0.05853493511676788 older dist loss 0.035425953567028046\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.09945227205753326 class loss 0.003826697589829564 dist loss 0.0590815544128418 older dist loss 0.03654401749372482\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.0985342264175415 class loss 0.0031702774576842785 dist loss 0.05844050645828247 older dist loss 0.03692344203591347\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.09710007160902023 class loss 0.003127537202090025 dist loss 0.05793905630707741 older dist loss 0.03603347763419151\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.09492161870002747 class loss 0.002855636877939105 dist loss 0.05707194283604622 older dist loss 0.03499404340982437\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.09500090777873993 class loss 0.0031087277457118034 dist loss 0.05661724880337715 older dist loss 0.0352749340236187\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.09572263807058334 class loss 0.0036777090281248093 dist loss 0.056052543222904205 older dist loss 0.03599238395690918\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.1004042774438858 class loss 0.0033356330823153257 dist loss 0.059865161776542664 older dist loss 0.037203483283519745\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.09616781771183014 class loss 0.003927567508071661 dist loss 0.057181425392627716 older dist loss 0.03505882993340492\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.09921495616436005 class loss 0.003987208008766174 dist loss 0.05832401663064957 older dist loss 0.03690372779965401\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.09744055569171906 class loss 0.0028860396705567837 dist loss 0.058114249259233475 older dist loss 0.03644026443362236\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.0974494218826294 class loss 0.002110686618834734 dist loss 0.0592520609498024 older dist loss 0.03608667105436325\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.0959215760231018 class loss 0.0032981163822114468 dist loss 0.05761846899986267 older dist loss 0.03500499576330185\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.09537836909294128 class loss 0.003156528575345874 dist loss 0.05689697712659836 older dist loss 0.03532486408948898\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.09764660894870758 class loss 0.0030072845984250307 dist loss 0.05793082341551781 older dist loss 0.03670850023627281\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.09863467514514923 class loss 0.002350910333916545 dist loss 0.05978500097990036 older dist loss 0.036498766392469406\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09163212776184082 class loss 0.0010562671814113855 dist loss 0.05530702322721481 older dist loss 0.035268835723400116\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.09207336604595184 class loss 0.001593658933416009 dist loss 0.0555984266102314 older dist loss 0.03488127514719963\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.08892776072025299 class loss 0.0011681191390380263 dist loss 0.054188963025808334 older dist loss 0.03357067331671715\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.09034839272499084 class loss 0.0013188817538321018 dist loss 0.05486086755990982 older dist loss 0.03416864573955536\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.0886228159070015 class loss 0.001354794716462493 dist loss 0.053495410829782486 older dist loss 0.03377261012792587\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.0894622802734375 class loss 0.0010089834686368704 dist loss 0.054674193263053894 older dist loss 0.03377910703420639\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.09057507663965225 class loss 0.0016377525171265006 dist loss 0.054587844759225845 older dist loss 0.0343494787812233\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.09066115319728851 class loss 0.0008748949621804059 dist loss 0.055554457008838654 older dist loss 0.034231800585985184\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.09368488192558289 class loss 0.0011836414923891425 dist loss 0.05723468214273453 older dist loss 0.035266559571027756\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.09045384079217911 class loss 0.0009117965819314122 dist loss 0.05475418269634247 older dist loss 0.0347878597676754\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.09034712612628937 class loss 0.0009299695957452059 dist loss 0.05514306202530861 older dist loss 0.03427409008145332\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.0914829894900322 class loss 0.0007583818514831364 dist loss 0.05599682405591011 older dist loss 0.034727782011032104\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.09307753294706345 class loss 0.0014820134965702891 dist loss 0.05654814466834068 older dist loss 0.03504737466573715\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.09178771078586578 class loss 0.0009718878427520394 dist loss 0.05636540427803993 older dist loss 0.03445041552186012\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.08836077153682709 class loss 0.0007808861555531621 dist loss 0.05366696044802666 older dist loss 0.03391292318701744\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08964984118938446 class loss 0.0009005603496916592 dist loss 0.054662469774484634 older dist loss 0.034086812287569046\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08960142731666565 class loss 0.0006248854915611446 dist loss 0.05500189587473869 older dist loss 0.033974651247262955\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08927741646766663 class loss 0.0008584497263655066 dist loss 0.05432489886879921 older dist loss 0.03409406915307045\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09045858681201935 class loss 0.000568505609408021 dist loss 0.05522158741950989 older dist loss 0.03466849401593208\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08956469595432281 class loss 0.0014371456345543265 dist loss 0.054573286324739456 older dist loss 0.03355426713824272\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08969506621360779 class loss 0.0006726446445100009 dist loss 0.05485106632113457 older dist loss 0.03417135775089264\n",
            "Reducing each exemplar set to size: 29\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 59.64\n",
            "\n",
            "Test Accuracy (all groups seen so far): 49.19\n",
            "\n",
            "the model knows 70 classes:\n",
            " \n",
            "GROUP:  8\n",
            "NEW CLASS SAMPLES:  1850\n",
            "ALL DB:  5910\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.09966635704040527 class loss 0.012287203222513199 dist loss 0.05375416576862335 older dist loss 0.033624984323978424\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.0959949642419815 class loss 0.008539444766938686 dist loss 0.05452541261911392 older dist loss 0.03293010592460632\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.10019470751285553 class loss 0.009827959351241589 dist loss 0.05611775815486908 older dist loss 0.03424898535013199\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.09973537176847458 class loss 0.010040851309895515 dist loss 0.05492731183767319 older dist loss 0.03476720675826073\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.09692132472991943 class loss 0.00940917432308197 dist loss 0.0543428510427475 older dist loss 0.03316929563879967\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.09582003951072693 class loss 0.008093944750726223 dist loss 0.054773952811956406 older dist loss 0.032952144742012024\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.09540184587240219 class loss 0.008183193393051624 dist loss 0.054196249693632126 older dist loss 0.033022403717041016\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.09512384980916977 class loss 0.006737672723829746 dist loss 0.055023886263370514 older dist loss 0.033362291753292084\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.10039246827363968 class loss 0.009009112603962421 dist loss 0.05626934766769409 older dist loss 0.03511400893330574\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.09767893701791763 class loss 0.009737558662891388 dist loss 0.05443829670548439 older dist loss 0.033503081649541855\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.10270033776760101 class loss 0.011114823631942272 dist loss 0.05678334832191467 older dist loss 0.034802161157131195\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.09515416622161865 class loss 0.008267355151474476 dist loss 0.053891800343990326 older dist loss 0.032995015382766724\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.0948987528681755 class loss 0.005011954344809055 dist loss 0.055580444633960724 older dist loss 0.0343063548207283\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.09505943208932877 class loss 0.0048528448678553104 dist loss 0.055846720933914185 older dist loss 0.03435986489057541\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.09555463492870331 class loss 0.007527665700763464 dist loss 0.0546969398856163 older dist loss 0.033330027014017105\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.09809894114732742 class loss 0.005760909523814917 dist loss 0.05748558044433594 older dist loss 0.03485245257616043\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.0957288146018982 class loss 0.005672052036970854 dist loss 0.05599444359540939 older dist loss 0.03406231850385666\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.1002209484577179 class loss 0.007966512814164162 dist loss 0.05711781978607178 older dist loss 0.03513661399483681\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.09311898052692413 class loss 0.004628614988178015 dist loss 0.054989662021398544 older dist loss 0.033500704914331436\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.09768684208393097 class loss 0.0050459676422178745 dist loss 0.05760176479816437 older dist loss 0.035039108246564865\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.09606753289699554 class loss 0.004665096756070852 dist loss 0.05637186020612717 older dist loss 0.03503057360649109\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.09471239894628525 class loss 0.005754958372563124 dist loss 0.05515335500240326 older dist loss 0.03380408510565758\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.09578953683376312 class loss 0.005905437748879194 dist loss 0.05573400482535362 older dist loss 0.03415009751915932\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.09496312588453293 class loss 0.004464719444513321 dist loss 0.05658522993326187 older dist loss 0.033913176506757736\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.09645086526870728 class loss 0.00529566640034318 dist loss 0.05659285932779312 older dist loss 0.034562334418296814\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.09876509010791779 class loss 0.005648327525705099 dist loss 0.057722724974155426 older dist loss 0.03539404273033142\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.09507032483816147 class loss 0.0047942823730409145 dist loss 0.05573578178882599 older dist loss 0.03454026207327843\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.0967978835105896 class loss 0.0030612878035753965 dist loss 0.05780762434005737 older dist loss 0.03592897579073906\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.09307493269443512 class loss 0.004731971304863691 dist loss 0.05503544583916664 older dist loss 0.0333075150847435\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.09167435765266418 class loss 0.004046603571623564 dist loss 0.054530274122953415 older dist loss 0.03309747576713562\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.09465207904577255 class loss 0.0038239152636379004 dist loss 0.05602443218231201 older dist loss 0.034803733229637146\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.0942174419760704 class loss 0.004028055351227522 dist loss 0.05621790885925293 older dist loss 0.033971477299928665\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.09269419312477112 class loss 0.0031706809531897306 dist loss 0.05545654892921448 older dist loss 0.03406696394085884\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.09545266628265381 class loss 0.0037322682328522205 dist loss 0.05721873790025711 older dist loss 0.034501660615205765\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.0909154862165451 class loss 0.002909679664298892 dist loss 0.054647743701934814 older dist loss 0.03335805982351303\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.09257644414901733 class loss 0.002510149497538805 dist loss 0.05568785220384598 older dist loss 0.03437844291329384\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.09132083505392075 class loss 0.002205612836405635 dist loss 0.0550667904317379 older dist loss 0.03404843062162399\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.0878271833062172 class loss 0.002476954599842429 dist loss 0.05294303596019745 older dist loss 0.03240719437599182\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.0907924622297287 class loss 0.0028160361107438803 dist loss 0.05444037914276123 older dist loss 0.033536046743392944\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.09673537313938141 class loss 0.002873005112633109 dist loss 0.05810502544045448 older dist loss 0.035757340490818024\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.09535074234008789 class loss 0.0019065345404669642 dist loss 0.057695552706718445 older dist loss 0.03574865683913231\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.09474924206733704 class loss 0.0032686323393136263 dist loss 0.056738026440143585 older dist loss 0.034742578864097595\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.0946129634976387 class loss 0.0037138964980840683 dist loss 0.056758031249046326 older dist loss 0.03414103761315346\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.09468942880630493 class loss 0.004245597869157791 dist loss 0.05613802745938301 older dist loss 0.03430580347776413\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.09076835215091705 class loss 0.0024224158842116594 dist loss 0.05442092567682266 older dist loss 0.03392501547932625\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.09370379149913788 class loss 0.0026734082493931055 dist loss 0.05656841769814491 older dist loss 0.034461960196495056\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.09416922926902771 class loss 0.002286606701090932 dist loss 0.05678475275635719 older dist loss 0.0350978747010231\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.09444640576839447 class loss 0.0019651225302368402 dist loss 0.057183168828487396 older dist loss 0.03529810905456543\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.09560869634151459 class loss 0.0032756924629211426 dist loss 0.057646721601486206 older dist loss 0.03468628600239754\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08928480744361877 class loss 0.0013146419078111649 dist loss 0.05488951876759529 older dist loss 0.033080652356147766\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.08908724784851074 class loss 0.0012445718748494983 dist loss 0.05449749156832695 older dist loss 0.033345188945531845\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.08788275718688965 class loss 0.0018856553360819817 dist loss 0.05350979045033455 older dist loss 0.03248731419444084\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.08753259479999542 class loss 0.0013551571173593402 dist loss 0.05346709489822388 older dist loss 0.03271034359931946\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.08751799166202545 class loss 0.0013525801477953792 dist loss 0.05341026186943054 older dist loss 0.03275515139102936\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.08463473618030548 class loss 0.0013459993060678244 dist loss 0.051806703209877014 older dist loss 0.03148203343153\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.0872400626540184 class loss 0.001024116761982441 dist loss 0.053465068340301514 older dist loss 0.03275087848305702\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.08728398382663727 class loss 0.0011728337267413735 dist loss 0.053295597434043884 older dist loss 0.03281555697321892\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.08784686774015427 class loss 0.0010135259944945574 dist loss 0.05386500433087349 older dist loss 0.03296833857893944\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.09103363007307053 class loss 0.001218109973706305 dist loss 0.05593109503388405 older dist loss 0.03388442471623421\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.08709779381752014 class loss 0.0015148271340876818 dist loss 0.052985139191150665 older dist loss 0.032597824931144714\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.08882089704275131 class loss 0.0014464262640103698 dist loss 0.05457203462719917 older dist loss 0.032802436500787735\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.08927837014198303 class loss 0.0011773327132686973 dist loss 0.05433923378586769 older dist loss 0.03376180678606033\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.08729061484336853 class loss 0.0008946675807237625 dist loss 0.05341820791363716 older dist loss 0.032977741211652756\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.08728175610303879 class loss 0.001033301930874586 dist loss 0.05356501415371895 older dist loss 0.032683439552783966\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08612336218357086 class loss 0.0009803446009755135 dist loss 0.05295190587639809 older dist loss 0.03219110891222954\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.09009118378162384 class loss 0.0014886058634147048 dist loss 0.05483883619308472 older dist loss 0.03376374393701553\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08605004847049713 class loss 0.0008203612524084747 dist loss 0.05303686857223511 older dist loss 0.03219281882047653\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08912405371665955 class loss 0.000916995748411864 dist loss 0.054688360542058945 older dist loss 0.03351869806647301\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08954910933971405 class loss 0.0008537154644727707 dist loss 0.05497545003890991 older dist loss 0.03371994569897652\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08593810349702835 class loss 0.0007353034452535212 dist loss 0.05278933793306351 older dist loss 0.032413460314273834\n",
            "Reducing each exemplar set to size: 25\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 49.80\n",
            "\n",
            "Test Accuracy (all groups seen so far): 45.41\n",
            "\n",
            "the model knows 80 classes:\n",
            " \n",
            "GROUP:  9\n",
            "NEW CLASS SAMPLES:  1400\n",
            "ALL DB:  5400\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.09866127371788025 class loss 0.011396070010960102 dist loss 0.05466632917523384 older dist loss 0.03259887546300888\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.09163562953472137 class loss 0.007110114675015211 dist loss 0.05303424969315529 older dist loss 0.03149126097559929\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.09333466738462448 class loss 0.00906580314040184 dist loss 0.0531800240278244 older dist loss 0.03108884207904339\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.089522585272789 class loss 0.006807100027799606 dist loss 0.052157435566186905 older dist loss 0.03055804781615734\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.09063516557216644 class loss 0.006517897825688124 dist loss 0.05292617529630661 older dist loss 0.03119109384715557\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.08858011662960052 class loss 0.004927593283355236 dist loss 0.052494172006845474 older dist loss 0.03115835227072239\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.09039773046970367 class loss 0.006143359467387199 dist loss 0.052922505885362625 older dist loss 0.0313318707048893\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.0889582484960556 class loss 0.0055865035392344 dist loss 0.05254894867539406 older dist loss 0.030822791159152985\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.08951223641633987 class loss 0.003935830667614937 dist loss 0.053797945380210876 older dist loss 0.03177846223115921\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.08979745209217072 class loss 0.005750616546720266 dist loss 0.05279753729701042 older dist loss 0.03124929778277874\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.09259964525699615 class loss 0.004174004774540663 dist loss 0.05565733462572098 older dist loss 0.03276830166578293\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.09218256175518036 class loss 0.0046131545677781105 dist loss 0.05501341447234154 older dist loss 0.032555997371673584\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.09289030730724335 class loss 0.00636205542832613 dist loss 0.05447918549180031 older dist loss 0.032049063593149185\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.08985038101673126 class loss 0.003958585672080517 dist loss 0.0538385808467865 older dist loss 0.03205321729183197\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.08929023146629333 class loss 0.0062635354697704315 dist loss 0.05221393331885338 older dist loss 0.030812760815024376\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.08962506055831909 class loss 0.0035502754617482424 dist loss 0.05431464686989784 older dist loss 0.03176013380289078\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.09347528219223022 class loss 0.004528896417468786 dist loss 0.05594498664140701 older dist loss 0.033001404255628586\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.08975420892238617 class loss 0.004198069218546152 dist loss 0.05381670594215393 older dist loss 0.03173943608999252\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.08992965519428253 class loss 0.004050040617585182 dist loss 0.053934987634420395 older dist loss 0.0319446325302124\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.09188665449619293 class loss 0.0032532685436308384 dist loss 0.05527539923787117 older dist loss 0.03335798904299736\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.0935097336769104 class loss 0.0036828075535595417 dist loss 0.056185849010944366 older dist loss 0.03364108130335808\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.08833946287631989 class loss 0.0028613798785954714 dist loss 0.05359738692641258 older dist loss 0.0318806916475296\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.09197874367237091 class loss 0.0034318361431360245 dist loss 0.05562230572104454 older dist loss 0.0329245999455452\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.08956094086170197 class loss 0.0026341229677200317 dist loss 0.054731715470552444 older dist loss 0.03219510614871979\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.0899171531200409 class loss 0.0029883752577006817 dist loss 0.054680902510881424 older dist loss 0.0322478748857975\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.09019376337528229 class loss 0.0025661399122327566 dist loss 0.05523402616381645 older dist loss 0.032393600791692734\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.08941854536533356 class loss 0.0029169442132115364 dist loss 0.05448886752128601 older dist loss 0.032012734562158585\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.09111131727695465 class loss 0.0032891735900193453 dist loss 0.05477563291788101 older dist loss 0.03304651007056236\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.09090021252632141 class loss 0.0030782162211835384 dist loss 0.054943837225437164 older dist loss 0.03287816047668457\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.08833536505699158 class loss 0.0027422141283750534 dist loss 0.05383254215121269 older dist loss 0.03176061436533928\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.09032606333494186 class loss 0.0031218104995787144 dist loss 0.05479799211025238 older dist loss 0.03240625932812691\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.08798761665821075 class loss 0.0020680732559412718 dist loss 0.05354990437626839 older dist loss 0.03236963972449303\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.09088113903999329 class loss 0.002953873248770833 dist loss 0.05504659563302994 older dist loss 0.032880671322345734\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.09003014862537384 class loss 0.0026422340888530016 dist loss 0.05489492416381836 older dist loss 0.032492995262145996\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.09201577305793762 class loss 0.0021262685768306255 dist loss 0.0567476823925972 older dist loss 0.03314182534813881\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.08939070254564285 class loss 0.001839329837821424 dist loss 0.054725419729948044 older dist loss 0.032825954258441925\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.09159152209758759 class loss 0.0021151206456124783 dist loss 0.05635060369968414 older dist loss 0.03312579542398453\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.09222465753555298 class loss 0.002539233770221472 dist loss 0.056355807930231094 older dist loss 0.033329617232084274\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.09042614698410034 class loss 0.0017710995161905885 dist loss 0.055524859577417374 older dist loss 0.03313019126653671\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.09216257929801941 class loss 0.0026718946173787117 dist loss 0.05627750605344772 older dist loss 0.03321317583322525\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.09131309390068054 class loss 0.00129220774397254 dist loss 0.0564630962908268 older dist loss 0.033557791262865067\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.0899697095155716 class loss 0.002423017518594861 dist loss 0.055132824927568436 older dist loss 0.032413870096206665\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.08874410390853882 class loss 0.001812505885027349 dist loss 0.054536789655685425 older dist loss 0.032394811511039734\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.08922448754310608 class loss 0.00219077430665493 dist loss 0.054721541702747345 older dist loss 0.032312169671058655\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.08768114447593689 class loss 0.0016463759820908308 dist loss 0.05438385158777237 older dist loss 0.03165091946721077\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.08794914186000824 class loss 0.0017755600856617093 dist loss 0.05414634197950363 older dist loss 0.032027240842580795\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.08686649799346924 class loss 0.0016673770733177662 dist loss 0.05362503603100777 older dist loss 0.03157408535480499\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.08944918215274811 class loss 0.0016707811737433076 dist loss 0.05527620017528534 older dist loss 0.0325021967291832\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.08787955343723297 class loss 0.002168804407119751 dist loss 0.05347772687673569 older dist loss 0.03223302215337753\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08466493338346481 class loss 0.0007416895241476595 dist loss 0.05255429074168205 older dist loss 0.03136895224452019\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.08654625713825226 class loss 0.0008232510299421847 dist loss 0.053852178156375885 older dist loss 0.031870827078819275\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.0869583860039711 class loss 0.0010161249665543437 dist loss 0.054096926003694534 older dist loss 0.0318453349173069\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.0854606106877327 class loss 0.001164384768344462 dist loss 0.05302882194519043 older dist loss 0.031267404556274414\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.0874672457575798 class loss 0.0007288407650776207 dist loss 0.05413726344704628 older dist loss 0.032601140439510345\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.08418552577495575 class loss 0.0007837142911739647 dist loss 0.052476316690444946 older dist loss 0.030925491824746132\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.08337194472551346 class loss 0.0009185935487039387 dist loss 0.052027542144060135 older dist loss 0.03042580746114254\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.0875910073518753 class loss 0.0005590977962128818 dist loss 0.05465902015566826 older dist loss 0.032372891902923584\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.08228641003370285 class loss 0.0006701438687741756 dist loss 0.051434628665447235 older dist loss 0.030181637033820152\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.08441776037216187 class loss 0.00055603904183954 dist loss 0.05268463119864464 older dist loss 0.031177084892988205\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.08622822165489197 class loss 0.0008501561242155731 dist loss 0.05379865691065788 older dist loss 0.031579405069351196\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.08737225085496902 class loss 0.0007354181143455207 dist loss 0.054598771035671234 older dist loss 0.03203806281089783\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.08321188390254974 class loss 0.0006559128523804247 dist loss 0.05191238597035408 older dist loss 0.030643589794635773\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.08585941046476364 class loss 0.0005717426538467407 dist loss 0.05371176451444626 older dist loss 0.03157590329647064\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.08292484283447266 class loss 0.000686316576320678 dist loss 0.05133052170276642 older dist loss 0.03090800903737545\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08380241692066193 class loss 0.0004554584447760135 dist loss 0.05242278799414635 older dist loss 0.030924171209335327\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08598016947507858 class loss 0.0006783061544410884 dist loss 0.05388185754418373 older dist loss 0.03142000734806061\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08628043532371521 class loss 0.0006515916902571917 dist loss 0.053576670587062836 older dist loss 0.0320521779358387\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08608554303646088 class loss 0.0006317014922387898 dist loss 0.053735584020614624 older dist loss 0.031718261539936066\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08546265959739685 class loss 0.000511143181938678 dist loss 0.0539313405752182 older dist loss 0.03102017194032669\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08642850816249847 class loss 0.0009037276031449437 dist loss 0.053691450506448746 older dist loss 0.03183332830667496\n",
            "Reducing each exemplar set to size: 23\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 45.12\n",
            "\n",
            "Test Accuracy (all groups seen so far): 42.80\n",
            "\n",
            "the model knows 90 classes:\n",
            " \n",
            "GROUP:  10\n",
            "NEW CLASS SAMPLES:  950\n",
            "ALL DB:  5090\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.09178990870714188 class loss 0.009593906812369823 dist loss 0.05188341438770294 older dist loss 0.030312586575746536\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.0881807953119278 class loss 0.00803361739963293 dist loss 0.05045919865369797 older dist loss 0.029687974601984024\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.08676502108573914 class loss 0.00559247424826026 dist loss 0.05080610513687134 older dist loss 0.03036644496023655\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.08628220856189728 class loss 0.004395324736833572 dist loss 0.05182329937815666 older dist loss 0.030063584446907043\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.08799363672733307 class loss 0.005246776156127453 dist loss 0.0520932674407959 older dist loss 0.030653594061732292\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.08493366837501526 class loss 0.004034976474940777 dist loss 0.0508592315018177 older dist loss 0.030039459466934204\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.08756022155284882 class loss 0.0054304469376802444 dist loss 0.051795121282339096 older dist loss 0.030334655195474625\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.08385946601629257 class loss 0.0035958176013082266 dist loss 0.0504864864051342 older dist loss 0.02977716363966465\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.08622041344642639 class loss 0.0030628282111138105 dist loss 0.0527648851275444 older dist loss 0.03039269521832466\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.09197966009378433 class loss 0.005352518986910582 dist loss 0.05451598018407822 older dist loss 0.03211116045713425\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.08911457657814026 class loss 0.005257983226329088 dist loss 0.05258529633283615 older dist loss 0.031271301209926605\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.08696258068084717 class loss 0.003249458270147443 dist loss 0.05294995382428169 older dist loss 0.03076317347586155\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.08411389589309692 class loss 0.0019871636759489775 dist loss 0.051981136202812195 older dist loss 0.03014560043811798\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.08803143352270126 class loss 0.0031123037915676832 dist loss 0.05334966629743576 older dist loss 0.0315694622695446\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.08414194732904434 class loss 0.0038192083593457937 dist loss 0.05070655792951584 older dist loss 0.029616182669997215\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.08775118738412857 class loss 0.003237501485273242 dist loss 0.053301889449357986 older dist loss 0.031211797147989273\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.08567461371421814 class loss 0.0036097813863307238 dist loss 0.05196264013648033 older dist loss 0.030102193355560303\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.08449871838092804 class loss 0.0026752380654215813 dist loss 0.05169105529785156 older dist loss 0.03013242781162262\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.08436563611030579 class loss 0.0029133406933397055 dist loss 0.05143655836582184 older dist loss 0.0300157368183136\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.08645276725292206 class loss 0.0028945470694452524 dist loss 0.05284350365400314 older dist loss 0.030714714899659157\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.08499855548143387 class loss 0.002992711029946804 dist loss 0.05168967694044113 older dist loss 0.030316168442368507\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.08636370301246643 class loss 0.0032093848567456007 dist loss 0.05254913493990898 older dist loss 0.03060518205165863\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.0845610722899437 class loss 0.003071465063840151 dist loss 0.05145164206624031 older dist loss 0.03003796376287937\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.08871947228908539 class loss 0.0026523242704570293 dist loss 0.05443863943219185 older dist loss 0.031628504395484924\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.08721700310707092 class loss 0.0030492721125483513 dist loss 0.053032055497169495 older dist loss 0.0311356782913208\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.08415427058935165 class loss 0.002267672447487712 dist loss 0.05181242525577545 older dist loss 0.03007417358458042\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.08887823671102524 class loss 0.0026091099716722965 dist loss 0.05426092445850372 older dist loss 0.032008200883865356\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.08493185043334961 class loss 0.0016676037339493632 dist loss 0.052370235323905945 older dist loss 0.030894016847014427\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.08482810854911804 class loss 0.0013792052632197738 dist loss 0.05291301757097244 older dist loss 0.03053588978946209\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.08717405050992966 class loss 0.0019609322771430016 dist loss 0.053696420043706894 older dist loss 0.03151669725775719\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.08878129720687866 class loss 0.002642767271026969 dist loss 0.05454108119010925 older dist loss 0.03159744665026665\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.08534467220306396 class loss 0.001684715272858739 dist loss 0.05296437442302704 older dist loss 0.030695587396621704\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.08674968779087067 class loss 0.0021320534870028496 dist loss 0.05355249345302582 older dist loss 0.031065136194229126\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.08878394961357117 class loss 0.002635915530845523 dist loss 0.054490286856889725 older dist loss 0.03165775164961815\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.08521036803722382 class loss 0.0016156906494870782 dist loss 0.053170979022979736 older dist loss 0.03042370080947876\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.0832643061876297 class loss 0.0013514846796169877 dist loss 0.05152352526783943 older dist loss 0.030389294028282166\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.0845465362071991 class loss 0.0012897384585812688 dist loss 0.05289232358336449 older dist loss 0.03036447800695896\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.08639305830001831 class loss 0.001595514127984643 dist loss 0.05352247878909111 older dist loss 0.03127506375312805\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.08703762292861938 class loss 0.0018225848907604814 dist loss 0.054044436663389206 older dist loss 0.031170597299933434\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.08498945087194443 class loss 0.0012294088955968618 dist loss 0.05320718511939049 older dist loss 0.030552858486771584\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.08740593492984772 class loss 0.0014501983532682061 dist loss 0.05400875210762024 older dist loss 0.03194698318839073\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.08580327033996582 class loss 0.0009140825713984668 dist loss 0.05341535806655884 older dist loss 0.031473834067583084\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.08687546104192734 class loss 0.002108813263475895 dist loss 0.05392953380942345 older dist loss 0.03083711303770542\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.08472378551959991 class loss 0.0011769894044846296 dist loss 0.05263004079461098 older dist loss 0.03091675043106079\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.08542023599147797 class loss 0.0012418109690770507 dist loss 0.053167104721069336 older dist loss 0.031011316925287247\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.08730494976043701 class loss 0.0010009584948420525 dist loss 0.05466289445757866 older dist loss 0.031641095876693726\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.08715245872735977 class loss 0.001454311073757708 dist loss 0.054006148129701614 older dist loss 0.03169199824333191\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.08452635258436203 class loss 0.0016012036940082908 dist loss 0.05226508155465126 older dist loss 0.03066006861627102\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.08502333611249924 class loss 0.0016446858644485474 dist loss 0.052488259971141815 older dist loss 0.030890390276908875\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0834566280245781 class loss 0.000835668295621872 dist loss 0.05219647288322449 older dist loss 0.030424484983086586\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.08163142949342728 class loss 0.0008131425129249692 dist loss 0.051114555448293686 older dist loss 0.02970373071730137\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.08134844899177551 class loss 0.0008904330315999687 dist loss 0.05088629946112633 older dist loss 0.029571715742349625\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.08460882306098938 class loss 0.0006956832949072123 dist loss 0.053046513348817825 older dist loss 0.030866626650094986\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.08242340385913849 class loss 0.0008755584130994976 dist loss 0.05135474354028702 older dist loss 0.030193103477358818\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.08053100109100342 class loss 0.0005829395377077162 dist loss 0.05043990537524223 older dist loss 0.029508152976632118\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.0808715671300888 class loss 0.0010168756125494838 dist loss 0.050459761172533035 older dist loss 0.029394928365945816\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.08325263112783432 class loss 0.0007747075287625194 dist loss 0.051960572600364685 older dist loss 0.030517352744936943\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.08119678497314453 class loss 0.0006642612279392779 dist loss 0.05072021484375 older dist loss 0.02981230616569519\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.08245540410280228 class loss 0.0007423243951052427 dist loss 0.051533106714487076 older dist loss 0.03017997182905674\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.08176440745592117 class loss 0.0006132537964731455 dist loss 0.05141586437821388 older dist loss 0.029735291376709938\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.08233742415904999 class loss 0.0005225986824370921 dist loss 0.05186404287815094 older dist loss 0.029950786381959915\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.07978467643260956 class loss 0.00042683756328187883 dist loss 0.05019227787852287 older dist loss 0.029165560379624367\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.08353597670793533 class loss 0.0010588787263259292 dist loss 0.05197806656360626 older dist loss 0.030499029904603958\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.08358272910118103 class loss 0.0007756974664516747 dist loss 0.052329689264297485 older dist loss 0.030477343127131462\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08268959820270538 class loss 0.00045012097689323127 dist loss 0.05184175446629524 older dist loss 0.030397718772292137\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08265718817710876 class loss 0.0005084557342343032 dist loss 0.05217123404145241 older dist loss 0.029977494850754738\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08274875581264496 class loss 0.00045668703387491405 dist loss 0.05195670947432518 older dist loss 0.030335355550050735\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08153196424245834 class loss 0.0008439411758445203 dist loss 0.05110640451312065 older dist loss 0.029581621289253235\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08198801428079605 class loss 0.0006434828392229974 dist loss 0.05141964554786682 older dist loss 0.029924888163805008\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.08261483907699585 class loss 0.0004876443126704544 dist loss 0.051814716309309006 older dist loss 0.03031248226761818\n",
            "Reducing each exemplar set to size: 20\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 36.12\n",
            "\n",
            "Test Accuracy (all groups seen so far): 40.03\n",
            "\n",
            "the model knows 100 classes:\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffaf648e-84cf-4971-bd59-9b69a6f58242"
      },
      "source": [
        "if herding:\n",
        "  method = 'iCaRL_{}_herding'.format(classifier)\n",
        "else:\n",
        "  method = 'iCaRL_{}_random'.format(classifier)\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics iCaRL for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGxWw4fyzyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}