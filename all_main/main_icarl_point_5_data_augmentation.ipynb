{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ris.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/classifiers/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "outputId": "6771d15a-940d-4077-faad-fb4553c80434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "outputId": "2188f20a-f543-4dd6-e43a-de54ece5b117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "outputId": "27e5f8e4-ea8f-4d0f-b857-f9aadbedb6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "outputId": "a821b2f3-03b9-4a9b-9f12-b919f26a2959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Cifar100/': No such file or directory\n",
            "rm: cannot remove 'DATA': No such file or directory\n",
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 2161 (delta 19), reused 34 (delta 13), pack-reused 2119\u001b[K\n",
            "Receiving objects: 100% (2161/2161), 18.01 MiB | 16.93 MiB/s, done.\n",
            "Resolving deltas: 100% (1285/1285), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "outputId": "55785dda-4ff5-4360-f83f-fcabe59eea5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}/cifar-100-python'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-13 16:17:11--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  30.9MB/s    in 5.8s    \n",
            "\n",
            "2020-06-13 16:17:17 (27.7 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "outputId": "cce56dfe-271b-4e76-b7d4-015cdf8669f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = 66\n",
        "\n",
        "# icarl params\n",
        "herding = True # if false random exemplars, if true nme (herding)\n",
        "classifier = \"NCM\" # NCM, FCC, KNN, SVC, COS"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 66, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "outputId": "225f4459-76a0-4d77-e4a8-767c64be6395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "     \n",
        "        # add other classifiers\n",
        "        if classifier == 'NCM':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.classify(images)\n",
        "        elif classifier == 'FCC':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.FCC_classify(images)\n",
        "        elif classifier == 'KNN' or classifier == 'SVC':\n",
        "          preds = net.KNN_SVC_classify(images)\n",
        "          preds = preds.to(DEVICE)\n",
        "        elif classifier == 'COS':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.COS_classify(images)\n",
        "\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "  accuracy = correct/len(dataset)\n",
        "  if method == 'test':\n",
        "    all_preds_cm.extend(preds.tolist())\n",
        "    all_labels_cm.extend(labels.data.tolist())\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "          train_set_big = train_subset\n",
        "        else:\n",
        "          test_set = utils.joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, train_dataset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(math.ceil(K/icarl.n_classes))\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          icarl.construct_exemplar_set(class_train_subset,m,y)\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier\n",
        "        icarl.computeMeans()\n",
        "\n",
        "        # common training on exemplars for KNN and SVC classifier\n",
        "        if classifier == 'KNN':\n",
        "          K_nn = 5\n",
        "          icarl.modelTrain(classifier, K_nn)\n",
        "        elif classifier == 'SVC':\n",
        "          icarl.modelTrain(classifier)\n",
        "\n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAn-c_mMrMB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This class implements the main model of iCaRL \n",
        "    and all the methods regarding the exemplars\n",
        "    from delivery: iCaRL is made up of 2 components\n",
        "    - feature extractor (a convolutional NN) => resnet32 optimized on cifar100\n",
        "    - classifier => a FC layer OR a non-parametric classifier (NME)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "import gc #extensive use in order to manage memory issues\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToPILImage \n",
        "\n",
        "from Cifar100 import utils\n",
        "from Cifar100.resnet import resnet32\n",
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# new classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def auto_loss_rebalancing(n_known, n_classes, loss_type):\n",
        "  alpha = n_known/n_classes \n",
        "\n",
        "  if loss_type == 'class':\n",
        "    return 1-alpha\n",
        "  return alpha\n",
        "\n",
        "def get_rebalancing(rebalancing=None):\n",
        "  if rebalancing is None:\n",
        "    return lambda n_known, n_classes, loss_type: 1\n",
        "  if rebalancing in ['auto', 'AUTO']:\n",
        "    return auto_loss_rebalancing\n",
        "  if callable(rebalancing):\n",
        "    return rebalancing\n",
        "\n",
        "# feature_size: 2048, why?\n",
        "# n_classes: 10 => 100\n",
        "class ICaRL(nn.Module):\n",
        "  def __init__(self, feature_size, n_classes,\\\n",
        "      BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE, MILESTONES, MOMENTUM, K,\\\n",
        "      herding, reverse_index = None, class_loss_criterion='bce', dist_loss_criterion='bce', loss_rebalancing='auto', lambda0=1):\n",
        "    super(ICaRL, self).__init__()\n",
        "    self.net = resnet32()\n",
        "    self.net.fc = nn.Linear(self.net.fc.in_features, n_classes)\n",
        "\n",
        "    self.feature_extractor = resnet32()\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    self.n_classes = n_classes\n",
        "    self.n_known = 0\n",
        "\n",
        "    # Hyper-parameters from iCaRL\n",
        "    self.BATCH_SIZE = BATCH_SIZE\n",
        "    self.WEIGHT_DECAY  = WEIGHT_DECAY\n",
        "    self.LR = LR\n",
        "    self.GAMMA = GAMMA # this allow LR to become 1/5 LR after MILESTONES epochs\n",
        "    self.NUM_EPOCHS = NUM_EPOCHS\n",
        "    self.DEVICE = DEVICE\n",
        "    self.MILESTONES = MILESTONES # when the LR decreases, according to icarl\n",
        "    self.MOMENTUM = MOMENTUM\n",
        "    self.K = K\n",
        "    \n",
        "    self.reverse_index=reverse_index\n",
        "\n",
        "    self.optimizer, self.scheduler = utils.getOptimizerScheduler(self.LR, self.MOMENTUM, self.WEIGHT_DECAY, self.MILESTONES, self.GAMMA, self.parameters())\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    # List containing exemplar_sets\n",
        "    # Each exemplar_set is a np.array of N images\n",
        "    self.exemplar_sets = []\n",
        "    self.exemplar_sets_indices = []\n",
        "\n",
        "    \n",
        "    # for the classification/distillation loss we have two alternatives\n",
        "    # 1- BCE loss with Logits (reduction could be mean or sum)\n",
        "    # 2- BCE loss + sigmoid\n",
        "    # actually we use just one loss as explained on the forum\n",
        "\n",
        "    self.class_loss, self.dist_loss = self.build_loss(class_loss_criterion, dist_loss_criterion, loss_rebalancing, lambda0=lambda0)\n",
        "\n",
        "    # Means of exemplars (cntroids)\n",
        "    self.compute_means = True\n",
        "    self.exemplar_means = []\n",
        "    self.exemplar_mean_nn = [] # means not normalized\n",
        "\n",
        "    self.herding = herding # random choice of exemplars or icarl exemplars strategy?\n",
        "\n",
        "    # this is used as explained in the forum to compute the exemplar mean in a more accurate way\n",
        "    # populated during construct exemplar set and used in the classify step\n",
        "    self.data_from_classes = []\n",
        "    self.means_from_classes = []\n",
        "\n",
        "    # Knn, svc classification\n",
        "    self.model = None\n",
        "\n",
        "    # QUA! #\n",
        "    ###############################################################################################################################################################################################\n",
        "    self.oldNet= None\n",
        "    self.moreOldNet= None\n",
        "\n",
        "    ###############################################################################################################################################################################################\n",
        "  \n",
        "  # increment the number of classes considered by the net\n",
        "  # incremental learning approach, 0,10..100\n",
        "  def increment_classes(self, n):\n",
        "        gc.collect()\n",
        "\n",
        "        in_features = self.net.fc.in_features\n",
        "        out_features = self.net.fc.out_features\n",
        "        weights = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features + n) #add 10 classes to the fc last layer\n",
        "        self.net.fc.weight.data[:out_features] = weights\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "        self.n_classes += n #icrement #classes considered\n",
        "\n",
        "  # computes the mean of each exemplar set\n",
        "  def computeMeans(self):\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # new mean mgmt\n",
        "    tensors_mean = []\n",
        "    exemplar_mean_nn=[]\n",
        "    with torch.no_grad():\n",
        "      for tensor_set in self.data_from_classes:\n",
        "        features = []\n",
        "        for tensor, _ in tensor_set:\n",
        "          \n",
        "          tensor = tensor.to(self.DEVICE)\n",
        "          feature = feature_extractor(tensor)\n",
        "\n",
        "          feature.data = feature.data / feature.data.norm() # Normalize\n",
        "          features.append(feature)\n",
        "\n",
        "          # cleaning \n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        features = torch.stack(features) #(num_exemplars,num_features)\n",
        "        mean_tensor = features.mean(0) \n",
        "        exemplar_mean_nn.append(mean_tensor.to('cpu'))\n",
        "        mean_tensor.data = mean_tensor.data / mean_tensor.data.norm() # Re-normalize\n",
        "        mean_tensor = mean_tensor.to('cpu')\n",
        "        tensors_mean.append(mean_tensor)\n",
        "\n",
        "    self.exemplar_means = tensors_mean  # nb the mean is computed over all the imgs\n",
        "    self.exemplar_mean_nn= exemplar_mean_nn # exemplars means not normalized\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # train procedure common for KNN and SVC classifier (save a lot of training time)\n",
        "  def modelTrain(self, method, K_nn = None):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # -- train a SVC classifier\n",
        "    X_train, y_train = [], []\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets:\n",
        "          for exemplar, label in  exemplar_set:\n",
        "            exemplar = exemplar.to(self.DEVICE)\n",
        "            feature = feature_extractor(exemplar)\n",
        "            feature = feature.squeeze()\n",
        "            feature.data = feature.data / feature.data.norm() # Normalize\n",
        "            X_train.append(feature.cpu().detach().numpy())\n",
        "            y_train.append(label)\n",
        "    \n",
        "    if method == 'KNN':\n",
        "      model = KNeighborsClassifier(n_neighbors = K_nn)\n",
        "    elif method == 'SVC':\n",
        "      model = LinearSVC()\n",
        "    self.model = model.fit(X_train, y_train)\n",
        "\n",
        "  # common classify function\n",
        "  def KNN_SVC_classify(self, images):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # --- prediction\n",
        "    X_pred = []\n",
        "    images = images.to(self.DEVICE)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    features = feature_extractor(images)\n",
        "    for feature in features:\n",
        "      feature = feature.squeeze()\n",
        "      feature.data = feature.data / feature.data.norm() # Normalize\n",
        "      X_pred.append(feature.cpu().detach().numpy())\n",
        "    \n",
        "    preds = self.model.predict(X_pred)\n",
        "    # --- end prediction\n",
        "    return torch.tensor(preds)\n",
        "    \n",
        "  # classify base on cosine similarity\n",
        "  def COS_classify(self, batch_imgs):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_imgs_size = batch_imgs.size(0)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    means_exemplars = torch.cat(self.exemplar_mean_nn, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "    means_exemplars = means_exemplars.transpose(1, 2) # means no normalized\n",
        "\n",
        "    feature = feature_extractor(batch_imgs) # features no normalized\n",
        "    \n",
        "    feature=feature.to('cpu')\n",
        "    means_exemplars = means_exemplars.to('cpu')\n",
        "\n",
        "    preds=[]\n",
        "    for a in feature:\n",
        "      a=a.detach().numpy()\n",
        "      aa=np.linalg.norm(a)\n",
        "      res=[]\n",
        "      for b in means_exemplars:\n",
        "        b=b.detach().numpy()\n",
        "        bb=np.linalg.norm(b)\n",
        "        dot = np.dot(a, b)\n",
        "        cos = dot / (aa * bb)\n",
        "        res.append(cos)\n",
        "      preds.append(np.argmax(np.array(res)))\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return torch.FloatTensor(preds).to(self.DEVICE)\n",
        "\n",
        "  # classification via fc layer (similar to lwf approach)\n",
        "  def FCC_classify(self, images):\n",
        "    _, preds = torch.max(torch.softmax(self.net(images), dim=1), dim=1, keepdim=False)\n",
        "    return preds\n",
        "  # NME classification from iCaRL paper\n",
        "  def classify(self, batch_imgs):\n",
        "      \"\"\"Classify images by nearest-mean-of-exemplars\n",
        "      Args:\n",
        "          batch_imgs: input image batch\n",
        "      Returns:\n",
        "          preds: Tensor of size (batch_size,)\n",
        "      \"\"\"\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      batch_imgs_size = batch_imgs.size(0)\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # update exemplar_means with the mean\n",
        "      # of all the train data for a given class\n",
        "\n",
        "      means_exemplars = torch.cat(self.exemplar_means, dim=0)\n",
        "      means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "      means_exemplars = means_exemplars.transpose(1, 2) \n",
        "\n",
        "      feature = feature_extractor(batch_imgs) \n",
        "      aus_normalized_features = []\n",
        "      for el in feature: # Normalize\n",
        "          el.data = el.data / el.data.norm()\n",
        "          aus_normalized_features.append(el)\n",
        "\n",
        "      feature = torch.stack(aus_normalized_features,dim=0)\n",
        "\n",
        "      feature = feature.unsqueeze(2) \n",
        "      feature = feature.expand_as(means_exemplars) \n",
        "\n",
        "      means_exemplars = means_exemplars.to(self.DEVICE)\n",
        "\n",
        "      # Nearest prototype\n",
        "      preds = torch.argmin((feature - means_exemplars).pow(2).sum(1),dim=1)\n",
        "\n",
        "      # cleaning\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      return preds\n",
        "\n",
        "  # implementation of alg. 4 of icarl paper\n",
        "  # iCaRL ConstructExemplarSet\n",
        "  def construct_exemplar_set(self, tensors, m, label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "          tensors: train_subset containing a single label\n",
        "          m: number of exemplars allowed/exemplar set (class)\n",
        "          label: considered class\n",
        "    \"\"\"\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    exemplar_set_indices = set()\n",
        "    exemplar_list_indices = []\n",
        "    exemplar_set = []\n",
        "    if self.herding:\n",
        "\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # Compute and cache features for each example\n",
        "      features = []\n",
        "\n",
        "      loader = DataLoader(tensors,batch_size=self.BATCH_SIZE,shuffle=True,drop_last=False,num_workers = 4)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for _, images, labels in loader:\n",
        "          images = images.to(self.DEVICE)\n",
        "          labels = labels.to(self.DEVICE)\n",
        "          feature = feature_extractor(images) \n",
        "\n",
        "          feature = feature / np.linalg.norm(feature.cpu()) # Normalize\n",
        "          \n",
        "          features.append(feature)\n",
        "\n",
        "      features_s = torch.cat(features)\n",
        "      \n",
        "      class_mean = features_s.mean(0)\n",
        "      class_mean = class_mean / np.linalg.norm(class_mean.cpu()) # Normalize\n",
        "      class_mean = torch.stack([class_mean]*features_s.size()[0])\n",
        "\n",
        "      summon = torch.zeros(1,features_s.size()[1]).to(self.DEVICE) #(1,num_features)\n",
        "      for k in range(1, (m + 1)):\n",
        "          S = torch.cat([summon]*features_s.size()[0]) # second addend, features in the exemplar set\n",
        "          results = pd.DataFrame((class_mean-(1/k)*(features_s + S)).pow(2).sum(1).cpu(), columns=['result']).sort_values('result')\n",
        "          results['index'] = results.index\n",
        "          results = results.to_numpy()\n",
        "\n",
        "          # select argmin not included in exemplar_set_indices\n",
        "          for i in range(results.shape[0]):\n",
        "            index = results[i, 1]\n",
        "            exemplar_k_index = tensors[index][0]\n",
        "            if exemplar_k_index not in exemplar_set_indices:\n",
        "              exemplar_k = tensors[index][1].unsqueeze(dim = 0) # take the image from the tuple (index, img, label)\n",
        "              exemplar_set.append((exemplar_k, label))\n",
        "              exemplar_k_index = tensors[index][0] # index of the img on the real dataset\n",
        "              \n",
        "              exemplar_list_indices.append(exemplar_k_index)\n",
        "              exemplar_set_indices.add(exemplar_k_index)\n",
        "              break\n",
        "\n",
        "          # features of the exemplar k\n",
        "          phi = feature_extractor(exemplar_k.to(self.DEVICE)) #feature_extractor(exemplar_k.to(self.DEVICE))\n",
        "          summon += phi # update sum of features\n",
        "    else:\n",
        "      tensors_size = len(tensors)\n",
        "      unique_random_indexes = random.sample(range(0, tensors_size), m) # random sample without replacement k exemplars\n",
        "      i = 0\n",
        "      for k in range(1, (m + 1)):\n",
        "        index = unique_random_indexes[i]\n",
        "        exemplar_k = tensors[index][1].unsqueeze(dim = 0)\n",
        "        exemplar_k_index = tensors[index][0]\n",
        "        exemplar_set.append((exemplar_k, label))\n",
        "        exemplar_set_indices.add(exemplar_k_index)\n",
        "        i = i + 1\n",
        "\n",
        "    # --- new ---\n",
        "    tensor_set = []\n",
        "    for i in range(0, len(tensors)):\n",
        "      t = tensors[i][1].unsqueeze(dim = 0)\n",
        "      tensor_set.append((t, label))\n",
        "    \n",
        "    self.exemplar_sets.append(exemplar_set) #update exemplar sets with the updated exemplars images\n",
        "    self.exemplar_sets_indices.append(exemplar_list_indices)\n",
        "\n",
        "    # this is used to compute more accurately the means of the exemplar (see also computeMeans and classify)\n",
        "    self.data_from_classes.append(tensor_set)\n",
        "\n",
        "    # cleaning\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # build a exemplar dataset as a subset of the train dataset\n",
        "  def build_exemplars_dataset(self, train_dataset): #complete train dataset\n",
        "    all_exemplars_indices = []\n",
        "    for exemplar_set_indices in self.exemplar_sets_indices:\n",
        "        all_exemplars_indices.extend(exemplar_set_indices)\n",
        "\n",
        "    exemplars_dataset = Subset(train_dataset, all_exemplars_indices)\n",
        "    return exemplars_dataset\n",
        "\n",
        "  def update_representation(self, dataset, train_dataset_big, new_classes):\n",
        "    # 1 - retrieve the classes from the dataset (which is the current train_subset)\n",
        "    # 2 - retrieve the new classes\n",
        "    # 1,2 are done in the main_icarl\n",
        "    #gc.collect()\n",
        "\n",
        "    # 3 - increment classes\n",
        "    #          (add output nodes)\n",
        "    #          (update n_classes)\n",
        "    # 5        store network outputs with pre-update parameters\n",
        "    self.increment_classes(len(new_classes))\n",
        "\n",
        "    # 4 - combine current train_subset (dataset) with exemplars\n",
        "    #     to form a new augmented train dataset\n",
        "    # join the datasets\n",
        "    exemplars_dataset = self.build_exemplars_dataset(train_dataset_big)\n",
        "    \n",
        "    ######################################################################################\n",
        "  \n",
        "    if self.n_known==0:\n",
        "      cut=0\n",
        "    else:\n",
        "      cut=self.n_known/10\n",
        "    new_dataset_size = 5000 - int(math.ceil(450*cut))\n",
        "    unique_random_indexes = random.sample(range(0, len(dataset)), new_dataset_size) # random sample without replacement\n",
        "    new_dataset=torch.utils.data.Subset(dataset, unique_random_indexes)\n",
        "\n",
        "    # trying to save the images in a list\n",
        "    new_classes_dataset=[]\n",
        "    loader_imgs = DataLoader(new_dataset, batch_size=self.BATCH_SIZE, shuffle = False, num_workers=4)\n",
        "    for __, images, labels in loader_imgs:\n",
        "      for img, label in zip(images, labels):\n",
        "        new_classes_dataset.append((0, img, label))\n",
        "\n",
        "    #print(\"NEW CLASS SAMPLES: \",len(new_dataset))\n",
        "    ######################################################################################\n",
        "    #\n",
        "    if len(exemplars_dataset) > 0:\n",
        "      #join_dataset = ConcatDataset(new_dataset, exemplars_dataset)\n",
        "      #augmented_dataset = ConcatDataset(join_dataset, exemplars_dataset)\n",
        "      \n",
        "      load_exemplars = DataLoader(exemplars_dataset, batch_size=self.BATCH_SIZE, shuffle = False, num_workers=4)\n",
        "      new_image = []\n",
        "      old_image = []\n",
        "      for _, images, labels in load_exemplars:\n",
        "        for img,lab in zip(images,labels):\n",
        "          tran = transforms.Compose([transforms.ToPILImage(),transforms.RandomVerticalFlip(p=0.7),transforms.RandomGrayscale(p=0.7),transforms.ToTensor()])\n",
        "          new_img = tran(img)\n",
        "          new_image.append((0, new_img, lab))\n",
        "          old_image.append((0, img, lab))\n",
        "\n",
        "      #new_classes_dataset.extend(new_image)\n",
        "      #new_classes_dataset.extend(old_image)\n",
        "\n",
        "      augmented_dataset = new_classes_dataset+new_image+old_image\n",
        "\n",
        "      \n",
        "      #augmented_dataset = ConcatDataset(dataset, exemplars_dataset)\n",
        "      #augmented_dataset = utils.joinSubsets(train_dataset_big, [dataset, exemplars_dataset])\n",
        "    else: \n",
        "      augmented_dataset = new_classes_dataset # first iteration\n",
        "\n",
        "    print(\"ALL DB: \",len(augmented_dataset))\n",
        "    # 6 - run network training, with loss function\n",
        "\n",
        "    net = self.net\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=self.LR, weight_decay=self.WEIGHT_DECAY, momentum=self.MOMENTUM)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.MILESTONES, gamma=self.GAMMA, last_epoch=-1)\n",
        "\n",
        "    criterion = utils.getLossCriterion()\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    net = net.to(self.DEVICE)\n",
        "\n",
        "    # define the loader for the augmented_dataset\n",
        "    l = DataLoader(augmented_dataset, batch_size=self.BATCH_SIZE,shuffle=True, num_workers=4, drop_last = True)\n",
        "\n",
        "    \n",
        "    if len(self.exemplar_sets) > 0:\n",
        "      # QUA! #\n",
        "      #########################################################################################\n",
        "\n",
        "      #   print(self.oldNetTeachers[1].size())\n",
        "      if self.oldNet!=None:\n",
        "        self.moreOldNet=self.oldNet\n",
        "      self.oldNet= copy.deepcopy(net)\n",
        "\n",
        "      #########################################################################################\n",
        "    for epoch in range(self.NUM_EPOCHS):\n",
        "        print(\"NUM_EPOCHS: \",epoch,\"/\", self.NUM_EPOCHS,\" LR: \",scheduler.get_lr())\n",
        "        for indices, images, labels in l:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(self.DEVICE)\n",
        "            labels = labels.to(self.DEVICE)\n",
        "            net.train()\n",
        "            # PyTorch, by default, accumulates gradients after each backward pass\n",
        "            # We need to manually set the gradients to zero before starting a new iteration\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            \n",
        " \n",
        "            # QUA! #\n",
        "            #############################################################################################################\n",
        "            # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "            # Loss = only classification on new classes\n",
        "            loss = self.class_loss(outputs, labels, col_start=self.n_known)\n",
        "            class_loss = loss.item() # Used for logging for debugging purposes\n",
        "            \n",
        "            # Distilation loss for old classes, class loss on new classes\n",
        "            dist_loss = None\n",
        "            older_dist_loss=None\n",
        "            if len(self.exemplar_sets) > 0:\n",
        "              old_net=self.oldNet\n",
        "              out_old = torch.sigmoid(old_net(images))\n",
        "              dist_loss = self.dist_loss(outputs, out_old, col_end=self.n_known)\n",
        "\n",
        "              if self.moreOldNet!=None:\n",
        "                older_net=self.moreOldNet # old old net\n",
        "                older_out = torch.sigmoid(older_net(images))\n",
        "                older_dist_loss = self.double_dist_loss(outputs[0:128,0:-20], older_out[0:128,0:-10], col_end=self.n_known)\n",
        "                loss += older_dist_loss\n",
        "\n",
        "              loss += dist_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ##############################################################################################################\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LOSS: \", loss.item(), 'class loss', class_loss, 'dist loss', dist_loss.item() if dist_loss is not None else dist_loss, 'older dist loss',older_dist_loss.item() if older_dist_loss is not None else older_dist_loss)\n",
        "\n",
        "    self.net = copy.deepcopy(net)\n",
        "    self.feature_extractor = copy.deepcopy(net)\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    #cleaning\n",
        "    del net\n",
        "    torch.cuda.empty_cache()\n",
        " # QUA! #\n",
        " ###################################################################################################################\n",
        "  def double_dist_loss(self,outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    rebalancing=None\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    dist_loss_func = self.bce_dist_loss\n",
        "    alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "    return 0.5*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        " ##################################################################################################################\n",
        "\n",
        "  def build_loss(self, class_loss_criterion, dist_loss_criterion, rebalancing=None, lambda0=1):\n",
        "    class_loss_func = None\n",
        "    dist_loss_func = None\n",
        "\n",
        "    if class_loss_criterion in ['l2', 'L2']:\n",
        "      class_loss_func = self.l2_class_loss\n",
        "    elif class_loss_criterion in ['bce', 'BCE']:\n",
        "      class_loss_func = self.bce_class_loss\n",
        "    elif class_loss_criterion in ['ce', 'CE']:\n",
        "      class_loss_func = self.ce_class_loss\n",
        "\n",
        "    if dist_loss_criterion in ['l2', 'L2']:\n",
        "      dist_loss_func = self.l2_dist_loss\n",
        "    elif dist_loss_criterion in ['bce', 'BCE']:\n",
        "      dist_loss_func = self.bce_dist_loss\n",
        "    elif dist_loss_criterion in ['ce', 'CE']:\n",
        "      dist_loss_func = self.ce_dist_loss\n",
        "\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    \n",
        "    def class_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'class')\n",
        "      return alpha*class_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    def dist_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "      return lambda0*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    return class_loss, dist_loss\n",
        "\n",
        "  def bce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def bce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def ce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, self.reverse_index.getNodes(labels), decode=False, row_start=row_start, row_end=row_end, col_start=None, col_end=col_end)\n",
        "    \n",
        "  def ce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, labels, decode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "\n",
        "  def bce_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
        "\n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "\n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  def ce_loss(self, outputs, labels, decode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if decode:\n",
        "      labels = torch.argmax(labels, dim=1)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end])\n",
        "\n",
        "\n",
        "  def l2_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.MSELoss(reduction = 'mean')\n",
        "    \n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  # implementation of alg. 5 of icarl paper\n",
        "  # iCaRL ReduceExemplarSet\n",
        "  def reduce_exemplar_sets(self, m):\n",
        "  \t    # i keep only the first m exemplar images\n",
        "        # where m is the UPDATED K/number_classes_seen\n",
        "        # the number of images per each exemplar set (class) progressively decreases\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:m] \n",
        "        for x, P_x in enumerate(self.exemplar_sets_indices):\n",
        "            self.exemplar_sets_indices[x] = P_x[:m] \n",
        "\n",
        "\n",
        "# ---------- \n",
        "from torch.utils.data import Dataset\n",
        "\"\"\"\n",
        "  Merge two different datasets (train and exemplars in our case)\n",
        "  format:\n",
        "  train\n",
        "  --------\n",
        "  exemplars\n",
        "  train leans on cifar100\n",
        "  exemplars is managed here (exemplar_transform is performed) => changed\n",
        "\"\"\"\n",
        "class ConcatDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, dataset1, dataset2):\n",
        "        self.dataset1 = dataset1\n",
        "        self.dataset2 = dataset2\n",
        "        self.l1 = len(dataset1)\n",
        "        self.l2 = len(dataset2)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        if index < self.l1:\n",
        "            _, image,label = self.dataset1[index] #here it leans on cifar100 get item\n",
        "            return _, image,label\n",
        "        else:\n",
        "            _, image, label = self.dataset2[index - self.l1]\n",
        "            return _, image,label\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.l1 + self.l2)\n",
        "#------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "outputId": "05c58bb4-6a60-4cc0-d9c8-2e06868e61b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, herding, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "outputId": "50785313-a3f9-4712-82fc-608e0764bc1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "ALL DB:  5000\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:396: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LOSS:  0.32529017329216003 class loss 0.32529017329216003 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.3132565915584564 class loss 0.3132565915584564 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.31138092279434204 class loss 0.31138092279434204 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.30321189761161804 class loss 0.30321189761161804 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.30343085527420044 class loss 0.30343085527420044 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.31456074118614197 class loss 0.31456074118614197 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.30319127440452576 class loss 0.30319127440452576 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.281195729970932 class loss 0.281195729970932 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.27044960856437683 class loss 0.27044960856437683 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.2790062129497528 class loss 0.2790062129497528 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.2704424262046814 class loss 0.2704424262046814 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.2827659547328949 class loss 0.2827659547328949 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.2582068145275116 class loss 0.2582068145275116 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.28309890627861023 class loss 0.28309890627861023 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.2858341634273529 class loss 0.2858341634273529 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.246379092335701 class loss 0.246379092335701 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.25693485140800476 class loss 0.25693485140800476 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.2356833517551422 class loss 0.2356833517551422 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.2373788207769394 class loss 0.2373788207769394 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.2620912492275238 class loss 0.2620912492275238 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.2302323430776596 class loss 0.2302323430776596 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.2369733303785324 class loss 0.2369733303785324 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.21896834671497345 class loss 0.21896834671497345 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.18758992850780487 class loss 0.18758992850780487 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.23583455383777618 class loss 0.23583455383777618 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.22063496708869934 class loss 0.22063496708869934 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.23617029190063477 class loss 0.23617029190063477 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.2041129171848297 class loss 0.2041129171848297 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.19197498261928558 class loss 0.19197498261928558 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.1853436827659607 class loss 0.1853436827659607 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.18006375432014465 class loss 0.18006375432014465 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.18321719765663147 class loss 0.18321719765663147 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.18264055252075195 class loss 0.18264055252075195 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.14275915920734406 class loss 0.14275915920734406 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.15596283972263336 class loss 0.15596283972263336 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.1486380696296692 class loss 0.1486380696296692 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.16434195637702942 class loss 0.16434195637702942 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.13017451763153076 class loss 0.13017451763153076 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.17504285275936127 class loss 0.17504285275936127 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.1201336607336998 class loss 0.1201336607336998 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.11135774850845337 class loss 0.11135774850845337 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.14718127250671387 class loss 0.14718127250671387 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.13521797955036163 class loss 0.13521797955036163 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.09831266850233078 class loss 0.09831266850233078 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.12971225380897522 class loss 0.12971225380897522 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.12527652084827423 class loss 0.12527652084827423 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.08414077758789062 class loss 0.08414077758789062 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.0813966616988182 class loss 0.0813966616988182 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.07736314833164215 class loss 0.07736314833164215 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.02630920708179474 class loss 0.02630920708179474 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.03433907777070999 class loss 0.03433907777070999 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.021860802546143532 class loss 0.021860802546143532 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.022609522566199303 class loss 0.022609522566199303 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.010419311933219433 class loss 0.010419311933219433 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.0035918119829148054 class loss 0.0035918119829148054 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.007182547356933355 class loss 0.007182547356933355 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.009476817212998867 class loss 0.009476817212998867 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.004336924757808447 class loss 0.004336924757808447 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.004082733299583197 class loss 0.004082733299583197 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.0019156866474077106 class loss 0.0019156866474077106 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.0012329895980656147 class loss 0.0012329895980656147 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.0016196806682273746 class loss 0.0016196806682273746 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.0020601924043148756 class loss 0.0020601924043148756 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.0014424807159230113 class loss 0.0014424807159230113 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0016850989777594805 class loss 0.0016850989777594805 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0019624363631010056 class loss 0.0019624363631010056 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0007704246090725064 class loss 0.0007704246090725064 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.000638522207736969 class loss 0.000638522207736969 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0026693427935242653 class loss 0.0026693427935242653 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0016678624087944627 class loss 0.0016678624087944627 dist loss None older dist loss None\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 81.00\n",
            "\n",
            "Test Accuracy (all groups seen so far): 74.70\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "ALL DB:  8550\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.13757145404815674 class loss 0.0834515169262886 dist loss 0.054119933396577835 older dist loss None\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.11673198640346527 class loss 0.06580068916082382 dist loss 0.05093130096793175 older dist loss None\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.11528361588716507 class loss 0.0725555494427681 dist loss 0.04272806644439697 older dist loss None\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.0984298586845398 class loss 0.05274614319205284 dist loss 0.045683715492486954 older dist loss None\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.09924782812595367 class loss 0.059303831309080124 dist loss 0.03994399309158325 older dist loss None\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.10091971606016159 class loss 0.05227827653288841 dist loss 0.04864143952727318 older dist loss None\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.0779523029923439 class loss 0.03935801610350609 dist loss 0.038594286888837814 older dist loss None\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.07995520532131195 class loss 0.03967918083071709 dist loss 0.040276020765304565 older dist loss None\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.07984751462936401 class loss 0.03893302008509636 dist loss 0.04091449826955795 older dist loss None\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.07032661139965057 class loss 0.027623310685157776 dist loss 0.0427033007144928 older dist loss None\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.0772753581404686 class loss 0.03494631499052048 dist loss 0.04232904314994812 older dist loss None\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.0668313056230545 class loss 0.02763221226632595 dist loss 0.0391990952193737 older dist loss None\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.07052011787891388 class loss 0.027786925435066223 dist loss 0.042733192443847656 older dist loss None\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.058580078184604645 class loss 0.018034113571047783 dist loss 0.04054596647620201 older dist loss None\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.07623868435621262 class loss 0.02900429628789425 dist loss 0.047234389930963516 older dist loss None\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.05886729434132576 class loss 0.014598958194255829 dist loss 0.04426833614706993 older dist loss None\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.052807874977588654 class loss 0.015875546261668205 dist loss 0.0369323268532753 older dist loss None\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.05060388147830963 class loss 0.010946705937385559 dist loss 0.03965717554092407 older dist loss None\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.04371482878923416 class loss 0.0047276499681174755 dist loss 0.0389871783554554 older dist loss None\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.05085105821490288 class loss 0.011969256214797497 dist loss 0.038881801068782806 older dist loss None\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.05216997116804123 class loss 0.011258719488978386 dist loss 0.04091125354170799 older dist loss None\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.041095033288002014 class loss 0.005979116540402174 dist loss 0.03511591628193855 older dist loss None\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.04446788877248764 class loss 0.005047081038355827 dist loss 0.039420805871486664 older dist loss None\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.04862742871046066 class loss 0.007259042002260685 dist loss 0.04136838763952255 older dist loss None\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.04107513278722763 class loss 0.0060829357244074345 dist loss 0.034992195665836334 older dist loss None\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.038977108895778656 class loss 0.005492463707923889 dist loss 0.03348464518785477 older dist loss None\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.039306703954935074 class loss 0.0035025121178478003 dist loss 0.03580419346690178 older dist loss None\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.03762125223875046 class loss 0.004241172224283218 dist loss 0.03338008001446724 older dist loss None\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.038490548729896545 class loss 0.0042158761061728 dist loss 0.03427467122673988 older dist loss None\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.036828868091106415 class loss 0.003452156437560916 dist loss 0.03337671235203743 older dist loss None\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.04248983785510063 class loss 0.004852533806115389 dist loss 0.03763730451464653 older dist loss None\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.04713793098926544 class loss 0.009426685981452465 dist loss 0.0377112440764904 older dist loss None\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.03727380558848381 class loss 0.0016535058384761214 dist loss 0.035620298236608505 older dist loss None\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.035646095871925354 class loss 0.00198740279302001 dist loss 0.033658694475889206 older dist loss None\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.031656015664339066 class loss 0.0020879788789898157 dist loss 0.029568035155534744 older dist loss None\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.040111053735017776 class loss 0.004483008291572332 dist loss 0.035628046840429306 older dist loss None\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.04608120769262314 class loss 0.0059004477225244045 dist loss 0.040180761367082596 older dist loss None\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.03406582772731781 class loss 0.001761260675266385 dist loss 0.032304566353559494 older dist loss None\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.03948313370347023 class loss 0.005996966268867254 dist loss 0.03348616883158684 older dist loss None\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.03416626900434494 class loss 0.0020213841926306486 dist loss 0.03214488551020622 older dist loss None\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.03369348496198654 class loss 0.003084594151005149 dist loss 0.03060889057815075 older dist loss None\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.037932101637125015 class loss 0.0026043232064694166 dist loss 0.03532777726650238 older dist loss None\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.04162737727165222 class loss 0.003097118576988578 dist loss 0.03853026032447815 older dist loss None\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.03084494359791279 class loss 0.0014417091151699424 dist loss 0.029403233900666237 older dist loss None\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.04049224406480789 class loss 0.0032670553773641586 dist loss 0.037225186824798584 older dist loss None\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.037115126848220825 class loss 0.0022201186511665583 dist loss 0.03489500656723976 older dist loss None\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.03544660285115242 class loss 0.0011815339094027877 dist loss 0.03426506742835045 older dist loss None\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.03709957003593445 class loss 0.0021003875881433487 dist loss 0.03499918058514595 older dist loss None\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.03939076513051987 class loss 0.002581087639555335 dist loss 0.03680967912077904 older dist loss None\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.03135044127702713 class loss 0.0007142061367630959 dist loss 0.03063623420894146 older dist loss None\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.029931554570794106 class loss 0.0007416664739139378 dist loss 0.029189888387918472 older dist loss None\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.027451351284980774 class loss 0.00023840312496758997 dist loss 0.027212947607040405 older dist loss None\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.02849154733121395 class loss 0.0004145288548897952 dist loss 0.0280770193785429 older dist loss None\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.026026543229818344 class loss 0.0001805080391932279 dist loss 0.025846034288406372 older dist loss None\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.024946464225649834 class loss 0.00023196381516754627 dist loss 0.024714499711990356 older dist loss None\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.028450217097997665 class loss 0.00013275996025186032 dist loss 0.02831745706498623 older dist loss None\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.02885862998664379 class loss 0.0002136458206223324 dist loss 0.028644984588027 older dist loss None\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.024335496127605438 class loss 0.00018604246724862605 dist loss 0.0241494532674551 older dist loss None\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.02409796603024006 class loss 0.0001678554544923827 dist loss 0.023930110037326813 older dist loss None\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.024417977780103683 class loss 0.0001517138589406386 dist loss 0.024266263470053673 older dist loss None\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.026334688067436218 class loss 0.0001521856029285118 dist loss 0.026182502508163452 older dist loss None\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.025297988206148148 class loss 0.00013277881953399628 dist loss 0.02516520954668522 older dist loss None\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.028615396469831467 class loss 0.00025469702086411417 dist loss 0.028360700234770775 older dist loss None\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.026897579431533813 class loss 0.0001633489300729707 dist loss 0.026734231039881706 older dist loss None\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.028414245694875717 class loss 0.00013201571709942073 dist loss 0.028282230719923973 older dist loss None\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.026823516935110092 class loss 0.00014400276995729655 dist loss 0.026679513975977898 older dist loss None\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.029978467151522636 class loss 0.0002124794846167788 dist loss 0.029765987768769264 older dist loss None\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.029209187254309654 class loss 0.0001254749804502353 dist loss 0.02908371202647686 older dist loss None\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.02655537985265255 class loss 0.0001257135154446587 dist loss 0.026429666206240654 older dist loss None\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.024514300748705864 class loss 0.00019331184739712626 dist loss 0.02432098798453808 older dist loss None\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 61.56\n",
            "\n",
            "Test Accuracy (all groups seen so far): 61.10\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "ALL DB:  8100\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.16479751467704773 class loss 0.04460442066192627 dist loss 0.05642376467585564 older dist loss 0.06376933306455612\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.14284616708755493 class loss 0.044546060264110565 dist loss 0.044083371758461 older dist loss 0.054216742515563965\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.13073945045471191 class loss 0.038229815661907196 dist loss 0.04525730758905411 older dist loss 0.04725231975317001\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.1269398182630539 class loss 0.036295726895332336 dist loss 0.04313581809401512 older dist loss 0.04750826582312584\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.11961939930915833 class loss 0.027348283678293228 dist loss 0.04636654257774353 older dist loss 0.04590457305312157\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.11161486804485321 class loss 0.02297356352210045 dist loss 0.042816027998924255 older dist loss 0.045825280249118805\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.09817285090684891 class loss 0.019557008519768715 dist loss 0.039419710636138916 older dist loss 0.039196133613586426\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.11014659702777863 class loss 0.02653992548584938 dist loss 0.04510442540049553 older dist loss 0.03850223869085312\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.11500678956508636 class loss 0.03326839953660965 dist loss 0.04138907790184021 older dist loss 0.04034930840134621\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.1021561399102211 class loss 0.01846281997859478 dist loss 0.04309125244617462 older dist loss 0.04060206934809685\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.10097823292016983 class loss 0.012766201049089432 dist loss 0.044989146292209625 older dist loss 0.04322288557887077\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.09805674850940704 class loss 0.012613936327397823 dist loss 0.04290643334388733 older dist loss 0.04253637418150902\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.08879189193248749 class loss 0.015028426423668861 dist loss 0.03798358514904976 older dist loss 0.03577987849712372\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.0912407711148262 class loss 0.01027258113026619 dist loss 0.04345173388719559 older dist loss 0.037516456097364426\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.09398715198040009 class loss 0.008438696153461933 dist loss 0.04222562536597252 older dist loss 0.04332282766699791\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.09043213725090027 class loss 0.008563248440623283 dist loss 0.043742671608924866 older dist loss 0.03812621906399727\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.08572795987129211 class loss 0.003815519390627742 dist loss 0.04135214537382126 older dist loss 0.040560293942689896\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.09554195404052734 class loss 0.010327508673071861 dist loss 0.046113934367895126 older dist loss 0.03910050541162491\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.09245572239160538 class loss 0.005113678984344006 dist loss 0.042435552924871445 older dist loss 0.04490648955106735\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.07973414659500122 class loss 0.0037616905756294727 dist loss 0.042931776493787766 older dist loss 0.03304068371653557\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.09020446985960007 class loss 0.0028069789987057447 dist loss 0.04415857419371605 older dist loss 0.043238915503025055\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.08202821016311646 class loss 0.0043885535560548306 dist loss 0.040464796125888824 older dist loss 0.03717486187815666\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.08621114492416382 class loss 0.0032698248978704214 dist loss 0.04591965675354004 older dist loss 0.03702166676521301\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.08374178409576416 class loss 0.002938472665846348 dist loss 0.0406985767185688 older dist loss 0.040104735642671585\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.08399392664432526 class loss 0.0018104321788996458 dist loss 0.042328447103500366 older dist loss 0.03985504433512688\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.07769082486629486 class loss 0.0037426676135510206 dist loss 0.03876684978604317 older dist loss 0.03518131002783775\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.07958468794822693 class loss 0.0031982334330677986 dist loss 0.039319541305303574 older dist loss 0.03706691786646843\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.0854693129658699 class loss 0.003199099563062191 dist loss 0.044324424117803574 older dist loss 0.037945788353681564\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.07590503990650177 class loss 0.0020776737947016954 dist loss 0.03753767907619476 older dist loss 0.03628969192504883\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.07990036904811859 class loss 0.0025976584292948246 dist loss 0.041236475110054016 older dist loss 0.03606623411178589\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.08453517407178879 class loss 0.001720281084999442 dist loss 0.04067298397421837 older dist loss 0.04214191064238548\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.08039985597133636 class loss 0.002247154712677002 dist loss 0.04214102774858475 older dist loss 0.036011673510074615\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.0771578997373581 class loss 0.001413693418726325 dist loss 0.039924997836351395 older dist loss 0.03581921383738518\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.08224828541278839 class loss 0.0013918945332989097 dist loss 0.04272567853331566 older dist loss 0.038130711764097214\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.07975634187459946 class loss 0.0017225472256541252 dist loss 0.04001764580607414 older dist loss 0.038016147911548615\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.08080506324768066 class loss 0.0019795724656432867 dist loss 0.04041130840778351 older dist loss 0.03841418772935867\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.08297375589609146 class loss 0.0017606164328753948 dist loss 0.042002931237220764 older dist loss 0.039210207760334015\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.07389994710683823 class loss 0.002823428250849247 dist loss 0.037297673523426056 older dist loss 0.0337788462638855\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.07801927626132965 class loss 0.001446166425012052 dist loss 0.038386404514312744 older dist loss 0.03818671032786369\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.07405838370323181 class loss 0.0021306690759956837 dist loss 0.03833574801683426 older dist loss 0.033591967076063156\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.07760876417160034 class loss 0.0027681903447955847 dist loss 0.03986731544137001 older dist loss 0.03497326001524925\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.07885134220123291 class loss 0.0013069974957033992 dist loss 0.03969073295593262 older dist loss 0.03785360977053642\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.08245857059955597 class loss 0.0016115199541673064 dist loss 0.04038029909133911 older dist loss 0.04046675190329552\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.077488973736763 class loss 0.0013053234433755279 dist loss 0.03930624574422836 older dist loss 0.036877404898405075\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.0815218985080719 class loss 0.0019874409772455692 dist loss 0.04055889695882797 older dist loss 0.038975562900304794\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.08262645453214645 class loss 0.0007947531994432211 dist loss 0.042253341525793076 older dist loss 0.039578359574079514\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.08072999119758606 class loss 0.0020951074548065662 dist loss 0.04011096432805061 older dist loss 0.038523923605680466\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.08029624819755554 class loss 0.001269171480089426 dist loss 0.043072741478681564 older dist loss 0.03595433756709099\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.07627230882644653 class loss 0.0007557863718830049 dist loss 0.037651028484106064 older dist loss 0.03786548972129822\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06920270621776581 class loss 0.0006504155462607741 dist loss 0.03817377984523773 older dist loss 0.030378514900803566\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.06996838003396988 class loss 0.0002835329214576632 dist loss 0.03710183873772621 older dist loss 0.03258300945162773\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.0720876008272171 class loss 0.00025585858384147286 dist loss 0.03745428845286369 older dist loss 0.03437745198607445\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.07098419964313507 class loss 0.0002509020850993693 dist loss 0.03585514798760414 older dist loss 0.03487814962863922\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.06357847154140472 class loss 0.00016266226884908974 dist loss 0.034459419548511505 older dist loss 0.02895638905465603\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.06388556957244873 class loss 0.000262714980635792 dist loss 0.034040018916130066 older dist loss 0.029582837596535683\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.07095839083194733 class loss 0.0002186624042224139 dist loss 0.03913380205631256 older dist loss 0.031605929136276245\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.06706400215625763 class loss 0.0002209460799349472 dist loss 0.03600747138261795 older dist loss 0.0308355875313282\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.06865157932043076 class loss 0.00023946078727021813 dist loss 0.03837974742054939 older dist loss 0.03003237210214138\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.07066904008388519 class loss 0.0001906510442495346 dist loss 0.038185134530067444 older dist loss 0.032293256372213364\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.0625707358121872 class loss 0.00020875832706224173 dist loss 0.03308918699622154 older dist loss 0.029272792860865593\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.06699684262275696 class loss 0.00019699815311469138 dist loss 0.03689522296190262 older dist loss 0.029904622584581375\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.06577937304973602 class loss 0.00017189164645969868 dist loss 0.034567706286907196 older dist loss 0.031039774417877197\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.06364995241165161 class loss 0.0002689529210329056 dist loss 0.034848902374506 older dist loss 0.02853209339082241\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.06554526835680008 class loss 0.00014474656200036407 dist loss 0.03535602241754532 older dist loss 0.030044501647353172\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.07114801555871964 class loss 0.00015859463019296527 dist loss 0.03889402002096176 older dist loss 0.03209540247917175\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06933580338954926 class loss 0.0002104355808114633 dist loss 0.0358310341835022 older dist loss 0.03329433128237724\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06518975645303726 class loss 0.00020818531629629433 dist loss 0.03488273173570633 older dist loss 0.030098838731646538\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06964439153671265 class loss 0.0002028177259489894 dist loss 0.0365695059299469 older dist loss 0.03287206590175629\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0655108317732811 class loss 0.0001696426043054089 dist loss 0.03444822132587433 older dist loss 0.03089296817779541\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06944919377565384 class loss 0.0001854520378401503 dist loss 0.037494029849767685 older dist loss 0.03176971152424812\n",
            "Reducing each exemplar set to size: 67\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 56.04\n",
            "\n",
            "Test Accuracy (all groups seen so far): 52.53\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "ALL DB:  7670\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.11201121658086777 class loss 0.0356854572892189 dist loss 0.03948957473039627 older dist loss 0.03683618828654289\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.10235504806041718 class loss 0.028191495686769485 dist loss 0.0403488464653492 older dist loss 0.03381470590829849\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.09016510844230652 class loss 0.024214738979935646 dist loss 0.03722640499472618 older dist loss 0.028723960742354393\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.09555180370807648 class loss 0.0267894696444273 dist loss 0.03951934352517128 older dist loss 0.0292429868131876\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.0924692153930664 class loss 0.019796734675765038 dist loss 0.04032241180539131 older dist loss 0.03235006332397461\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.08901158720254898 class loss 0.02413942478597164 dist loss 0.0361911877989769 older dist loss 0.02868097461760044\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.08407333493232727 class loss 0.018051309511065483 dist loss 0.03677855059504509 older dist loss 0.02924346923828125\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.08424420654773712 class loss 0.016531066969037056 dist loss 0.038026656955480576 older dist loss 0.02968648634850979\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.08179480582475662 class loss 0.014408612623810768 dist loss 0.03870768845081329 older dist loss 0.02867850661277771\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.08484010398387909 class loss 0.013136920519173145 dist loss 0.03927672281861305 older dist loss 0.03242645785212517\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.07589679956436157 class loss 0.006952354218810797 dist loss 0.038037147372961044 older dist loss 0.030907293781638145\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.07856947183609009 class loss 0.006034992169588804 dist loss 0.04110514745116234 older dist loss 0.03142932802438736\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.07247294485569 class loss 0.0046339379623532295 dist loss 0.03818104416131973 older dist loss 0.029657963663339615\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.07111603021621704 class loss 0.004679293837398291 dist loss 0.038304802030324936 older dist loss 0.028131937608122826\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.07261866331100464 class loss 0.003799306694418192 dist loss 0.03857581317424774 older dist loss 0.03024354577064514\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.06459662318229675 class loss 0.0029048945289105177 dist loss 0.03611506149172783 older dist loss 0.025576669722795486\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.0715680867433548 class loss 0.004669286776334047 dist loss 0.03727613389492035 older dist loss 0.029622668400406837\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.06428121030330658 class loss 0.002114823553711176 dist loss 0.03603914752602577 older dist loss 0.02612723782658577\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.06513683497905731 class loss 0.0028091163840144873 dist loss 0.036953989416360855 older dist loss 0.025373727083206177\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.06340958923101425 class loss 0.0016477954341098666 dist loss 0.03657688945531845 older dist loss 0.025184905156493187\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.0644519031047821 class loss 0.001695286831818521 dist loss 0.03589208051562309 older dist loss 0.026864532381296158\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.06459452211856842 class loss 0.0013875890290364623 dist loss 0.0368373766541481 older dist loss 0.026369554921984673\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.06530921161174774 class loss 0.0007988973520696163 dist loss 0.03674423694610596 older dist loss 0.027766073122620583\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.06306478381156921 class loss 0.0006530102691613138 dist loss 0.035778939723968506 older dist loss 0.02663283422589302\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.0681598037481308 class loss 0.0017838322091847658 dist loss 0.03869473561644554 older dist loss 0.02768123708665371\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.06838199496269226 class loss 0.0011081852717325091 dist loss 0.037919607013463974 older dist loss 0.02935420349240303\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.06733600795269012 class loss 0.0011852331226691604 dist loss 0.037867993116378784 older dist loss 0.028282785788178444\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.06385237723588943 class loss 0.0020836484618484974 dist loss 0.036613818258047104 older dist loss 0.02515490911900997\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.06365522742271423 class loss 0.001540481112897396 dist loss 0.03570020943880081 older dist loss 0.026414532214403152\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.06486692279577255 class loss 0.0009439613786526024 dist loss 0.03568919003009796 older dist loss 0.02823377214372158\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.06597083806991577 class loss 0.001522246515378356 dist loss 0.03720969706773758 older dist loss 0.027238894253969193\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.07171960920095444 class loss 0.001158187398687005 dist loss 0.04029116779565811 older dist loss 0.030270254239439964\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.06758148968219757 class loss 0.0013705920428037643 dist loss 0.03700646758079529 older dist loss 0.02920442633330822\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.0645739957690239 class loss 0.000695404305588454 dist loss 0.03634391725063324 older dist loss 0.02753467671573162\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.06641592085361481 class loss 0.0013511009747162461 dist loss 0.03720029816031456 older dist loss 0.027864521369338036\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.0665067732334137 class loss 0.0011055255308747292 dist loss 0.03775525465607643 older dist loss 0.027645990252494812\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.07177887111902237 class loss 0.0013074054149910808 dist loss 0.03946251422166824 older dist loss 0.031008953228592873\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.06346545368432999 class loss 0.0007695158128626645 dist loss 0.036134518682956696 older dist loss 0.026561418548226357\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.061650827527046204 class loss 0.0007856848533265293 dist loss 0.03534615784883499 older dist loss 0.025518983602523804\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.06646108627319336 class loss 0.0011837305501103401 dist loss 0.03694021701812744 older dist loss 0.0283371414989233\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.07144688069820404 class loss 0.000639979843981564 dist loss 0.0389232374727726 older dist loss 0.03188366815447807\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.06617938727140427 class loss 0.0011155171087011695 dist loss 0.03759581595659256 older dist loss 0.027468055486679077\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.06634156405925751 class loss 0.001360965776257217 dist loss 0.03690548613667488 older dist loss 0.028075113892555237\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.06686273217201233 class loss 0.0010249974438920617 dist loss 0.0374273806810379 older dist loss 0.028410350903868675\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.06457522511482239 class loss 0.0011810268042609096 dist loss 0.03614264354109764 older dist loss 0.027251554653048515\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.060338813811540604 class loss 0.0015223317313939333 dist loss 0.033960986882448196 older dist loss 0.024855494499206543\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.06615471839904785 class loss 0.0004766721685882658 dist loss 0.03832042217254639 older dist loss 0.02735762670636177\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.06279146671295166 class loss 0.0011044068960472941 dist loss 0.03645080327987671 older dist loss 0.025236254557967186\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.06857281923294067 class loss 0.0012597577879205346 dist loss 0.03869113698601723 older dist loss 0.02862192504107952\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.061227213591337204 class loss 0.00023279408924281597 dist loss 0.03537321090698242 older dist loss 0.025621209293603897\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.05939275771379471 class loss 0.0001545969134895131 dist loss 0.03488486632704735 older dist loss 0.02435329370200634\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.061229486018419266 class loss 0.00017248248332180083 dist loss 0.035762034356594086 older dist loss 0.025294968858361244\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.06380919367074966 class loss 0.0002546550240367651 dist loss 0.03649425134062767 older dist loss 0.027060285210609436\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.06365653872489929 class loss 0.00018326153804082423 dist loss 0.03674730658531189 older dist loss 0.026725968345999718\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.05988116189837456 class loss 0.00016426038928329945 dist loss 0.03386988863348961 older dist loss 0.02584701217710972\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.05792566016316414 class loss 0.00015569236711598933 dist loss 0.0328112468123436 older dist loss 0.024958720430731773\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.05711232125759125 class loss 0.0001707995979813859 dist loss 0.03280150890350342 older dist loss 0.0241400133818388\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.06295748054981232 class loss 0.00017250060045626014 dist loss 0.03595218062400818 older dist loss 0.02683279849588871\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.06096646189689636 class loss 0.00013282788859214634 dist loss 0.03573889285326004 older dist loss 0.025094741955399513\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.058016687631607056 class loss 0.00018443359294906259 dist loss 0.034799542278051376 older dist loss 0.023032713681459427\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.06055412441492081 class loss 0.00020786040113307536 dist loss 0.03469712287187576 older dist loss 0.025649143382906914\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.060421887785196304 class loss 0.00017826228577177972 dist loss 0.034835267812013626 older dist loss 0.02540835738182068\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.060984596610069275 class loss 0.0001254714879905805 dist loss 0.03391744941473007 older dist loss 0.026941675692796707\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.06236490607261658 class loss 0.00022218776575755328 dist loss 0.03613712266087532 older dist loss 0.026005594059824944\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06347855925559998 class loss 0.0001628421014174819 dist loss 0.03668621927499771 older dist loss 0.0266294963657856\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0600995309650898 class loss 0.00017365298117510974 dist loss 0.03555731847882271 older dist loss 0.024368559941649437\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.062439367175102234 class loss 0.00016353845421690494 dist loss 0.036534592509269714 older dist loss 0.02574123814702034\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06118526682257652 class loss 0.00015787608572281897 dist loss 0.03521580249071121 older dist loss 0.025811588391661644\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05635511875152588 class loss 0.00012705735571216792 dist loss 0.03227877616882324 older dist loss 0.023949285969138145\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05871063470840454 class loss 0.0001682102301856503 dist loss 0.033796027302742004 older dist loss 0.024746395647525787\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 39.90\n",
            "\n",
            "Test Accuracy (all groups seen so far): 45.50\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "ALL DB:  7200\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.0924491435289383 class loss 0.024000754579901695 dist loss 0.039229463785886765 older dist loss 0.029218925163149834\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.08968634903430939 class loss 0.021462110802531242 dist loss 0.03922451660037041 older dist loss 0.02899971976876259\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.09139500558376312 class loss 0.021244680508971214 dist loss 0.039819009602069855 older dist loss 0.030331319198012352\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.08564381301403046 class loss 0.01880110427737236 dist loss 0.03896763548254967 older dist loss 0.02787507325410843\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.08132076263427734 class loss 0.016171688213944435 dist loss 0.03784056380391121 older dist loss 0.027308516204357147\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.07479840517044067 class loss 0.009181930683553219 dist loss 0.03809129819273949 older dist loss 0.02752518095076084\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.07326267659664154 class loss 0.00815847609192133 dist loss 0.03897171840071678 older dist loss 0.026132481172680855\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.07211670279502869 class loss 0.008535441011190414 dist loss 0.037248991429805756 older dist loss 0.026332272216677666\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.07583190500736237 class loss 0.005825333762913942 dist loss 0.0416158102452755 older dist loss 0.02839076519012451\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.06778138875961304 class loss 0.0045021455734968185 dist loss 0.03704181686043739 older dist loss 0.026237430050969124\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.0680115669965744 class loss 0.003205579472705722 dist loss 0.038524847477674484 older dist loss 0.026281138882040977\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.06800556182861328 class loss 0.0025553195737302303 dist loss 0.03864550590515137 older dist loss 0.026804734021425247\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.06735177338123322 class loss 0.0023637765552848577 dist loss 0.038177795708179474 older dist loss 0.02681020088493824\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.06491446495056152 class loss 0.0017178846755996346 dist loss 0.03722546994686127 older dist loss 0.02597111091017723\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.06253519654273987 class loss 0.0014876871136948466 dist loss 0.03571689873933792 older dist loss 0.025330612435936928\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.06504172831773758 class loss 0.0009719011723063886 dist loss 0.03850989043712616 older dist loss 0.025559935718774796\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.06330536305904388 class loss 0.0010005395160987973 dist loss 0.036939963698387146 older dist loss 0.025364860892295837\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.06290994584560394 class loss 0.0012152576819062233 dist loss 0.03592071309685707 older dist loss 0.025773970410227776\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.06399814784526825 class loss 0.0009603155194781721 dist loss 0.03664417937397957 older dist loss 0.026393650099635124\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.06157516688108444 class loss 0.0007076564943417907 dist loss 0.035831257700920105 older dist loss 0.025036251172423363\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.06279245018959045 class loss 0.000627575849648565 dist loss 0.03760745748877525 older dist loss 0.024557417258620262\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.06051432341337204 class loss 0.0007397346198558807 dist loss 0.036007676273584366 older dist loss 0.023766912519931793\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.06537246704101562 class loss 0.0005408867727965117 dist loss 0.038633864372968674 older dist loss 0.026197712868452072\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.06232994422316551 class loss 0.0008194280089810491 dist loss 0.03693169727921486 older dist loss 0.024578819051384926\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.06381721794605255 class loss 0.0006214005989022553 dist loss 0.03827410563826561 older dist loss 0.024921709671616554\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.060367435216903687 class loss 0.0006793410866521299 dist loss 0.03558148071169853 older dist loss 0.02410661242902279\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.06504954397678375 class loss 0.0005550836795009673 dist loss 0.03779279440641403 older dist loss 0.026701670140028\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.060661233961582184 class loss 0.0005684729549102485 dist loss 0.035996709018945694 older dist loss 0.02409605123102665\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.06474782526493073 class loss 0.00047701160656288266 dist loss 0.038624491542577744 older dist loss 0.02564631961286068\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.061878662556409836 class loss 0.00041444608359597623 dist loss 0.03661980852484703 older dist loss 0.02484440803527832\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.061584096401929855 class loss 0.0009015329414978623 dist loss 0.036746468394994736 older dist loss 0.023936094716191292\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.061881184577941895 class loss 0.0009051598608493805 dist loss 0.036497365683317184 older dist loss 0.02447865903377533\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.06106977164745331 class loss 0.0006091092946007848 dist loss 0.03579378128051758 older dist loss 0.024666881188750267\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.06595799326896667 class loss 0.0012630387209355831 dist loss 0.03816644474864006 older dist loss 0.026528511196374893\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.06149169057607651 class loss 0.0008375495090149343 dist loss 0.03606804087758064 older dist loss 0.02458610199391842\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.06487914174795151 class loss 0.0005039671086706221 dist loss 0.03802219405770302 older dist loss 0.026352979242801666\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.0642935261130333 class loss 0.0010009079705923796 dist loss 0.03824896365404129 older dist loss 0.025043655186891556\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.06415002048015594 class loss 0.0010336466366425157 dist loss 0.037367083132267 older dist loss 0.025749292224645615\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.06914481520652771 class loss 0.0008187179337255657 dist loss 0.040148116648197174 older dist loss 0.028177978470921516\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.06268574297428131 class loss 0.0008381337975151837 dist loss 0.03693776950240135 older dist loss 0.024909842759370804\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.0654180720448494 class loss 0.0006594785954803228 dist loss 0.038004737347364426 older dist loss 0.026753855869174004\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.06305094063282013 class loss 0.0007527802372351289 dist loss 0.037037238478660583 older dist loss 0.0252609234303236\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.0650973990559578 class loss 0.001021232339553535 dist loss 0.03794034197926521 older dist loss 0.026135824620723724\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.06307666748762131 class loss 0.0004413798451423645 dist loss 0.0378982312977314 older dist loss 0.024737054482102394\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.06321192532777786 class loss 0.0009650718420743942 dist loss 0.03749928995966911 older dist loss 0.024747563526034355\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.06621165573596954 class loss 0.000552062876522541 dist loss 0.03903951868414879 older dist loss 0.02662007324397564\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.06345512717962265 class loss 0.0005436543724499643 dist loss 0.03608369082212448 older dist loss 0.02682778239250183\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.06463386863470078 class loss 0.000500523135997355 dist loss 0.037742163985967636 older dist loss 0.026391183957457542\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.06124477833509445 class loss 0.00032876268960535526 dist loss 0.03684256598353386 older dist loss 0.02407345175743103\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.057764798402786255 class loss 0.00018747975991573185 dist loss 0.03467018902301788 older dist loss 0.022907130420207977\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.06310964375734329 class loss 0.0002548781631048769 dist loss 0.037196751683950424 older dist loss 0.02565801329910755\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.058840446174144745 class loss 0.00016876902373041958 dist loss 0.034804392606019974 older dist loss 0.023867284879088402\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.055802106857299805 class loss 0.00011618663847912103 dist loss 0.03317917510867119 older dist loss 0.022506747394800186\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.06002317741513252 class loss 0.0001563051191624254 dist loss 0.035446930676698685 older dist loss 0.024419941008090973\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.05982159823179245 class loss 0.00014018555521033704 dist loss 0.035753075033426285 older dist loss 0.023928338661789894\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.05621467903256416 class loss 0.00013896269956603646 dist loss 0.033889561891555786 older dist loss 0.02218615449965\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.06007775291800499 class loss 0.00015725592675153166 dist loss 0.03548333793878555 older dist loss 0.024437159299850464\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.06156574934720993 class loss 0.00027170105022378266 dist loss 0.0366288386285305 older dist loss 0.024665212258696556\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.05734919384121895 class loss 0.00010952151205856353 dist loss 0.03420978784561157 older dist loss 0.023029884323477745\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.0587761253118515 class loss 0.00013984488032292575 dist loss 0.03470855951309204 older dist loss 0.0239277184009552\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.06124141439795494 class loss 0.00013684229634236544 dist loss 0.03661391884088516 older dist loss 0.024490652605891228\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.05722181871533394 class loss 0.0001225674495799467 dist loss 0.03324243798851967 older dist loss 0.023856813088059425\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.05780370905995369 class loss 0.00018639862537384033 dist loss 0.03448828309774399 older dist loss 0.02312902733683586\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.059742845594882965 class loss 0.00018855695088859648 dist loss 0.03538193181157112 older dist loss 0.024172358214855194\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0592525377869606 class loss 0.00018962909234687686 dist loss 0.03483754023909569 older dist loss 0.024225367233157158\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05762563645839691 class loss 0.00012678245548158884 dist loss 0.033854078501462936 older dist loss 0.02364477328956127\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05393466353416443 class loss 0.00011088811879744753 dist loss 0.03215095028281212 older dist loss 0.021672822535037994\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.056331660598516464 class loss 0.0001446819369448349 dist loss 0.033327918499708176 older dist loss 0.022859059274196625\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05497467517852783 class loss 0.00016323318413924426 dist loss 0.03265783563256264 older dist loss 0.02215360850095749\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05982279032468796 class loss 0.00013435438449960202 dist loss 0.035120829939842224 older dist loss 0.024567607790231705\n",
            "Reducing each exemplar set to size: 40\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 40.40\n",
            "\n",
            "Test Accuracy (all groups seen so far): 41.14\n",
            "\n",
            "the model knows 50 classes:\n",
            " \n",
            "GROUP:  6\n",
            "ALL DB:  6750\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.08679820597171783 class loss 0.018381159752607346 dist loss 0.04153771325945854 older dist loss 0.026879334822297096\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.07753447443246841 class loss 0.016375619918107986 dist loss 0.03584715723991394 older dist loss 0.025311697274446487\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.07603338360786438 class loss 0.013041479513049126 dist loss 0.037913139909505844 older dist loss 0.02507876232266426\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.07159911841154099 class loss 0.009690678678452969 dist loss 0.03804110363125801 older dist loss 0.02386733703315258\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.06961040198802948 class loss 0.009174909442663193 dist loss 0.03620285913348198 older dist loss 0.024232638999819756\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.07092337310314178 class loss 0.007263183128088713 dist loss 0.0385904461145401 older dist loss 0.025069743394851685\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.06916508078575134 class loss 0.006455048453062773 dist loss 0.037821076810359955 older dist loss 0.02488895319402218\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.06876575201749802 class loss 0.0050857216119766235 dist loss 0.0378091037273407 older dist loss 0.025870924815535545\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.06809239089488983 class loss 0.0047449758276343346 dist loss 0.038348253816366196 older dist loss 0.024999160319566727\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.06567864120006561 class loss 0.004445796366780996 dist loss 0.037608712911605835 older dist loss 0.023624127730727196\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.06450040638446808 class loss 0.002195439301431179 dist loss 0.037332747131586075 older dist loss 0.02497221529483795\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.06202154979109764 class loss 0.0024997354485094547 dist loss 0.03597719222307205 older dist loss 0.02354462258517742\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.06254091113805771 class loss 0.0017748181708157063 dist loss 0.03716537728905678 older dist loss 0.023600716143846512\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.06392895430326462 class loss 0.0007263144361786544 dist loss 0.038000546395778656 older dist loss 0.02520209550857544\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.057099416851997375 class loss 0.0007340112933889031 dist loss 0.03451487794518471 older dist loss 0.021850528195500374\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.06181951239705086 class loss 0.0007729764911346138 dist loss 0.03673848137259483 older dist loss 0.02430805377662182\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.0593460388481617 class loss 0.0007990716258063912 dist loss 0.03537631034851074 older dist loss 0.023170657455921173\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.06079966947436333 class loss 0.0011761683272197843 dist loss 0.03646710515022278 older dist loss 0.02315639518201351\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.05943932384252548 class loss 0.0008588744094595313 dist loss 0.03551677614450455 older dist loss 0.023063672706484795\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.06276005506515503 class loss 0.0009286194108426571 dist loss 0.03750154748558998 older dist loss 0.024329887703061104\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.060791656374931335 class loss 0.00075003900565207 dist loss 0.036256324499845505 older dist loss 0.023785294964909554\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.05849847197532654 class loss 0.0007440386689268053 dist loss 0.035298485308885574 older dist loss 0.02245594933629036\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.06144873797893524 class loss 0.0005267893429845572 dist loss 0.037189241498708725 older dist loss 0.023732705041766167\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.058598592877388 class loss 0.0006145057850517333 dist loss 0.03532080352306366 older dist loss 0.022663285955786705\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.06267782300710678 class loss 0.000608602655120194 dist loss 0.03749231994152069 older dist loss 0.024576900526881218\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.057795021682977676 class loss 0.0004725445178337395 dist loss 0.034934528172016144 older dist loss 0.022387949749827385\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.06266550719738007 class loss 0.00073600112227723 dist loss 0.03723519295454025 older dist loss 0.02469431422650814\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.062080468982458115 class loss 0.0009468734497204423 dist loss 0.0372445210814476 older dist loss 0.023889074102044106\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.06058691442012787 class loss 0.00045580562436953187 dist loss 0.03615937381982803 older dist loss 0.023971732705831528\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.05896691977977753 class loss 0.00045332207810133696 dist loss 0.03621841222047806 older dist loss 0.022295184433460236\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.058204229921102524 class loss 0.0004946512635797262 dist loss 0.03470180928707123 older dist loss 0.023007769137620926\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.056467924267053604 class loss 0.0005890032043680549 dist loss 0.03413110226392746 older dist loss 0.02174781821668148\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.05953149497509003 class loss 0.0002554894599597901 dist loss 0.03561492636799812 older dist loss 0.02366107888519764\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.06056884676218033 class loss 0.0003546660300344229 dist loss 0.03657619655132294 older dist loss 0.023637985810637474\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.057842399924993515 class loss 0.00030977249843999743 dist loss 0.03526649251580238 older dist loss 0.02226613461971283\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.061741337180137634 class loss 0.0008573506493121386 dist loss 0.03656294196844101 older dist loss 0.024321042001247406\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.05934571474790573 class loss 0.0006160478806123137 dist loss 0.035441745072603226 older dist loss 0.023287922143936157\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.0571318194270134 class loss 0.00024412925995420665 dist loss 0.0340292789041996 older dist loss 0.022858409211039543\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.05574896186590195 class loss 0.00046775778173469007 dist loss 0.03295392915606499 older dist loss 0.022327272221446037\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.058771565556526184 class loss 0.0003623676602728665 dist loss 0.035158317536115646 older dist loss 0.0232508797198534\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.06193910539150238 class loss 0.0004532610473688692 dist loss 0.03656860813498497 older dist loss 0.0249172355979681\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.06102915480732918 class loss 0.0005524181760847569 dist loss 0.03615209832787514 older dist loss 0.02432463876903057\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.06028169393539429 class loss 0.0006336136721074581 dist loss 0.03592316433787346 older dist loss 0.023724917322397232\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.06143341213464737 class loss 0.0007250780472531915 dist loss 0.03670346736907959 older dist loss 0.024004867300391197\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.062124498188495636 class loss 0.002548031974583864 dist loss 0.03569543734192848 older dist loss 0.023881031200289726\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.06659220159053802 class loss 0.0015753833577036858 dist loss 0.038788218051195145 older dist loss 0.02622859552502632\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.0610712468624115 class loss 0.0019760432187467813 dist loss 0.03648155927658081 older dist loss 0.02261364459991455\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.06372996419668198 class loss 0.0017986976308748126 dist loss 0.037563830614089966 older dist loss 0.02436743676662445\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.06155764311552048 class loss 0.0013629819732159376 dist loss 0.03627372905611992 older dist loss 0.023920932784676552\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06152694672346115 class loss 0.00039535172982141376 dist loss 0.03741135820746422 older dist loss 0.023720234632492065\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.06206434965133667 class loss 0.0001744099281495437 dist loss 0.037864524871110916 older dist loss 0.024025416001677513\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.06006873771548271 class loss 0.00022166005510371178 dist loss 0.03642125055193901 older dist loss 0.02342582680284977\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.05630633980035782 class loss 0.00020966658485122025 dist loss 0.034373749047517776 older dist loss 0.021722925826907158\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.05859818309545517 class loss 0.00015973309928085655 dist loss 0.035570550709962845 older dist loss 0.022867901250720024\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.056416939944028854 class loss 0.00029937250656075776 dist loss 0.034082669764757156 older dist loss 0.0220348984003067\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.058251574635505676 class loss 0.0001756195561029017 dist loss 0.03547528013586998 older dist loss 0.022600676864385605\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.057572558522224426 class loss 0.00018587631348054856 dist loss 0.035064369440078735 older dist loss 0.022322311997413635\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.056949954479932785 class loss 0.00017259482410736382 dist loss 0.03426678106188774 older dist loss 0.022510578855872154\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.05816005542874336 class loss 0.00012161666381871328 dist loss 0.03546490520238876 older dist loss 0.02257353439927101\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.05495711416006088 class loss 0.00013439606118481606 dist loss 0.033392079174518585 older dist loss 0.021430637687444687\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.0592757873237133 class loss 0.00019017845625057817 dist loss 0.03551338240504265 older dist loss 0.023572226986289024\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.05643787607550621 class loss 0.0001087930504581891 dist loss 0.03423679992556572 older dist loss 0.02209228277206421\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.05504442751407623 class loss 0.00016262954159174114 dist loss 0.033495597541332245 older dist loss 0.021386200562119484\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.05880139768123627 class loss 0.00017698670853860676 dist loss 0.035699550062417984 older dist loss 0.022924859076738358\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.057594116777181625 class loss 0.00011993486987194046 dist loss 0.034521110355854034 older dist loss 0.02295307070016861\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05902548134326935 class loss 0.00013025743828620762 dist loss 0.036447905004024506 older dist loss 0.022447317838668823\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.06013660132884979 class loss 0.00019488940597511828 dist loss 0.036616258323192596 older dist loss 0.023325452581048012\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.0571746751666069 class loss 0.00014518469106405973 dist loss 0.03508227318525314 older dist loss 0.021947216242551804\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05586125701665878 class loss 0.0001099958608392626 dist loss 0.03392484784126282 older dist loss 0.02182641252875328\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05171003192663193 class loss 0.00013658883108291775 dist loss 0.031074201688170433 older dist loss 0.02049923874437809\n",
            "Reducing each exemplar set to size: 34\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 37.48\n",
            "\n",
            "Test Accuracy (all groups seen so far): 36.68\n",
            "\n",
            "the model knows 60 classes:\n",
            " \n",
            "GROUP:  7\n",
            "ALL DB:  6380\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.0725131630897522 class loss 0.013852465897798538 dist loss 0.03510881960391998 older dist loss 0.023551877588033676\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.06845451891422272 class loss 0.00962147768586874 dist loss 0.036225151270627975 older dist loss 0.022607892751693726\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.06913299858570099 class loss 0.010970783419907093 dist loss 0.035455599427223206 older dist loss 0.022706614807248116\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.0672130286693573 class loss 0.00902549922466278 dist loss 0.03557903692126274 older dist loss 0.02260848879814148\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.06544846296310425 class loss 0.006750885862857103 dist loss 0.03576718643307686 older dist loss 0.022930387407541275\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.07025522738695145 class loss 0.007424310781061649 dist loss 0.038682721555233 older dist loss 0.02414819598197937\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.06143852695822716 class loss 0.0034594133030623198 dist loss 0.035435475409030914 older dist loss 0.022543638944625854\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.06258419156074524 class loss 0.0026403970550745726 dist loss 0.036726076155900955 older dist loss 0.02321772091090679\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.06298960000276566 class loss 0.002715335227549076 dist loss 0.036975469440221786 older dist loss 0.023298796266317368\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.06270452588796616 class loss 0.002109475899487734 dist loss 0.037236638367176056 older dist loss 0.023358413949608803\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.06098148971796036 class loss 0.0021990817040205 dist loss 0.03610638901591301 older dist loss 0.022676020860671997\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.05895412713289261 class loss 0.0010255856905132532 dist loss 0.03525803983211517 older dist loss 0.022670499980449677\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.05970358848571777 class loss 0.0009397665853612125 dist loss 0.03620848059654236 older dist loss 0.022555341944098473\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.05637199431657791 class loss 0.0006182566285133362 dist loss 0.03433616831898689 older dist loss 0.021417569369077682\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.057102810591459274 class loss 0.0009387840400449932 dist loss 0.034543052315711975 older dist loss 0.021620973944664\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.06011158972978592 class loss 0.0008444131817668676 dist loss 0.03645357862114906 older dist loss 0.022813597694039345\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.0607161708176136 class loss 0.0005851921741850674 dist loss 0.037111349403858185 older dist loss 0.02301962859928608\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.05933430790901184 class loss 0.0005985101452097297 dist loss 0.036284297704696655 older dist loss 0.022451499477028847\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.05969180166721344 class loss 0.0009604788501746953 dist loss 0.03592445328831673 older dist loss 0.022806869819760323\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.05964390188455582 class loss 0.0005809074500575662 dist loss 0.03649529069662094 older dist loss 0.02256770245730877\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.05432383716106415 class loss 0.000335527874995023 dist loss 0.033116891980171204 older dist loss 0.020871417596936226\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.05907768756151199 class loss 0.00047509619616903365 dist loss 0.036616045981645584 older dist loss 0.021986544132232666\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.05971478298306465 class loss 0.0004213179345242679 dist loss 0.036038074642419815 older dist loss 0.02325539104640484\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.0602257065474987 class loss 0.00047460116911679506 dist loss 0.03634517639875412 older dist loss 0.023405928164720535\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.05753151327371597 class loss 0.00036485734744928777 dist loss 0.0351024828851223 older dist loss 0.022064175456762314\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.05665817856788635 class loss 0.0004157516814302653 dist loss 0.03487768396735191 older dist loss 0.021364741027355194\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.05658328905701637 class loss 0.0002696756273508072 dist loss 0.03483249619603157 older dist loss 0.021481117233633995\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.05741863697767258 class loss 0.0006592523423023522 dist loss 0.034676261246204376 older dist loss 0.02208312414586544\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.05962322652339935 class loss 0.0004691709764301777 dist loss 0.03663763403892517 older dist loss 0.022516421973705292\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.05994901806116104 class loss 0.000337551609845832 dist loss 0.03657018393278122 older dist loss 0.02304128371179104\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.056913942098617554 class loss 0.0003463211178313941 dist loss 0.03491271659731865 older dist loss 0.021654905751347542\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.052991390228271484 class loss 0.00029816219466738403 dist loss 0.03284876048564911 older dist loss 0.01984446868300438\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.05723046511411667 class loss 0.0003620183852035552 dist loss 0.03542414307594299 older dist loss 0.021444305777549744\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.056189075112342834 class loss 0.0004269614873919636 dist loss 0.03449847549200058 older dist loss 0.021263640373945236\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.060104284435510635 class loss 0.00047329431981779635 dist loss 0.03646445646882057 older dist loss 0.023166533559560776\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.058250799775123596 class loss 0.0003903588221874088 dist loss 0.03546266257762909 older dist loss 0.022397780790925026\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.05920989066362381 class loss 0.00047659093979746103 dist loss 0.03636940196156502 older dist loss 0.022363895550370216\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.05927390605211258 class loss 0.0004890425479970872 dist loss 0.03588345646858215 older dist loss 0.02290140837430954\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.0581367202103138 class loss 0.00043533608550205827 dist loss 0.035916589200496674 older dist loss 0.021784795448184013\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.056724756956100464 class loss 0.00020278996089473367 dist loss 0.03453671932220459 older dist loss 0.02198524959385395\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.055552802979946136 class loss 0.0002550853241700679 dist loss 0.03430022671818733 older dist loss 0.020997492596507072\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.05526428669691086 class loss 0.0002284811926074326 dist loss 0.033850234001874924 older dist loss 0.02118556946516037\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.05791532248258591 class loss 0.0003190423594787717 dist loss 0.03545309603214264 older dist loss 0.022143185138702393\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.05925443023443222 class loss 0.00026774132857099175 dist loss 0.036119986325502396 older dist loss 0.02286670170724392\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.05855485051870346 class loss 0.00023999654513318092 dist loss 0.03578942269086838 older dist loss 0.02252543345093727\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.05855152755975723 class loss 0.0005825251573696733 dist loss 0.03598115220665932 older dist loss 0.021987849846482277\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.05684664845466614 class loss 0.00024238538753706962 dist loss 0.03454212099313736 older dist loss 0.022062139585614204\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.05770353227853775 class loss 0.00045635426067747176 dist loss 0.03530561551451683 older dist loss 0.02194156125187874\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.06087525188922882 class loss 0.00039112698868848383 dist loss 0.03735676780343056 older dist loss 0.023127356544137\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05841251090168953 class loss 0.0002382121456321329 dist loss 0.03636028617620468 older dist loss 0.02181401289999485\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.05990184471011162 class loss 0.00017373569426126778 dist loss 0.036740534007549286 older dist loss 0.022987574338912964\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.056765682995319366 class loss 0.00018834667571354657 dist loss 0.03489204868674278 older dist loss 0.021685287356376648\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.05688082426786423 class loss 0.00017009077419061214 dist loss 0.03493313491344452 older dist loss 0.021777600049972534\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.05715213716030121 class loss 0.00011439132504165173 dist loss 0.035534314811229706 older dist loss 0.02150343358516693\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.05431121215224266 class loss 0.00012141889601480216 dist loss 0.03337610512971878 older dist loss 0.02081368863582611\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.05529391020536423 class loss 9.671018779044971e-05 dist loss 0.03429035097360611 older dist loss 0.020906848832964897\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.05479556322097778 class loss 0.00011532215285114944 dist loss 0.033738791942596436 older dist loss 0.02094144932925701\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.058497264981269836 class loss 0.0001595436770003289 dist loss 0.0363919660449028 older dist loss 0.021945757791399956\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.053987324237823486 class loss 0.0001135638594860211 dist loss 0.033499691635370255 older dist loss 0.02037406899034977\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.05659905821084976 class loss 0.0001084772011381574 dist loss 0.03474372252821922 older dist loss 0.021746858954429626\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.05296602100133896 class loss 0.00012112758122384548 dist loss 0.03264990076422691 older dist loss 0.020194990560412407\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.05647261068224907 class loss 0.00018353188352193683 dist loss 0.034677088260650635 older dist loss 0.02161199040710926\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.0582159049808979 class loss 0.00019536812033038586 dist loss 0.035679422318935394 older dist loss 0.022341115400195122\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.05575717240571976 class loss 0.00012925302144140005 dist loss 0.034389056265354156 older dist loss 0.021238865330815315\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05439532548189163 class loss 0.00015542625624220818 dist loss 0.033410944044589996 older dist loss 0.020828954875469208\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05850439518690109 class loss 0.00013301748549565673 dist loss 0.035971030592918396 older dist loss 0.022400349378585815\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05650975927710533 class loss 0.0001538266078568995 dist loss 0.03500521555542946 older dist loss 0.021350717172026634\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.057175688445568085 class loss 0.00014225392078515142 dist loss 0.03525679558515549 older dist loss 0.021776637062430382\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05727790296077728 class loss 0.0001537291973363608 dist loss 0.03549225628376007 older dist loss 0.021631915122270584\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05550163984298706 class loss 0.00016445075743831694 dist loss 0.03399726003408432 older dist loss 0.021339930593967438\n",
            "Reducing each exemplar set to size: 29\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 41.94\n",
            "\n",
            "Test Accuracy (all groups seen so far): 36.00\n",
            "\n",
            "the model knows 70 classes:\n",
            " \n",
            "GROUP:  8\n",
            "ALL DB:  5910\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.06910037994384766 class loss 0.012365455739200115 dist loss 0.03472281992435455 older dist loss 0.02201210893690586\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.06847716867923737 class loss 0.01060484815388918 dist loss 0.035947684198617935 older dist loss 0.021924633532762527\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.06917552649974823 class loss 0.0122153265401721 dist loss 0.03530709818005562 older dist loss 0.021653098985552788\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.06374667584896088 class loss 0.008055325597524643 dist loss 0.03398929908871651 older dist loss 0.021702053025364876\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.06471119076013565 class loss 0.008468194864690304 dist loss 0.035126976668834686 older dist loss 0.021116020157933235\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.06287925690412521 class loss 0.006128254812210798 dist loss 0.03503202646970749 older dist loss 0.02171897515654564\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.05945699289441109 class loss 0.0046292939223349094 dist loss 0.03384719043970108 older dist loss 0.020980508998036385\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.0598117932677269 class loss 0.003789951791986823 dist loss 0.03461672365665436 older dist loss 0.021405117586255074\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.05586172267794609 class loss 0.0019990764558315277 dist loss 0.03343621641397476 older dist loss 0.0204264298081398\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.056686289608478546 class loss 0.0019180059898644686 dist loss 0.034502048045396805 older dist loss 0.02026623673737049\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.05670308694243431 class loss 0.001477161655202508 dist loss 0.03405875712633133 older dist loss 0.021167168393731117\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.0572718009352684 class loss 0.0011892685433849692 dist loss 0.03452537581324577 older dist loss 0.0215571578592062\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.05557993799448013 class loss 0.0009112430852837861 dist loss 0.03434549644589424 older dist loss 0.020323198288679123\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.05670362338423729 class loss 0.0009019518620334566 dist loss 0.03465557098388672 older dist loss 0.021146100014448166\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.05399421602487564 class loss 0.0007214539800770581 dist loss 0.03312456235289574 older dist loss 0.020148200914263725\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.052938297390937805 class loss 0.00048603929462842643 dist loss 0.032929372042417526 older dist loss 0.019522884860634804\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.056121475994586945 class loss 0.0008402485400438309 dist loss 0.03406939655542374 older dist loss 0.021211829036474228\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.05749096721410751 class loss 0.0006088998052291572 dist loss 0.03464915230870247 older dist loss 0.022232914343476295\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.05480746179819107 class loss 0.00047159745008684695 dist loss 0.033961646258831024 older dist loss 0.02037421613931656\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.05539592355489731 class loss 0.0006027756026014686 dist loss 0.03409915417432785 older dist loss 0.02069399505853653\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.05670952796936035 class loss 0.00036594661651179194 dist loss 0.03474176675081253 older dist loss 0.02160181663930416\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.05436825007200241 class loss 0.0004368963709566742 dist loss 0.03339897096157074 older dist loss 0.020532382652163506\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.05670952424407005 class loss 0.00046374546946026385 dist loss 0.03515686094760895 older dist loss 0.021088918671011925\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.054128482937812805 class loss 0.0003053141408599913 dist loss 0.03370402753353119 older dist loss 0.020119139924645424\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.05349259823560715 class loss 0.0004259349952917546 dist loss 0.03269510716199875 older dist loss 0.020371558144688606\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.05525345355272293 class loss 0.0002905291912611574 dist loss 0.03377900645136833 older dist loss 0.021183917298913002\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.05277533456683159 class loss 0.0003645916876848787 dist loss 0.03238201141357422 older dist loss 0.02002873085439205\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.05418711155653 class loss 0.00029336108127608895 dist loss 0.033869579434394836 older dist loss 0.020024172961711884\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.054851189255714417 class loss 0.0002832369937095791 dist loss 0.03414556756615639 older dist loss 0.02042238600552082\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.05348106473684311 class loss 0.0003412496007513255 dist loss 0.03291507437825203 older dist loss 0.020224738866090775\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.054421305656433105 class loss 0.00030427216552197933 dist loss 0.0335896834731102 older dist loss 0.020527351647615433\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.05135440081357956 class loss 0.0002564548049122095 dist loss 0.03168841451406479 older dist loss 0.019409533590078354\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.052545271813869476 class loss 0.00026084380806423724 dist loss 0.032541267573833466 older dist loss 0.019743161275982857\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.05172393471002579 class loss 0.0002749148989096284 dist loss 0.03173850476741791 older dist loss 0.019710514694452286\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.053655337542295456 class loss 0.0001973114995053038 dist loss 0.03323822095990181 older dist loss 0.02021980471909046\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.05701213330030441 class loss 0.00043277107761241496 dist loss 0.034774575382471085 older dist loss 0.02180478535592556\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.053758516907691956 class loss 0.0002528507902752608 dist loss 0.0333748459815979 older dist loss 0.02013082057237625\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.055569253861904144 class loss 0.0002856560458894819 dist loss 0.034176312386989594 older dist loss 0.02110728807747364\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.05389068275690079 class loss 0.00038531451718881726 dist loss 0.03322198614478111 older dist loss 0.020283380523324013\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.05460287258028984 class loss 0.00022775016259402037 dist loss 0.033576030284166336 older dist loss 0.020799092948436737\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.054898835718631744 class loss 0.00048327838885597885 dist loss 0.03382803872227669 older dist loss 0.02058752067387104\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.05326789617538452 class loss 0.0002148283238057047 dist loss 0.03274480253458023 older dist loss 0.02030826546251774\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.05463479831814766 class loss 0.00034280799445696175 dist loss 0.033762749284505844 older dist loss 0.020529240369796753\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.05431970953941345 class loss 0.00016975219477899373 dist loss 0.033419907093048096 older dist loss 0.020730052143335342\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.05258888751268387 class loss 0.0002922691055573523 dist loss 0.03251723200082779 older dist loss 0.0197793859988451\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.05550166964530945 class loss 0.00028062579804100096 dist loss 0.0342683307826519 older dist loss 0.02095271460711956\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.054549362510442734 class loss 0.0003176129830535501 dist loss 0.033615611493587494 older dist loss 0.020616138353943825\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.05187421292066574 class loss 0.00032463789102621377 dist loss 0.03237806260585785 older dist loss 0.019171513617038727\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.05709826201200485 class loss 0.0002983271551784128 dist loss 0.035187795758247375 older dist loss 0.02161214128136635\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.052324600517749786 class loss 0.0001152719632955268 dist loss 0.032362885773181915 older dist loss 0.019846444949507713\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.05242019146680832 class loss 0.0001064829557435587 dist loss 0.0327824130654335 older dist loss 0.01953129470348358\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.05290856212377548 class loss 0.00010739140270743519 dist loss 0.03276069834828377 older dist loss 0.020040472969412804\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.05422859638929367 class loss 0.00011213759717065841 dist loss 0.03349833935499191 older dist loss 0.020618120208382607\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.05477184057235718 class loss 0.00012297299690544605 dist loss 0.033749088644981384 older dist loss 0.020899780094623566\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.05262456834316254 class loss 0.00011801079381257296 dist loss 0.03262598067522049 older dist loss 0.01988057792186737\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.05377130210399628 class loss 9.263050014851615e-05 dist loss 0.033393744379282 older dist loss 0.020284928381443024\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.05536732077598572 class loss 0.00011872174218297005 dist loss 0.034310854971408844 older dist loss 0.02093774452805519\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.05477827787399292 class loss 0.00010815251152962446 dist loss 0.03417252376675606 older dist loss 0.020497601479291916\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.05223160609602928 class loss 0.00010832592670340091 dist loss 0.03256654366850853 older dist loss 0.019556736573576927\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.05265690013766289 class loss 9.263822721550241e-05 dist loss 0.032831821590662 older dist loss 0.019732439890503883\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.051961496472358704 class loss 0.00013314995157998055 dist loss 0.0320664718747139 older dist loss 0.01976187713444233\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.0529472678899765 class loss 0.00011575465759960935 dist loss 0.03271888941526413 older dist loss 0.020112626254558563\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.05350228399038315 class loss 0.00010828524682438001 dist loss 0.03315996378660202 older dist loss 0.020234037190675735\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.0543631836771965 class loss 8.030846220208332e-05 dist loss 0.03375333175063133 older dist loss 0.020529545843601227\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05300699174404144 class loss 0.00011052416084567085 dist loss 0.03303082287311554 older dist loss 0.019865646958351135\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05330153927206993 class loss 0.00013073765148874372 dist loss 0.033067818731069565 older dist loss 0.020102983340620995\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.051914289593696594 class loss 0.00010251824278384447 dist loss 0.0322587713599205 older dist loss 0.01955299824476242\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05535012483596802 class loss 0.00010916889004874974 dist loss 0.034425631165504456 older dist loss 0.020815325900912285\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05214838683605194 class loss 0.00011010541493305936 dist loss 0.03222997859120369 older dist loss 0.019808301702141762\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.052244409918785095 class loss 0.00011465998977655545 dist loss 0.0320759117603302 older dist loss 0.020053837448358536\n",
            "Reducing each exemplar set to size: 25\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 28.50\n",
            "\n",
            "Test Accuracy (all groups seen so far): 32.61\n",
            "\n",
            "the model knows 80 classes:\n",
            " \n",
            "GROUP:  9\n",
            "ALL DB:  5400\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.06328725814819336 class loss 0.009573337621986866 dist loss 0.03342711180448532 older dist loss 0.0202868040651083\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.06116408109664917 class loss 0.007408292964100838 dist loss 0.03361177444458008 older dist loss 0.020144015550613403\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.06403565406799316 class loss 0.009461157955229282 dist loss 0.034062568098306656 older dist loss 0.02051192708313465\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.05870278924703598 class loss 0.006154174450784922 dist loss 0.032681580632925034 older dist loss 0.019867034628987312\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.053777944296598434 class loss 0.003593281377106905 dist loss 0.03117632307112217 older dist loss 0.019008340314030647\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.05575965717434883 class loss 0.0034828484058380127 dist loss 0.032838791608810425 older dist loss 0.019438017159700394\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.05398646369576454 class loss 0.0030742110684514046 dist loss 0.03149905800819397 older dist loss 0.019413193687796593\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.05571666359901428 class loss 0.001781272585503757 dist loss 0.03392060473561287 older dist loss 0.02001478523015976\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.054848577827215195 class loss 0.0014842081582173705 dist loss 0.03339175507426262 older dist loss 0.01997261494398117\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.05473782867193222 class loss 0.0015233164886012673 dist loss 0.03334978222846985 older dist loss 0.019864730536937714\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.05405653268098831 class loss 0.001001816475763917 dist loss 0.03308740258216858 older dist loss 0.01996731199324131\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.05302359163761139 class loss 0.0010588106233626604 dist loss 0.032457079738378525 older dist loss 0.019507702440023422\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.05393729731440544 class loss 0.0005009117303416133 dist loss 0.033230021595954895 older dist loss 0.02020636387169361\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.05506964027881622 class loss 0.0010690506314858794 dist loss 0.03391091898083687 older dist loss 0.02008966915309429\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.05314590036869049 class loss 0.0005768280243501067 dist loss 0.032785724848508835 older dist loss 0.019783349707722664\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.05289853364229202 class loss 0.0006265300908125937 dist loss 0.03229273483157158 older dist loss 0.019979266449809074\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.0509897917509079 class loss 0.00036838758387602866 dist loss 0.03147586062550545 older dist loss 0.01914554089307785\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.04941687732934952 class loss 0.00026157329557463527 dist loss 0.030383901670575142 older dist loss 0.018771402537822723\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.052521489560604095 class loss 0.0004552678728941828 dist loss 0.03249116614460945 older dist loss 0.019575053825974464\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.055200837552547455 class loss 0.0004679013800341636 dist loss 0.033940188586711884 older dist loss 0.020792746916413307\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.052565798163414 class loss 0.0003406040195841342 dist loss 0.032549139112234116 older dist loss 0.019676055759191513\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.05161486566066742 class loss 0.00033866215380840003 dist loss 0.031913626939058304 older dist loss 0.01936257816851139\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.051079511642456055 class loss 0.0003187165129929781 dist loss 0.03182634711265564 older dist loss 0.018934447318315506\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.05092005431652069 class loss 0.00034843114553950727 dist loss 0.031703200191259384 older dist loss 0.018868420273065567\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.05136542022228241 class loss 0.0003700371889863163 dist loss 0.031938232481479645 older dist loss 0.019057150930166245\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.05242720991373062 class loss 0.0003503455372992903 dist loss 0.032539475709199905 older dist loss 0.019537391141057014\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.05254538729786873 class loss 0.00023926461290102452 dist loss 0.032691437751054764 older dist loss 0.01961468532681465\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.05068283900618553 class loss 0.00023266191419679672 dist loss 0.03133653849363327 older dist loss 0.019113639369606972\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.052612386643886566 class loss 0.00034193822648376226 dist loss 0.03296961262822151 older dist loss 0.019300835207104683\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.050379909574985504 class loss 0.00020963825227227062 dist loss 0.03128831461071968 older dist loss 0.01888195425271988\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.05178581178188324 class loss 0.00021871787612326443 dist loss 0.03192095458507538 older dist loss 0.019646141678094864\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.05135580897331238 class loss 0.0001720903383102268 dist loss 0.03194550424814224 older dist loss 0.019238214939832687\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.05230624973773956 class loss 0.00021109925000928342 dist loss 0.032660264521837234 older dist loss 0.019434884190559387\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.052643559873104095 class loss 0.0002807052223943174 dist loss 0.03290766477584839 older dist loss 0.019455192610621452\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.05012527108192444 class loss 0.00019455948495306075 dist loss 0.031094124540686607 older dist loss 0.01883658953011036\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.05260968953371048 class loss 0.00028346796170808375 dist loss 0.03241116926074028 older dist loss 0.019915049895644188\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.04995673522353172 class loss 0.00020182080334052444 dist loss 0.03115491382777691 older dist loss 0.01860000006854534\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.05210535228252411 class loss 0.00020023083197884262 dist loss 0.03241683915257454 older dist loss 0.01948828063905239\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.05302942544221878 class loss 0.00020624519675038755 dist loss 0.03295542672276497 older dist loss 0.019867753610014915\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.05198434367775917 class loss 0.00018339451344218105 dist loss 0.032399121671915054 older dist loss 0.019401827827095985\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.05179591476917267 class loss 0.00023159320699051023 dist loss 0.032017309218645096 older dist loss 0.01954701356589794\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.054672617465257645 class loss 0.00028187595307826996 dist loss 0.034336961805820465 older dist loss 0.02005377970635891\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.05194854736328125 class loss 0.00014518517127726227 dist loss 0.03226931765675545 older dist loss 0.019534042105078697\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.05244668573141098 class loss 0.00021274990285746753 dist loss 0.032722823321819305 older dist loss 0.019511112943291664\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.052702657878398895 class loss 0.00033028743928298354 dist loss 0.03278746083378792 older dist loss 0.019584907218813896\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.05180330574512482 class loss 0.00021845933224540204 dist loss 0.032265592366456985 older dist loss 0.019319256767630577\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.054438747465610504 class loss 0.00025939970510080457 dist loss 0.033863916993141174 older dist loss 0.02031543292105198\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.05033888667821884 class loss 0.00018332619220018387 dist loss 0.031656909734010696 older dist loss 0.018498649820685387\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.053247589617967606 class loss 0.000265442329691723 dist loss 0.03302481025457382 older dist loss 0.01995733752846718\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.051278892904520035 class loss 0.00013650891196448356 dist loss 0.03225645050406456 older dist loss 0.018885932862758636\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.05375289171934128 class loss 0.00020020997908432037 dist loss 0.033613719046115875 older dist loss 0.019938962534070015\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.051890965551137924 class loss 0.00011815564357675612 dist loss 0.032217592000961304 older dist loss 0.019555218517780304\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.05217115581035614 class loss 9.165049414150417e-05 dist loss 0.032549891620874405 older dist loss 0.019529612734913826\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.05216610059142113 class loss 8.678908488946036e-05 dist loss 0.03255113959312439 older dist loss 0.01952817104756832\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.053476009517908096 class loss 0.00012878896086476743 dist loss 0.03335979953408241 older dist loss 0.019987421110272408\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.05075329542160034 class loss 9.178418258670717e-05 dist loss 0.031597234308719635 older dist loss 0.01906427927315235\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.05303409323096275 class loss 0.0001192440468003042 dist loss 0.033124230802059174 older dist loss 0.019790617749094963\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.04986879974603653 class loss 0.00012026882177451625 dist loss 0.031205661594867706 older dist loss 0.01854286901652813\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.048674099147319794 class loss 7.436727901222184e-05 dist loss 0.030314156785607338 older dist loss 0.01828557252883911\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.05282902717590332 class loss 0.0001471825671615079 dist loss 0.03284667432308197 older dist loss 0.019835172221064568\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.05122809857130051 class loss 0.00014080510300118476 dist loss 0.03212309628725052 older dist loss 0.018964195623993874\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.05100706219673157 class loss 9.093416156247258e-05 dist loss 0.032120756804943085 older dist loss 0.018795371055603027\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.048905402421951294 class loss 8.473216439597309e-05 dist loss 0.030648427084088326 older dist loss 0.018172243610024452\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.05008772760629654 class loss 7.122512761270627e-05 dist loss 0.0314268060028553 older dist loss 0.018589694052934647\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.04926644265651703 class loss 9.054741531144828e-05 dist loss 0.03084653988480568 older dist loss 0.018329354003071785\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05175952613353729 class loss 0.00010242040298180655 dist loss 0.03218264877796173 older dist loss 0.019474457949399948\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05168920010328293 class loss 0.00010966193076455966 dist loss 0.03237071633338928 older dist loss 0.019208820536732674\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05010785534977913 class loss 7.721775909885764e-05 dist loss 0.03151246905326843 older dist loss 0.018518168479204178\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05154373124241829 class loss 8.391003211727366e-05 dist loss 0.03214920684695244 older dist loss 0.019310614094138145\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.053241170942783356 class loss 0.0001404501817887649 dist loss 0.03366360068321228 older dist loss 0.019437119364738464\n",
            "Reducing each exemplar set to size: 23\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 29.34\n",
            "\n",
            "Test Accuracy (all groups seen so far): 30.84\n",
            "\n",
            "the model knows 90 classes:\n",
            " \n",
            "GROUP:  10\n",
            "ALL DB:  5090\n",
            "NUM_EPOCHS:  0 / 70  LR:  [2]\n",
            "LOSS:  0.06925719976425171 class loss 0.011973551474511623 dist loss 0.0358528271317482 older dist loss 0.02143082581460476\n",
            "NUM_EPOCHS:  1 / 70  LR:  [2]\n",
            "LOSS:  0.05787796527147293 class loss 0.005387517623603344 dist loss 0.03296590596437454 older dist loss 0.01952454075217247\n",
            "NUM_EPOCHS:  2 / 70  LR:  [2]\n",
            "LOSS:  0.05383840948343277 class loss 0.004296251572668552 dist loss 0.031159525737166405 older dist loss 0.01838262937963009\n",
            "NUM_EPOCHS:  3 / 70  LR:  [2]\n",
            "LOSS:  0.053180500864982605 class loss 0.004478353541344404 dist loss 0.030326934531331062 older dist loss 0.01837521232664585\n",
            "NUM_EPOCHS:  4 / 70  LR:  [2]\n",
            "LOSS:  0.056747399270534515 class loss 0.004316052887588739 dist loss 0.032789748162031174 older dist loss 0.01964160054922104\n",
            "NUM_EPOCHS:  5 / 70  LR:  [2]\n",
            "LOSS:  0.05189526081085205 class loss 0.003057992784306407 dist loss 0.03086104989051819 older dist loss 0.017976218834519386\n",
            "NUM_EPOCHS:  6 / 70  LR:  [2]\n",
            "LOSS:  0.05326473340392113 class loss 0.0024433068465441465 dist loss 0.03197219967842102 older dist loss 0.01884922757744789\n",
            "NUM_EPOCHS:  7 / 70  LR:  [2]\n",
            "LOSS:  0.052952371537685394 class loss 0.0021266876719892025 dist loss 0.03190264850854874 older dist loss 0.01892303302884102\n",
            "NUM_EPOCHS:  8 / 70  LR:  [2]\n",
            "LOSS:  0.051845986396074295 class loss 0.001655510626733303 dist loss 0.031556688249111176 older dist loss 0.018633786588907242\n",
            "NUM_EPOCHS:  9 / 70  LR:  [2]\n",
            "LOSS:  0.050449028611183167 class loss 0.0012989792739972472 dist loss 0.030854100361466408 older dist loss 0.018295947462320328\n",
            "NUM_EPOCHS:  10 / 70  LR:  [2]\n",
            "LOSS:  0.051852934062480927 class loss 0.000993184163235128 dist loss 0.03212893009185791 older dist loss 0.01873081736266613\n",
            "NUM_EPOCHS:  11 / 70  LR:  [2]\n",
            "LOSS:  0.0498514249920845 class loss 0.0009565895306877792 dist loss 0.030868342146277428 older dist loss 0.01802649535238743\n",
            "NUM_EPOCHS:  12 / 70  LR:  [2]\n",
            "LOSS:  0.04967543110251427 class loss 0.0008280090405605733 dist loss 0.030805598944425583 older dist loss 0.01804182305932045\n",
            "NUM_EPOCHS:  13 / 70  LR:  [2]\n",
            "LOSS:  0.049072518944740295 class loss 0.0005364057724364102 dist loss 0.030509859323501587 older dist loss 0.01802625134587288\n",
            "NUM_EPOCHS:  14 / 70  LR:  [2]\n",
            "LOSS:  0.05158211663365364 class loss 0.0007668748148716986 dist loss 0.03207021579146385 older dist loss 0.018745025619864464\n",
            "NUM_EPOCHS:  15 / 70  LR:  [2]\n",
            "LOSS:  0.04926571249961853 class loss 0.00037929153768345714 dist loss 0.030638322234153748 older dist loss 0.018248099833726883\n",
            "NUM_EPOCHS:  16 / 70  LR:  [2]\n",
            "LOSS:  0.05032302439212799 class loss 0.00040900992462411523 dist loss 0.03159981966018677 older dist loss 0.018314195796847343\n",
            "NUM_EPOCHS:  17 / 70  LR:  [2]\n",
            "LOSS:  0.05070686340332031 class loss 0.00039892454515211284 dist loss 0.031600359827280045 older dist loss 0.018707579001784325\n",
            "NUM_EPOCHS:  18 / 70  LR:  [2]\n",
            "LOSS:  0.04866737127304077 class loss 0.0003172922588419169 dist loss 0.03043338842689991 older dist loss 0.017916690558195114\n",
            "NUM_EPOCHS:  19 / 70  LR:  [2]\n",
            "LOSS:  0.05030936747789383 class loss 0.00031866037170402706 dist loss 0.0316089428961277 older dist loss 0.018381766974925995\n",
            "NUM_EPOCHS:  20 / 70  LR:  [2]\n",
            "LOSS:  0.04963034391403198 class loss 0.00022618703951593488 dist loss 0.031081266701221466 older dist loss 0.018322892487049103\n",
            "NUM_EPOCHS:  21 / 70  LR:  [2]\n",
            "LOSS:  0.04912157356739044 class loss 0.000318538659485057 dist loss 0.030666517093777657 older dist loss 0.018136518076062202\n",
            "NUM_EPOCHS:  22 / 70  LR:  [2]\n",
            "LOSS:  0.048146121203899384 class loss 0.0005049582105129957 dist loss 0.03015306033194065 older dist loss 0.01748810149729252\n",
            "NUM_EPOCHS:  23 / 70  LR:  [2]\n",
            "LOSS:  0.049039289355278015 class loss 0.0003573083085939288 dist loss 0.030586156994104385 older dist loss 0.018095824867486954\n",
            "NUM_EPOCHS:  24 / 70  LR:  [2]\n",
            "LOSS:  0.04813125729560852 class loss 0.00017844107060227543 dist loss 0.030299177393317223 older dist loss 0.017653638496994972\n",
            "NUM_EPOCHS:  25 / 70  LR:  [2]\n",
            "LOSS:  0.051067106425762177 class loss 0.00025695207295939326 dist loss 0.03192567080259323 older dist loss 0.018884485587477684\n",
            "NUM_EPOCHS:  26 / 70  LR:  [2]\n",
            "LOSS:  0.04725424200296402 class loss 0.00022167564020492136 dist loss 0.029774483293294907 older dist loss 0.01725808158516884\n",
            "NUM_EPOCHS:  27 / 70  LR:  [2]\n",
            "LOSS:  0.04921413213014603 class loss 0.00023054660414345562 dist loss 0.030607139691710472 older dist loss 0.018376445397734642\n",
            "NUM_EPOCHS:  28 / 70  LR:  [2]\n",
            "LOSS:  0.05073457211256027 class loss 0.00017479059170000255 dist loss 0.03195633366703987 older dist loss 0.018603447824716568\n",
            "NUM_EPOCHS:  29 / 70  LR:  [2]\n",
            "LOSS:  0.05125159025192261 class loss 0.0001809012464946136 dist loss 0.032026566565036774 older dist loss 0.019044121727347374\n",
            "NUM_EPOCHS:  30 / 70  LR:  [2]\n",
            "LOSS:  0.05165773257613182 class loss 0.00022992964659351856 dist loss 0.0323948934674263 older dist loss 0.019032908603549004\n",
            "NUM_EPOCHS:  31 / 70  LR:  [2]\n",
            "LOSS:  0.04811510443687439 class loss 0.00013824341294821352 dist loss 0.03020833432674408 older dist loss 0.01776852458715439\n",
            "NUM_EPOCHS:  32 / 70  LR:  [2]\n",
            "LOSS:  0.04753066599369049 class loss 0.0001674482336966321 dist loss 0.02967899665236473 older dist loss 0.017684223130345345\n",
            "NUM_EPOCHS:  33 / 70  LR:  [2]\n",
            "LOSS:  0.04985576868057251 class loss 0.00028865609783679247 dist loss 0.031091727316379547 older dist loss 0.018475383520126343\n",
            "NUM_EPOCHS:  34 / 70  LR:  [2]\n",
            "LOSS:  0.048550091683864594 class loss 0.000264570553554222 dist loss 0.030475473031401634 older dist loss 0.017810050398111343\n",
            "NUM_EPOCHS:  35 / 70  LR:  [2]\n",
            "LOSS:  0.05196700245141983 class loss 0.00033791380701586604 dist loss 0.03266758471727371 older dist loss 0.018961504101753235\n",
            "NUM_EPOCHS:  36 / 70  LR:  [2]\n",
            "LOSS:  0.049766428768634796 class loss 0.00024362746626138687 dist loss 0.03108634613454342 older dist loss 0.018436456099152565\n",
            "NUM_EPOCHS:  37 / 70  LR:  [2]\n",
            "LOSS:  0.04942673444747925 class loss 0.0001290742220589891 dist loss 0.030979285016655922 older dist loss 0.01831837370991707\n",
            "NUM_EPOCHS:  38 / 70  LR:  [2]\n",
            "LOSS:  0.051567427814006805 class loss 0.0002498192770872265 dist loss 0.03227183222770691 older dist loss 0.01904577575623989\n",
            "NUM_EPOCHS:  39 / 70  LR:  [2]\n",
            "LOSS:  0.049780502915382385 class loss 0.00013437239977065474 dist loss 0.031025972217321396 older dist loss 0.01862015761435032\n",
            "NUM_EPOCHS:  40 / 70  LR:  [2]\n",
            "LOSS:  0.05044972896575928 class loss 0.00022089439153205603 dist loss 0.03140298277139664 older dist loss 0.018825849518179893\n",
            "NUM_EPOCHS:  41 / 70  LR:  [2]\n",
            "LOSS:  0.0486966148018837 class loss 0.00019480007176753134 dist loss 0.03057522140443325 older dist loss 0.017926592379808426\n",
            "NUM_EPOCHS:  42 / 70  LR:  [2]\n",
            "LOSS:  0.047893401235342026 class loss 0.0001656610402278602 dist loss 0.030084876343607903 older dist loss 0.01764286309480667\n",
            "NUM_EPOCHS:  43 / 70  LR:  [2]\n",
            "LOSS:  0.05054127797484398 class loss 0.00022259268735069782 dist loss 0.03154866769909859 older dist loss 0.018770016729831696\n",
            "NUM_EPOCHS:  44 / 70  LR:  [2]\n",
            "LOSS:  0.04801589623093605 class loss 0.00014430312148761004 dist loss 0.03020324558019638 older dist loss 0.01766834780573845\n",
            "NUM_EPOCHS:  45 / 70  LR:  [2]\n",
            "LOSS:  0.050096191465854645 class loss 0.0002218954759882763 dist loss 0.03142724931240082 older dist loss 0.018447047099471092\n",
            "NUM_EPOCHS:  46 / 70  LR:  [2]\n",
            "LOSS:  0.048695988953113556 class loss 8.971229544840753e-05 dist loss 0.030888210982084274 older dist loss 0.01771806739270687\n",
            "NUM_EPOCHS:  47 / 70  LR:  [2]\n",
            "LOSS:  0.05031178146600723 class loss 0.00020409352146089077 dist loss 0.031449783593416214 older dist loss 0.018657905980944633\n",
            "NUM_EPOCHS:  48 / 70  LR:  [2]\n",
            "LOSS:  0.04997802525758743 class loss 0.00017721902986522764 dist loss 0.03124954178929329 older dist loss 0.018551262095570564\n",
            "NUM_EPOCHS:  49 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.04979956895112991 class loss 0.00016844738274812698 dist loss 0.031158138066530228 older dist loss 0.018472984433174133\n",
            "NUM_EPOCHS:  50 / 70  LR:  [0.4]\n",
            "LOSS:  0.048383139073848724 class loss 8.539409463992342e-05 dist loss 0.030236681923270226 older dist loss 0.018061062321066856\n",
            "NUM_EPOCHS:  51 / 70  LR:  [0.4]\n",
            "LOSS:  0.049398671835660934 class loss 0.00018587999511510134 dist loss 0.03089955635368824 older dist loss 0.01831323467195034\n",
            "NUM_EPOCHS:  52 / 70  LR:  [0.4]\n",
            "LOSS:  0.049690037965774536 class loss 8.646747301099822e-05 dist loss 0.03139195218682289 older dist loss 0.018211616203188896\n",
            "NUM_EPOCHS:  53 / 70  LR:  [0.4]\n",
            "LOSS:  0.04584336280822754 class loss 7.488742267014459e-05 dist loss 0.028902404010295868 older dist loss 0.01686607114970684\n",
            "NUM_EPOCHS:  54 / 70  LR:  [0.4]\n",
            "LOSS:  0.047621872276067734 class loss 6.708015280310065e-05 dist loss 0.03014547750353813 older dist loss 0.017409315332770348\n",
            "NUM_EPOCHS:  55 / 70  LR:  [0.4]\n",
            "LOSS:  0.050333134829998016 class loss 0.00014619513240177184 dist loss 0.03155913203954697 older dist loss 0.01862780749797821\n",
            "NUM_EPOCHS:  56 / 70  LR:  [0.4]\n",
            "LOSS:  0.046641089022159576 class loss 7.827193621778861e-05 dist loss 0.029342886060476303 older dist loss 0.01721993088722229\n",
            "NUM_EPOCHS:  57 / 70  LR:  [0.4]\n",
            "LOSS:  0.048869408667087555 class loss 9.059777221409604e-05 dist loss 0.030675092712044716 older dist loss 0.018103718757629395\n",
            "NUM_EPOCHS:  58 / 70  LR:  [0.4]\n",
            "LOSS:  0.04707500338554382 class loss 7.802755862940103e-05 dist loss 0.029414810240268707 older dist loss 0.017582163214683533\n",
            "NUM_EPOCHS:  59 / 70  LR:  [0.4]\n",
            "LOSS:  0.048659004271030426 class loss 7.725844625383615e-05 dist loss 0.030581073835492134 older dist loss 0.01800067164003849\n",
            "NUM_EPOCHS:  60 / 70  LR:  [0.4]\n",
            "LOSS:  0.045331843197345734 class loss 6.494436092907563e-05 dist loss 0.028658762574195862 older dist loss 0.016608137637376785\n",
            "NUM_EPOCHS:  61 / 70  LR:  [0.4]\n",
            "LOSS:  0.04853304103016853 class loss 7.563661347376183e-05 dist loss 0.030598407611250877 older dist loss 0.017858996987342834\n",
            "NUM_EPOCHS:  62 / 70  LR:  [0.4]\n",
            "LOSS:  0.04768434911966324 class loss 7.356397691182792e-05 dist loss 0.029933838173747063 older dist loss 0.01767694763839245\n",
            "NUM_EPOCHS:  63 / 70  LR:  [0.016000000000000004]\n",
            "LOSS:  0.046244390308856964 class loss 7.653671491425484e-05 dist loss 0.029248349368572235 older dist loss 0.016919506713747978\n",
            "NUM_EPOCHS:  64 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.05001657083630562 class loss 0.00010776997078210115 dist loss 0.03149097040295601 older dist loss 0.018417829647660255\n",
            "NUM_EPOCHS:  65 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.050524089485406876 class loss 7.618584641022608e-05 dist loss 0.03185337036848068 older dist loss 0.01859453320503235\n",
            "NUM_EPOCHS:  66 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.048308491706848145 class loss 0.00013882345228921622 dist loss 0.030530719086527824 older dist loss 0.017638947814702988\n",
            "NUM_EPOCHS:  67 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.047102704644203186 class loss 7.211887714220211e-05 dist loss 0.029916638508439064 older dist loss 0.017113948240876198\n",
            "NUM_EPOCHS:  68 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.04831073433160782 class loss 8.00670895841904e-05 dist loss 0.030367426574230194 older dist loss 0.017863238230347633\n",
            "NUM_EPOCHS:  69 / 70  LR:  [0.08000000000000002]\n",
            "LOSS:  0.04890044033527374 class loss 6.339432002278045e-05 dist loss 0.031019052490592003 older dist loss 0.017817990854382515\n",
            "Reducing each exemplar set to size: 20\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 24.28\n",
            "\n",
            "Test Accuracy (all groups seen so far): 28.17\n",
            "\n",
            "the model knows 100 classes:\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "outputId": "50119d57-0e34-4358-a81e-acc60e8cd32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if herding:\n",
        "  method = 'iCaRL_{}_herding'.format(classifier)\n",
        "else:\n",
        "  method = 'iCaRL_{}_random'.format(classifier)\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics iCaRL for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAJcCAYAAAAb0rWEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdeZgkVZno/+/bzSbKKg0idA+4oONtBaVhUFRAlAHEab1XHRxkwK1VXBEHQefndvU+uAyKM6NMKygqiqAgjoKKjMjAKNggKIK7QAONtAuCG9Bd7++PiIKkKqsqszIjMjPy+3mefDrzZEScE5FZVadPnPe8kZlIkiQ1zYJBN0CSJKkKdnIkSVIj2cmRJEmNZCdHkiQ1kp0cSZLUSHZyJElSI9nJUU8i4gER8Z8R8fuIOKuH4xwWEV/vZ9sGISLOj4gj5rFfI86/ahFxUUS8tI7jN/0ziYh9I+KmQbdDqpKdnDEREf8QEasi4g8Rsab8Y/zkPhz6ucB2wIMz83nzPUhmnp6ZB/ShPfdT/iLPiDhnSvmuZflFHR7n7RHx6bm2y8yDMvO0bts59fyj8NqIuCYi/hgRN0XEWRHx2A7aulN5bn8oH9dHxHFTtrk+Ip7eaftaruOHp5RfEhFHtrzePiJOKb9jd0bEjyLiHRHxwPL9jIjbImKDln02LMuGatGuqr6ToywiXhcRvyy/k9dFxC4t7y2KiM+U/+H5XUScPsi2SmAnZyxExBuADwL/j6JDsgT4MLC8D4f/K+AnmbmuD8eqylrgiRHx4JayI4Cf9KuCslPSz5+nk4DXAa8FtgZ2Ab4IPLOLY2yZmQ+i6Ij+fxHxjB7b9Efg8IjYqd2bEbE18G3gAcATM3Mz4BnAlsDDWzb9HXBQy+uDyrLaVPB5NV45wvUSiu/gg4BDgF+3bHI2cCvF75dtgffX3UZpmsz00eAHsAXwB+B5s2yzMUUn6Jby8UFg4/K9fYGbgGOA24A1wIvK994B3A3cU9bxEuDtwKdbjr0TkMAG5esjgV8AdwK/BA5rKb+kZb8nAd8Ffl/++6SW9y4C/i9waXmcrwPbzHBuk+0/GXhVWbYQuBl4K3BRy7YnAauBO4ArgKeU5QdOOc+rW9rx7rIdfwYeUZa9tHz/I8AXWo7/HuBCINq0897zBx4JrAf2nOUzeybwvbKtq4G3z3TNy7LLgX9qeX098PQuvkeT1/FfgY+3lF8CHFk+fxfwA2DBLMdJ4J+Bs1rKPg+8BcgO2jHrZw/sBfwPcDtwNbDvlH2nfl7PAH5Ufs/+DfhWy+d372fS0vZXAD8tj//vk59l+Z36F4o/+r8EXj31M+jy5/Zg4NryHG8G3tjy3iHAVWUb/gd4XMt7DwW+QNGx/yXw2pb3HgB8gqJDeS3wT8BNHbZnQfk923+G9w8ov1ML53O+PnxU9fB/Ms33RGAT4JxZtnkLxR+H3YBdgT0p/hBNeghFZ2kHio7Mv0fEVpn5NorRoc9l5oMy85TZGlLesvgQcFAW/8t/EsUv66nbbQ18pdz2wcCJwFemjMT8A/Aiiv8xbgS8cba6gU8C/1g+/1vgGooOXavvUlyDrYHPAGdFxCaZ+dUp57lryz6HAyuAzYAbphzvGOCxEXFkRDyF4todkZlz3ZbZn+KPz+WzbPPH8ny2pOjwvDIint1uw4jYC1gK/GyOejvxbuD/RMSj2rz3dODszJyY4xhfBJ4aEVtGxFbAU4Bzu2hD288+Inag+N68i+IzfCPwhYhY1LJv6+f1e4rRh38GtgF+Duw9R92HAHsAjwOeT/FdAngZxYjUbsATgLafRRdOAV5e/pwsBf4LICIeD5wKvJziZ+M/gC9FxMblyNR/UnTudqD4Hr0+Iibb+DaKEbWHl+2+39yxiPjw1NuRLXYsH0sjYnV5y+odLaNhewE/Bk6LiN9ExHcjYp8er4HUMzs5zfdg4Nc5++2kw4B3ZuZtmbmWYoTm8Jb37ynfvyczz6MYzWj3R64TExS/KB+QmWsy84dttnkm8NPM/FRmrsvMz1L8b/tZLdt8PDN/kpl/Bs6k+OMyo8z8H2Dr8o/zP1J0eqZu8+nM/E1Z579QjHDNdZ6fyMwflvvcM+V4f6K4jicCnwZek5mdTPR8MMWI2Wznc1Fm/iAzJzLz+8Bngal/VH4dEX+muIX0YYrORU8y81aKUbF3zqfdpb9Q/DH++/LxpbKsUzN99i8EzsvM88rrcgGwimJUZNK9nxdFp+SHmfn58rP7IMXtltmckJm3Z+aNwDdb6n4+cFJm3pSZvwNO6OJ82rkHeExEbJ6Zv8vMK8vyFcB/ZOZlmbk+i/lfd1F0MvYAFmXmOzPz7sz8BfBR4NCWNr47M3+bmasp/hNxr8w8KjOPmqE9O5b/HgA8FtgPeAFFx33y/QMorslDKEa1zo2IbXq5CFKv7OQ032+AbVonerbxUO4/CnFDWXbvMaZ0kv5EcU++K5n5R4o/aq8A1kTEVyLi0R20Z7JNO7S8bv1j1Gl7PkVxG2E/2oxsRcQby8mUv4+I2ylGr+b6Jb16tjcz8zKK23NB8Qe5E78Btp9tg4j4m4j4ZkSsjYjfU1zTqW3dhuK6HENxu2nDDuufy3uAv42IXaeUz9nuFpMja207nHOY6bP/K+B5EXH75AN48pQ2tX5eD219XY6wzfp5zlL3/Y4123GiiNqanBR+/gyb/R+KztkNEfGtiHhiWf5XwDFTznFxWf9fAQ+d8t6bKebhtWvj1J+x2fy5/Pe9ZSfveopRpINb3r8+M08p/zN0RlnXXCNjUqXs5DTftyn+pzfb8PktFL8gJy1h+q2cTv0R2LTl9UNa38zMr2XmMyj+8PyI4n+ac7Vnsk03z7NNkz4FHEXxv/0/tb5R3k46luJ/u1tl5pYUtzNisukzHHPWW08R8SqKEaFbyuN34kJgx4hYNss2n6EYAVmcmVtQjK7E1I3K/+2fSDFSMtP/0ruSmb+hGPX4v1Pe+gbwnA4n9P43xXdgO4p5Pf2wGvhUZm7Z8nhgZraOqrR+XmsoOghAMRm59XWX1nDfaAezHSeLqK0HlY+DZtjmu5m5nOKW3Be5r4O8mmI0pvUcNy1HO1cDv5zy3maZOdkRud/5UvxMderHFPPSWq9f6/PvM/1nYaii5TSe7OQ0XGb+nmKC7b9HxLMjYtMyZPegiHhvudlngX8uQ0C3KbefM1x6BldRzLdYEhFbAMdPvhER20XE8nJuzl0Ut73azd84D9glirD3DSLi74HHAF+eZ5sAyMxfUtzSeUubtzcD1lFM2NwgIt4KbN7y/q+AnbqJyCnDa99FcRvlcODYiJj1tlrZzp9S3F76bBSh2xtFxCYRcWjcFwq+GfDbzPxLROxJMU9lNieU9W/SUrZhedzJx2yjfVOdSDGn6q+nlG1OMS/jr6CYJxMRJ0bE46acY1Lcfvy7DuYoderTwLMi4m8jYmF5TvtGxI4zbP8V4H9FxP8uz/21TOmUd+FM4HXl+W4JvGmex6H8vA+LiC3K22h3cN/PyUeBV5QjeRERD4yIZ0bEZhSTy++MiDdFsX7VwohYGhF7tLTx+IjYqrwmr+m0TeV/Cj5H8R3arNx/Bff9TJ4DbBURR5T1Ppei03fpfK+D1A92csZAOb/kDRQTLNdS/I/v1dw3R+NdFHMXvk8RHXNlWTafui6g+GX4fYoIpdaOyYKyHbcAv6XocLyyzTF+QzHB8xiKWyDHAodk5q+nbjuP9l2Sme1Gqb4GfJUirPwGipGP1qH9yYUOfxMRVzKH8o/mp4H3ZObVZcflzcCnImLjDpr6Wopon3+niKL5OfAcirksUIzKvDMi7qTolM51K+wrFFE1L2spO4/iNsPk4+0dtAuAzLwDeC/FBN/Jst9SdHzuAS4r23YhxYjYtEnP5dyYdnOy5qWcZ7Kc4jpPfs//iRl+z5Xfp+dRdAB/QxHVNt8/yh+liPT6PkXU23kUneb18zze4cD1EXEHxa3Iw8o2r6L4DP+N4vP8GUUUGJm5nuLnZjeKyKpfAx+juO0KxVy7G8r3vk4xsnmviDg5Ik6epU2vpviPyS0UI8SfoZgEPfnZ/x3FZO/fA8cBy/vxMyv1Ivr3nyhJEkBEHAScnJlTb7tKqpEjOZLUo/L20MHl7dUdKMK1Z1u2QVIN7ORIAiAi3twS9dP6mCkCqKp2tGvDH8rJ4cMqKG4H/Y7idtV1FLcRJQ2Qt6skSVIjOZIjSZIaqZuQ0b6JiAMp8gQtBD42ZR2Lad79kkOnDTedc/XT5l3/7xfPdykM2GL1XGuFdVdvu+N1ul2v6qpHktSdVatWTFv3qmJ13tap7dxqH8mJiIUUYbEHUax98oKIeEzd7ZAkSc02iNtVewI/y8xfZObdwBkUa1tIkiT1zSA6OTtw/0XWbuL+OYkAiIgVEbEqIlZ990c/r61xkiSNm4n162t71GloJx5n5srMXJaZy/Z49MMH3RxJkjRiBjHx+GbunyRuR3pPvChJkuZpYqJdGsFqLFi4sLa6al8np8zp8xNgf4rOzXeBf5gth82yZSunNfI5u/7XtO16ibjq1Oq9955WtvjS4cpB1++oKaOwJKledUdXrbvnnto6AxtsuGFt51b7SE5mrouIV1MkRFwInNrPJH2SJKk7tc6V2XDD2qoayDo5mXkeRZZeSZKkSgykkyNJkobHxES9UU91GdroKkmSpF44kiNJ0pibWF9fdFWdRraT0y6S6rzvrJtWtvfzdp5W1mm+qHaRVEvPOKOjfZsUfVTHuTT9GnbK61C45tBDp5W1+9kbNn5+0nCpvZMTEYuBTwLbUSQEW5mZJ9XdDkmSVGjqnJxBjOSsA47JzCsjYjPgioi4IDOvHUBbJElSQ9U+8Tgz12TmleXzO4HraJO7SpIkqRcDnZMTETsBjwcua/PeCmAFwJIlh7Fo0VNrbZskSeOi7sSZdRlYCHlEPAj4AvD6zLxj6vutCTrt4EiSpG7VnrsKICI2BL4MfC0zT5xr+3a5qzp11id/O63sef+49XwP15a5ombW6bmMQk4wSapL3bmrbv/12to6A1tus6i2c6t9JCciAjgFuK6TDo4kSdJ8DGJOzt7A4cAPIuKqsuzNZT4rSZJUs6bOyRlEFvJLgFqH4SRJ0vgZ2RWPJUlSfzR1JMcEnZIkqZEaP5JzwFt2nVZ27DM/P61sxaPeMq1s9xO/Ma2s02ihXiKkRjWSqp1Oz2UcI6maFEXXqXbn3E6n12Ecr6FUhYmJZiboHOQ6OQsj4nsR8eVBtUGSJDXXIEdyXkeR0mHzAbZBkqSx55ycPoqIHYFnAh8bRP2SJKn5BjWS80HgWGCzmTYwd5UkSfWYmHAkpy8i4hDgtsy8YrbtzF0lSZJ6MagVj/8uIg4GNgE2j4hPZ+YLB9AWSZLGXlPn5AxixePjgeMBImJf4I1zdXD6HSb6ms0+ML1w2zM72rfTeu9YsmTe+7bTaejtTPUYatu7fl/Dcbz+/T7ncbyGqoa/I5vJxQAlSVIjDXQxwMy8CLhokG2QJGncuRigJEnSCGl8WgdJkjS7pk48diRHkiQ10kBGciJiS4rVjpcCCbw4M7890/b9jmDZ4owzpm/Ypujf/9+N08pe9ebpUVPtbH7j9H170ess/zoSHq7ee+9pZe2uw6hGLJg0Umqucf8ZbepIzqBuV50EfDUznxsRGwGbDqgdkiSpoWrv5ETEFsBTgSMBMvNu4O662yFJkgpGV/XPzsBa4OMR8b2I+FhEPHDqRhGxIiJWRcSqtWsvrr+VkiRppA2ik7MB8ATgI5n5eOCPwHFTNzJ3lSRJ9ZhYv762R50G0cm5CbgpMy8rX3+eotMjSZLUN4PIXXVrRKyOiEdl5o+B/YFruz1Ou0iexZdeOu92tYuIedWbp2/3xsNun1b2/tO3nFY2qjP1e2l3L9e/Scwd1h2vTX281prJxITRVf30GuD0MrLqF8CLBtQOSZLUUAPp5GTmVcCyQdQtSZLub2K90VWSJEkjw9xVkiSNuabOyXEkR5IkNdLIjuS0i+TpJeKq0wiDdpFUq1atmFb2yOecP+86emUExcwGdW28/oM1Cj8TdbRx2M5ZqtqgEnQeDbyUIjnnD4AXZeZfBtEWSZLGXVMTdNZ+uyoidgBeCyzLzKXAQuDQutshSZKabVC3qzYAHhAR91BkIL9lQO2QJGnsmaCzTzLzZuD9wI3AGuD3mfn1qduZoFOSJPViELertgKWU2QjfyjwwIh44dTtTNApSVI9mpqgcxC3q54O/DIz1wJExNnAk4BP93rgzW+8sddD3E+n0Q7Llq2cVnbpd9ZNKzt4r3oud6e5k+5YsmRaWZPyT41CRE0/jer5Gt1WGIU2SqNmEJ2cG4G9ImJT4M8UCTpXDaAdarB2f/AlSe0ZXdUnmXkZ8HngSorw8QXA9KEQSZKkHgwqQefbgLcNom5JknR/RldJkiSNkJFN6yBJkvqjqXNyRqKT0+kk0k6jE/p9vHb2ft7O08p+9oZHTSvb/cRvzLsO6DyiZtwiN7ZYvbptLrMmX4cmn5skzUdlnZyIOBU4BLitTN9ARGwNfA7YCbgeeH5m/q6qNjSdEUQza9fBkSS1NzHRzJGcKufkfAI4cErZccCFmflI4MLytSRJUt9VNpKTmRdHxE5TipcD+5bPTwMuAt5UVRskSdLcJtYbXdUP22XmmvL5rcB2M23Ymrvq99efX0/rJElSYwwshDwzE8hZ3r83d9UWOx1UY8skSVIT1B1d9auI2D4z10TE9sBtNdcvSZKmaOrE47o7OV8CjgBOKP89t5Od2oXG9hIePKiEgLufOL3sijc8vc1208PKq0i+2OSQ4yYlGm26UU0sOux6ua7XHHpo2/KlZ5zRU5ukTkTEYuCTFFNaEliZmSfNJ0K7yhDyz1JMMt4mIm6iSONwAnBmRLwEuAF4flX1S5KkzgzZYoDrgGMy88qI2Ay4IiIuAI6kiNA+ISKOo4jQnjV4qcroqhfM8Nb+VdUpSZJGWxmgtKZ8fmdEXAfswDwitEdixWNJklSdOhN0RsQKYEVL0crMXDnDtjsBjwcuo4sI7Ul2ciRJUm3KDk3bTk2riHgQ8AXg9Zl5R0S0HiMjYsYI7Ul2ciRJGnNDNieHiNiQooNzemaeXRZ3HaFdd+6q9wHPAu4Gfg68KDNvn8/xe4me6Xc0R7tIr3bta7ddu0iqdtttfuON82zdeJopr1cvSVyN+KmG17UavVxXo6g0SFEM2ZwCXJeZJ7a81XWEdt25qy4Almbm44CfAMdXWL8kSerAxPr1tT06sDdwOPC0iLiqfBxM0bl5RkT8FHh6+XpWteauysyvt7z8DvDcquqXJEmjJzMvAWKGt7uK0B5YWgfgxcCMSalac1etXXtxjc2SJGm8TExM1Pao00A6ORHxForFfk6faZvW3FWLFj21vsZJkqRGqD26KiKOpJiQvH+ZpFOSJA3QsEVX9UutnZyIOBA4FtgnM//U6X7DHunSaaRXL9ud9cnfTis74C27tt2/39dm2K9/O8OW12sUr6Ekjbq6c1cdD2wMXFAu6vOdzHxFVW2QJElzMwt5l2bIXXVKVfVJkiS1GmR0lSRJUmVM6yBJ0phr6sRjR3IkSVIjjcRITr+jUO5YsqTyOjrNZ9Wp5/3j1tPKfvaGR7XdduNbptfdSy4ao4B6N6hr2GsOL0njoe5F+upS2UhORJwaEbdFxDVt3jsmIjIitqmqfkmSNN6qHMn5BPBvwCdbCyNiMXAAYFptSZKGgHNyupSZFwPTV7CDD1AsCOhqx5IkqTJ1r3i8HLg5M68uFwOcbdsVwAqAJUsOw/xVkiRVw5GcHkXEpsCbgbd2sr0JOiVJUi/qHMl5OLAzMDmKsyNwZUTsmZm31tiOjqOcZopMmapdpEovkVSdtmX3E7/RdtsFn/7RtLKJMx7d17o7jc4xZ9Ngea01Tobt982wtWc2TY2uqq2Tk5k/ALadfB0R1wPLMvPXdbVBkiSNj1oTdGamuaskSRoyTZ2TU3eCztb3d6qqbkmSpJFY8ViSJFVnYqKZIznmrpIkSY3kSI4kSWNuYr3RVV2JiFOBQ4DbMnNpS/lrgFcB64GvZOaxVbWhV8MU6tdNWyZeOD1c/IoTpg/a7X5cZ1/qdnV3Gho5TNdQUrMN2++bYWvPOKo1d1VE7AcsB3bNzLsiYtsZ9pUkSTVxTk6XZshd9UrghMy8q9zmtqrqlyRJ463uice7AE+JiMsi4lsRscdMG0bEiohYFRGr1q69uMYmSpKkJqh74vEGwNbAXsAewJkR8bDMnJaRPDNXAisBli1bacZySZIq0tTFAOseybkJODsLlwMTwDY1t0GSJI2BukdyvgjsB3wzInYBNgJqz121eu+9p5X1O6FmO/1O1tbuPKD9ubSLpLr2o9+bVvaYlz2+o7oHFTXQ7hresWTJtLI6Ps8q9Ps7MlOSWaM+JLUyQWeX2uWuAk4FTo2Ia4C7gSPa3aqSJEnq1SByV72wqjolSVL3nJMjSZI0QkzrIEnSmHMkR5IkaYTUmrsqInYDTgY2AdYBR5Wh5LUaVORNvyNaej2PdpFUveS4qkPT82P1+1yadG0Gqd9Rb9KwaWp0VZUjOZ8ADpxS9l7gHZm5G/DW8rUkSVLfVRlddXFE7DS1GNi8fL4FcEtV9UuSpM44J6c/Xg+8LyJWA+8Hjp9pQ3NXSZKkXtTdyXklcHRmLgaOBk6ZacPMXJmZyzJz2aJFT62tgZIkjZuJifW1PepUdyfnCODs8vlZwJ411y9JksZE3evk3ALsA1wEPA34adUVzpS7Z6p+5wcaZORFL+3pd44raVh1k9fLSCo13cT6ZkZX1Z276mXASRGxAfAXYEVV9UuSpPE2iNxVu1dVpyRJ0iTTOkiSNObqnhBcF9M6SJKkRnIkR5KkMdfUxQAb38mpIypi2CIv+t2edpFU1xx66LSypWec0dd6pSoN28+tpP6rMrpqMfBJYDuKdA4rM/OkiNga+BywE3A98PzM/F1V7ZAkSbMzQWf31gHHZOZjgL2AV0XEY4DjgAsz85HAheVrSZKkvqoyhHwNsKZ8fmdEXAfsACynWD8H4DSKhQHfVFU7JEnS7Jo6J6eW6KoyG/njgcuA7coOEMCtFLez2u1jgk5JkjRvlU88jogHAV8AXp+Zd0TEve9lZkZEttsvM1cCKwGWLVvZdhtJktS7po7kVNrJiYgNKTo4p2fmZGLOX0XE9pm5JiK2B26rsg2qRrtIqmHL4SWNsyp+Hv0Z16ipMroqgFOA6zLzxJa3vkSRjfyE8t9zq2qDJEmaW1Ojq6ocydkbOBz4QURcVZa9maJzc2ZEvAS4AXh+hW2QJEljqsroqkuAmOHt/auqV5Ikdaepc3LMXSVJkhqp8WkdJEnS7MxCLkmSNEIGkbvqfcCzgLuBnwMvyszbuz1+L6GMTQ+DHNS1adI1VO+G/We0XR1V1DMoVZxHU66NxscgclddACzNzMcBPwGOr7ANkiRpDjkxUdujTpV1cjJzTWZeWT6/E7gO2CEzv56Z68rNvgPsWFUbJEnS+Kpl4vGU3FWtXgx8boZ9VgArAJYsOYxFi55aYQslSRpfCxY2c4pu5Wc1NXdVS/lbKG5pnd5uv8xcmZnLMnOZHRxJktStQeSuIiKOBA4B9s9Mk29KkjRACxbOtHbvaKs9d1VEHAgcC+yTmX+a7/F7meXf9AiBYb82TY9uU2HYv4d+56TmG0Tuqg8BGwMXFP0gvpOZr6iwHZIkaRYLFjiS05VZcledV1WdkiRJk0zrIEnSmGvqnJxmxoxJkqSx50iOJEljzjk5I+qKNzx9WtnuJ35jAC3RpHZRLdd+9HvTyh7zssfX0RxJUkNVdrsqIhZHxDcj4tqI+GFEvG7K+8dEREbENlW1QZIkzW3BwqjtUacqR3ImE3ReGRGbAVdExAWZeW2ZofwA4MYK65ckSWOsyhDyNcCa8vmdEXEdsANwLfABigUBz62qfkmS1JmmzsmpJbqqNUFnRCwHbs7Mq+fYZ0VErIqIVWvXXlxDKyVJUpPUmqCT4hbWm4G3zrWfCTolSVIvak3QGRGPBXYGri5TOuwIXBkRe2bmrTMdp12uozuWLJlWtvjSS6eVDXsklXmcCu0iqa44YXoffPfjJupojiSNlaYuBlhrgs7M/AGwbcs21wPLMvPXVbVDkiSNp9oTdGamuaskSRoiTZ14PIgEna3b7FRV/ZIkabw1fsVjSZI0u6bOyTFBpyRJaqSRGMlpF23U7wikQUU5jWok1eq9955W1i66rRftIqnqqHcmdXxH2tXRzqh+byQNJ0dyujRb7qqIeE1E/Kgsf29VbZAkSeOr9txVwHbAcmDXzLwrIrad9SiSJKlSRld1aZbcVS8DTsjMu8r3bquqDZIkaXzVnrsK2AV4SkRcFhHfiog9ZtjH3FWSJNVgwcKo7VHreVVdQWvuqsy8g2L0aGtgL+CfgDPL1ZHvx9xVkiSpF7XmriqLbwLOzswELo+ICWAbYG2VbZnLOEar9BIt1GlEU78jkmaqt46oq1GN6BtUVNg4/kxJo2rBgmauKFNldNW03FWlLwL7ldvsAmwEmLtK8zbIsPJh12lIuiQ1Ue25q4BTgVMj4hrgbuCIclRHkiQNQFPXyRlU7qoXVlWvJEkSmNZBkiQ11EikdZAkSdVp6mKAjuRIkqRGqmwkJyIWA5+kSOOQwMrMPCkidgNOBjahSP1wVGZeXlU7NLOmJCBtF0n13m/e2XbbY/fbrOrmdKwp17+begw1753XUFVw4nH3Zspd9V7gHZl5fkQcXL7et8J2SJKkMTSI3FUJbF5utgVwS1VtkCRJc2vqSM4gcle9HnhfRKwG3g8cP8M+5q6SJEnzNojcVa8Ejs7MxcDRFKsiT2PuKkmS6rFgQdT2qPW8qjz4DLmrjgAmn58F7FllGyRJ0niqMrpqptxVtwD7ABcBTwN+WlUbNL5miqK69qPfm1b2mJc9flrZsEewDHv7ZjIKbRx2XkNVoalzcgaRu+plwEkRsQHwF2BFhW2QJEljalC5q3avql5JktQdVzyWJEkaIeaukiRpzDV1To4jOZIkqZEaNZJTR8TJqEa1NFk3n0m7SKprDj10WlFVGzMAACAASURBVNnSM86opT3z5XdOUj85J6dLEbFJRFweEVdHxA8j4h1l+c4RcVlE/CwiPhcRG1XVBkmSNHoi4tSIuC0irmkpe3tE3BwRV5WPg+c6TpW3q+4CnpaZuwK7AQdGxF7Ae4APZOYjgN8BL6mwDZIkafR8AjiwTfkHMnO38nHeXAeprJOThT+ULzcsH0mxAODny/LTgGdX1QZJkjS3BQujtkcnMvNi4Lc9n1evB5hNRCwsFwK8DbgA+Dlwe2auKze5iSIzebt9TdApSVLDtP59Lx/dLAr86oj4fnk7a6u5Nq60k5OZ6zNzN2BHihxVj+5iXxN0SpJUgzoTdLb+fS8fKzts5keAh1NMgVkD/MtcO9QSXZWZt0fEN4EnAltGxAblaM6OwM39qqeOiBOjWoZPr59Ju0iq1XvvPa1s8aWX1tIedc5oR2l8ZOavJp9HxEeBL8+1T5XRVYsiYsvy+QOAZwDXAd8EnltudgRwblVtkCRJc1uwcEFtj/mKiO1bXj4HuGambSdVOZKzPXBaRCyk6EydmZlfjohrgTMi4l3A9ygylUuSJAEQEZ8F9gW2iYibgLcB+0bEbhRBTNcDL5/rOFUm6Pw+MG3ltcz8BcX8HEmSNASGLa1DZr6gTXHXgyKmdZAkSY3UqLQOkiSpe01N62AnpyLtonM2v/HGaWWDjAQxMmVm7SKpRjHHVdN5vSTNprJOTkRsAlwMbFzW8/nMfFtEnA4sA+4BLgdenpn3VNUOSZI0u2Gbk9Mvg8hddTrFooCPBR4AvLTCNkiSpDFVZXRVAtNyV7Um1IqIyykWBJQkSQPS1Dk5teauyszLWt7bEDgc+OoM+5q7SpIkzVutuasiYmnL2x8GLs7M/55hX3NXSZJUg2HLQt6386qjksy8nSKdw4EAEfE2YBHwhjrqlyRJ46fK6KpFwD1lcs7J3FXviYiXAn8L7J+ZE1XVP2idJnMcJMNvu9PLZ2q4+Hjwc5aGyyByV60DbgC+HREAZ2fmOytshyRJmkVTJx4PIneVCxBKkqTK2eGQJGnMuRigJEnSCHEkR5KkMeecnC7NlLuq5f0PAS/OzAdV1QbVq+mRJZ2eS9OvQ1O0+5ygt8/Kz1kaLlWO5EzmrvpDubrxJRFxfmZ+JyKWAVtVWLckSeqQc3K6lIVpuavKkPL3AcdWVbckSdIgcle9GvhSZq6ZY19zV0mSVAPTOsxDm9xVTwWeB/xrB/uau0qSJM1bLdFVZWqHbwL7AY8AflaudrxpRPwsMx9RRzskSdJ0Rld1aabcVZn5kJZt/jBOHZxeom5GIWJn2NrTb51+Bu3KRuHzazKvvzSeas9dVWF9kiRpHpoaXVV77qop27hGjiRJqoQrHkuSNOYWLGhmlqdmnpUkSRp7dnIkSVIjebuqRubEGW3D9PkZLdSdTiPeZtpWarqmTjyubCQnIjaJiMsj4uqI+GFEvKMsj4h4d0T8JCKui4jXVtUGSZI0vmpP0An8NbAYeHRmTkTEthW2QZIkzcHFALuUmQlMS9AJvBL4h8ycKLe7rao2SJKk8TWIBJ0PB/6+TL55fkQ8coZ9TdApSVINTNA5D20SdC4FNgb+kpnLgI8Cp86wrwk6JUnSvNWdoPNA4Cbg7PKtc4CP19EG9ZfRPYV+X4drDj10WtnSM87oax0qeA2l+xhd1aWIWBQRW5bPJxN0/gj4IkU2coB9gJ9U1QZJkjS+ak/QGRGXAKdHxNEUE5NfWmEbJEnSHIyu6tJMCToz83bgmVXVK0mSBK54LEnS2HNOjiRJ0ggZ2ZEco3u60+/r5bUu9Ps6tIuk6uWzMz+TpE40dU7OIHJX7R8RV0bEVRFxSUQ8oqo2SJKk8TWI3FUfAZZn5nURcRTwz8CRFbZDkiTNoqlzcgaRuyqBzcvyLYBbqmqDJEkaX4PIXfVS4LyIuAk4HDhhhn3NXSVJkuZtELmrjgYOzswdKVI6nDjDvuaukiSpBgsWRG2PWs+rjkrKBQC/CRwE7FqO6AB8DnhSHW2QJEnjpbI5ORGxCLinTM45mbvqPcAWEbFLZv6kLLtuPscf9hDYYQtxH/brNar6Hd7dbt92Zb3sO6qG7WdKahInHndvptxVLwO+EBETwO+AF1fYBkmSNKYGkbvqHOCcquqVJEndcTFASZKkETKyaR0kSVJ/NHVOjiM5kiSpkSofySknHq8Cbs7MQyJiZ+AM4MHAFcDhmXl31e2oUruojzuWLJlWZiRI8wwqyWkvEVejahTOpemfgZprwcJmjnnUcVav4/5h4u8BPpCZj6CIrnpJDW2QJEljpuq0DjsCzwQ+Vr4O4GnA58tNTgOeXWUbJEnS7Jq64vGMt6si4l8pkmm2lZmv7eD4HwSOBTYrXz8YuD0z15WvbwJ2mKH+FcAKgCVLDsPUDpIkqRuzzclZ1cuBI+IQ4LbMvCIi9u12/8xcCawEWLZs5YydLUmS1JumRlfN2MnJzNNaX0fEppn5py6OvTfwdxFxMLAJsDlwErBlRGxQjubsCNzcfbMlSZJmN2d0VUQ8ETgFeBCwJCJ2BV6emUfNtl9mHg8cXx5jX+CNmXlYRJwFPJciwuoI4NyezmAIND1nkAZrHPNUjSo/A42qcV7x+IPA3wK/AcjMq4FeJsi8CXhDRPyMYo7OKT0cS5Ikqa2O1snJzNVFYNS91ndTSWZeBFxUPv8FsGc3+0uSJHWrk07O6oh4EpARsSHT172RJEkjrKkTjzu5XfUK4FUUod63ALuVryVJkobWnCM5mflr4LAa2iJJkgagqROPO4muehhF6PdeFIsDfhs4upxbIw2ldhFJMJrRL73kqZrpOvRS9zAxV9Rw8nPRsOjkdtVngDOB7YGHAmcBn+20gohYGBHfi4gvl69Pj4gfR8Q1EXFqOc9HkiQNyIKFUduj1vPqYJtNM/NTmbmufHyaYnG/Tk2dqHw68GjgscADgJd2cSxJkqSOzJa7auvy6fkRcRzF4n0J/D1wXicHb0nQ+W7gDQCZeV7L+5dTrHosSZIGZBzn5FxB0amZPPOXt7yXlKsZz2Fqgs57lbepDqcY6ZnGBJ2SJKkXs+Wu2rmXA3eQoPPDwMWZ+d8z1G+CTkmSatDUdXI6WvE4IpYCj6FlLk5mfnKO3aYl6IyIT2fmCyPibcAi7j86JPVN0yM5Oj0/r4MGwc9Fw6KTEPK3AftSdHLOAw4CLgFm7eTMkKDzhRHxUopcWPtn5kQvjZckSb1r6khOJ9FVzwX2B27NzBcBuwJb9FDnycB2wLcj4qqIeGsPx5IkSWqrk9tVf87MiYhYFxGbA7cBXa0wNiVBZ0e3yCRJUj3GMbpq0qqI2BL4KEXE1R8oVj2WJEkaWp3krjqqfHpyRHwV2Dwzv19tsyRJUl2aOidntsUAnzDbe5l5ZTVNkiRJ6t1sIzn/Mst7CTytkwoiYiGwCrg5Mw9pKf8Q8OLMfFAnx5FUv2sOPXRa2dIzzhhAS+ozbsklx+181V5GfSM5dY4ZzbYY4H59qmMyd9XmkwURsQzYqk/HlyRJmqaTEPJ5a8ld9bGWsoXA+yjSPUiSJFWi0k4O9+Wual3079XAlzJzzWw7RsSKiFgVEavWrr24yjZKkjTW1mXW9qhTZZ2c1txVLWUPBZ4H/Otc+2fmysxclpnLTM4pSZK61UlahwAOAx6Wme+MiCXAQzLz8jl2nZa7CvghcBfws+KwbBoRP8vMR/RyEpIkaf7qHGHZqMaZx5FznFhEfITidtPTMvOvI2Ir4OuZuUfHldyXu+qQKeV/6CS6yizk0v0NMiJmUHWPYxTQOJ6zCqtWrah14Zo/TZxc29/ZTRe8orZz62TF47/JzCdExPcAMvN3EbFRxe2SJEk1qXuuTF066eTcU0ZEJUBELOL+E4nn1Jq7akq5a+RIkqRKdNLJ+RBwDrBtRLybIiv5P1faKkmSVJuxHcnJzNMj4gpgf4qFCp+dmddV3jJJkqQedBJdtQT4E/CfrWWZeWOVDZMkSfVYN+gGVKST21VfoZiPExSh4DsDPwb+VycVTM1dVYakv4tivZz1wEcy80PzaPu81RGxMGxREf1uz7Cd3zCp49oM8lq3q7uXc+5033H8fo3jOUv91Mntqse2vi6zkx/VRR1Tc1cdCSwGHp2ZExGxbRfHkiRJfdbUOTldr3icmVcCf9PJtu1yVwGvBN6ZmRPl8W7rtg2SJElz6WROzhtaXi4AngDc0uHxJ3NXbdZS9nDg7yPiOcBa4LWZ+dM29a4AVgAsWXIYpnaQJKka4zySs1nLY2OKOTrL59qpXe6q0sbAXzJzGfBR4NR2+5u7SpIk9WLWkZxy0vBmmfnGeRx7Wu6qiPg0cBNwdrnNOcDH53FsSZLUJ00dyZmxkxMRG2TmuojYez4HzszjgePLY+1LkbvqhRFxArAf8EtgH+An8zl+L+qIWBi2qIhBRfeMYxRW08+vnXbnfMUJ0weKdz9u+mLp43i9NLra/U6D/kcdqj9mG8m5nGL+zVUR8SXgLOCPk29m5tkz7TiHE4DTI+Jo4A/AS+d5HEmSpBl1sk7OJsBvgKdx33o5yX23nObUmrsqM2+niLiSJElDYOxuV1HkqnoDcA33dW4mNfNqSJKkxpitk7MQeBD379xMspMjSVJDjGNahzWZ+c7aWiJJktRHs3Vy2o3gaAz1EiFgfqzx1S6SShp13fwOGqXfV02dkzPbYoD796OCiFgYEd+LiC+Xr/ePiCsj4qqIuCQiHtGPeiRJklrNOJKTmb/tUx1TE3R+BFiemddFxFHAP1Mk7ZQkSQMwjiM5PZshQWdyX4dnCzrPgyVJktSxTtbJ6UW7BJ0vBc6LiD8DdwB7tdvRBJ2SJNXDkZwuzZKg82jg4MzckSJv1Ynt9jdBpyRJ6kWVIzntEnR+BXh0Zl5WbvM54KvzOfg1hx46rWzpGWfMs6mDMwoRRINqz7BdB3Wn0+/2KPwMjCM/l/HiSE6XMvP4zNwxM3cCDgX+C1gObBERu5SbPYNiUrIkaUjMlIRSGjVVz8m5nzKr+cuAL0TEBPA74MV1tkGSJN3fsK14HBGnApPTXpaWZVtT3AHaCbgeeH5m/m6241QaXTUpMy/KzEPK5+dk5mMzc9fM3Dczf1FHGyRJ0sj4BHDglLLjgAsz85HAheXrWdU6kiNJkobPsM3JycyLI2KnKcXLgX3L56cBFwFvmu04tYzkSJIkQbFETESsanms6HDX7TJzTfn8VmC7uXZwJEeSJNUmM1cCK3s8RkbEnMNPlXZyIuJ64E5gPbAuM5fNZ+JQu5n+nYaLr95772lliy+9tKN962BIpvpl2EJ+60jiqmr4mYyfYbtdNYNfRcT2mbkmIrYHbptrhzpuV+2Xmbtl5rLyddcThyRJ0tj7EnBE+fwI4Ny5dhjE7aquJw5JkqTqDNtITkR8lqKvsE1E3AS8DTgBODMiXgLcADx/ruNU3clJ4OvlfbP/KO/DdTRxqDV31aLdXsMWOx1UcVMlSdIwyMwXzPDW/t0cp+pOzpMz8+aI2Ba4ICJ+1PrmbBOHWicmPfI55w9XF1OSpAYZtpGcfql0Tk5m3lz+extwDrAn5cQhgE4nDkmSJHWrspGciHggsCAz7yyfHwC8k/smDp1AhxOHepnpP0yRVMNm2KJxOjGKba5Lp8kvZ9p22A17pKSq48999YYtrUO/VHm7ajvgnIiYrOczmfnViPguXU4ckiRJ6lZlnZwyJ9Wubcp/Q5cThyRJUnWckyNJkjRCTOsgSdKYcyRHkiRphDiSM8ZGMTqh0zY3KaqoF00633aRVEZcjYcmfY+HVVNHcgaRoPN9wLOAu4GfAy/KzNurbIckSRo/dYzk7JeZv255fQFwfGaui4j3AMdj7ipJkgamqSM5tc/JycyvZ+bkukPfAXasuw2SJKn5qu7kTCbovKJMuDnVi4Hz2+0YESsiYlVErFq79uJKGylJkpqn9gSdmXkxQES8hWIl6dPb7diaoHPZspXNHEeTJGkImNZhHloTdEbEZILOiyPiSOAQYP/M+m8EmgdlsOq4/qPwefo97F27SKpOr2vTI/D8fkkDSNAZEQcCxwL7ZOafqqpfkiR1pqkTjweRoPNnwMYUt68AvpOZr6iwHZIkaQwNIkHnI6qqU5Ikda+pIzmmdZAkSY1kWgdJksZcU0dyGt/JGcUIg1Fs8yjr5Xr3sq+faTU6jaRq+vVv+vlJnag9d1XLe8cA7wcWTUn7IEmSauRIzvxNzV1FRCymCCm/sYb6JUnSGBrU7aoPUKyVc+6A6pckSaWmrnhce+6qiFgO3JyZV8+2o7mrJElSL2rPXQW8meJW1azMXSVJUj2ckzMPbXJX7QPsDFxdrna8I3BlROyZmbdW0YZRjDDotc3DHkkyTG2B3tozbOei9vycpPFUe+6qzNy2ZZvrgWVGV0mSNDiO5HSvbe6qCuuTJEm6V+25q6Zss1NV9UuSpPHW+BWPJUnS7Jp6u8oEnZIkqZEcyZEkacw1dSRnILmrIuI1wKvK8q9k5rFVtqOfBhWe3a7emRguO/P18tpoLsO+BMMo8BpqWNSeuyoi9gOWA7tm5l3lQoGSJGlATOvQP68ETsjMu6BYKHAAbZAkSQ1Xe+4qYBfgKRFxWUR8KyL2aLejuaskSarHuszaHnUaRO6qDYCtgb2APYAzI+Jhmfc/c3NXSZKkXtSdu2pP4Cbg7LJTc3lETADbAGurbIskSWrP6KouzZS7CvgDsB/wzYjYBdgIGJncVYOKEDAyoTteL81Xu+/OFW94+rSy3U/8Rh3NGUn+/GlY1J67KiI2Ak6NiGuAu4Ejpt6qkiRJ9XEkp0sz5a7KzLuBF1ZVryRJErjisSRJY6+pIznmrpIkSY3kSI4kSWOuqSse28npg6bnaWn6+fXbqF6vTvOjDSpX2yCvYbtIqmFro+phXrzRUnuCzojYDTgZ2ISi83hUZl5eZTskSdL4qT1BJ/Be4B2ZeX5EHFy+3reGdkiSpDaceNw/CWxePt8CuGUAbZAkSQ1X9UjOZILOBP6jzEf1euBrEfF+ik7Wk9rtWCb0XAGwZMlhLFr01IqbKknSeHIkZ36enJlPAA4CXhURTwVeCRydmYuBo4FT2u2YmSszc1lmLrODI0mSujWIBJ1HAK8rNzkL+Nhcxxn2KIZhaksV2p3fsH8mgzRM16GbSJBhavcwtWUm7dp4zaGHTitbesYZdTRn7Azqd9AofDfnw5GcLkXEAyNis8nnFAk6r6GYg7NPudnTgJ9W1QZJkjS+BpGg8w/ASRGxAfAXynk3kiRpMJo6kjOIBJ2XALtXVa8kSRK44rEkSWOvqWkdTNApSZIaaSRGcpo6m32UDeozMaqrO91cm9V77z2tbPGll/azOY03TJFUg/xZqaNuf+77q6lzciodyYmILSPi8xHxo4i4LiKeGBFbR8QFEfHT8t+tqmyDJEkaT1XfrjoJ+GpmPppiEvJ1wHHAhZn5SODC8rUkSRqQdZm1PepU5To5WwBPpVzRODPvzszbgeXAaeVmpwHPrqoNkiRpfFU5krMzsBb4eER8LyI+Vi4KuF1mrim3uZViPZ1pImJFRKyKiFVr115cYTMlSRpvjuR0bwPgCcBHMvPxwB+ZcmsqM5Miiec05q6SJEm9qLKTcxNwU2ZeVr7+PEWn51cRsT1A+e9tFbZBkiSNqSpXPL41IlZHxKMy88fA/sC15eMI4ITy33OrakM3DE0eDZ1+Jt0kplTBcPFmGeR33Z+z0dPUEPKq18l5DXB6RGwE/AJ4EcXo0ZkR8RLgBuD5FbdBkiSNoUo7OZl5FbCszVv7V1mvJEnqnGkdJEmSRshIpHWQJEnVaeqcHEdyJElSI1U6khMRWwIfA5ZSrIfzYuB/A88C7gZ+DryoXAl5oIwGaBY/T0nqnCM589Mud9UFwNLMfBzwE+D4itsgSZLGUGUjOS25q46EIncVxejN11s2+w7w3KraIEmS5uZITvdmyl3V6sXA+e12NneVJEnqxcByV0XEWyhC809vt7O5qyRJqocJOrs3U+4qIuJI4BDgsDJJpyRJUl/VnrsqIg4EjgX2ycw/VVW/pGqY560+XmvVpakrHg8id9V3gY2BCyIC4DuZ+YqK2yFJksbMIHJXPaLKOiVJUneMrpIkSRohdnIkSVIjmaBTkqQx19TbVXZyKmJUhJrK73F9vNZSbyq9XRURW0bE5yPiRxFxXUQ8seW9YyIiI2KbKtsgSZJm19TFAKseyZlM0PncMox8U4CIWAwcANxYcf2SJGlMDSJBJ8AHKBYEPLeq+iVJUmeaOien9gSdEbEcuDkzr55tZxN0SpKkXlR5u2oyQedrMvOyiDgJeDvF6M4Bc+2cmSuBlQDLlq1sZhdTkqQhYFqH7rVL0Pl2ihGeq8uUDjsCV0bEnpl5a4Vtqd04RkUYUaa5rN5772lliy+9dAAtaRZ/9qT26k7QeWVm7j+5TURcDyzLzF9X1Q5JkjS7ps7JGUSCTkmSpMoNIkFn6/s7VVm/JEmaW1NHcsxdJUmSGsm0DpIkjbmmjuTYyRkR7aInYLgiKIapLRpORlJVo93PnpFsUsWdnIjYEvgYsBRI4MWZ+e2IeA3wKmA98JXMPLbKdjSVnQpJUj9kbjjoJlSi9txVEbEfsBzYNTPviohtK26DJEkaQ7XnroqIVwInZOZdZfltVbVBkiSNr9pzVwG7AE+JiMsi4lsRsUe7nc1dJUlSTSY2qu9Royo7OZO5qz6SmY8H/ggcV5ZvDewF/BNwZpQ5Hlpl5srMXJaZyxYtemqFzZQkSU1Ud+6q48ryszMzgcsjYgLYhmLUR5Ik1a3mEZa61J276lrg58B+wDcjYhdgI8DcVXMwkkr91GlCx2FK/DhMbRkF7cLFDSvXuBlE7qo/AqdGxDXA3cAR5aiOJEkaBEdyujdL7qoXVlmvJEmSKx5LkjTuGjqSY4JOSZLUSI7kSJI07ho6klN77irgz8DJwCbAOuCozLy8ynZIur9Oo5KGKXppmNoyqoyk0ripPXcVcCbwjsw8PyIOBt4L7FtxOyRJ0kwcyenOLLmrEti83GwL4Jaq2iBJksZXlSM5rbmrdgWuAF4HvB74WkS8n2Li85Pa7RwRK4AVAEuWHIapHSRJqsiQjeRExPXAncB6YF1mtluOZk6DyF31SuDozFwMHA2c0m5nc1dJkjTW9svM3ebbwYHB5K56MsWIDsBZFBOTJUnSoAzZSE6/VDaSk5m3Aqsj4lFl0WTuqluAfcqypwE/raoNkiRpuETEiohY1fJY0WazBL4eEVfM8H5HBpG76lzgpIjYAPgL5bwbSZLUfJm5Elg5x2ZPzsybI2Jb4IKI+FFmXtxtXYPIXXUJsHuV9UqSpC4M2e2qzLy5/Pe2iDgH2BPoupNjWgdJkjQ0IuKBEbHZ5HPgAOCa+RzLtA6SJI279UM1krMdcE5EQNFP+UxmfnU+B7KTI0mShkZm/gLYtR/HspMjSerI7xcvnlZmTrGGyKEayembyubkRMSjIuKqlscdEfH6iNg6Ii6IiJ+W/25VVRskSdL4qmwkJzN/DOwGEBELgZuBcygWBLwwM0+IiOPK12+qqh2SJGkOQxZd1S91RVftD/w8M28AlgOnleWnAc+uqQ2SJGmM1DUn51Dgs+Xz7TJzTfn8VopZ1NOYoFOSpJo4kjM/5WrHf0eRp+p+MjMplm6exgSdkiSpF3WM5BwEXJmZvypf/yoits/MNRGxPXBbDW2QJPVomCKpjPTqM0dy5u0F3HerCuBLwBHl8yMocllJkiT1VaUjOeVyzM8AXt5SfAJwZkS8BLgBeH6VbZAkSXNo6EhO1Qk6/wg8eErZbyiirSRJkirjiseSJI27ho7kmIVckiQ1kiM5kqSRYyRVnzV0JKeyTk5EPAr4XEvRw4C3AjsAzwLuBn4OvCgzb6+qHZIkaTxVdrsqM3+cmbtl5m7A7sCfKHJXXQAszczHAT8Bjq+qDZIkaXzVdbuqNXfVDS3l3wGeW1MbJElSO+ubebuqronHrbmrWr0YOL/dDhGxIiJWRcSqtWsvrrRxkiSpeSofyWnJXXX8lPK3AOuA09vtl5krgZUAy5atbJvfSpIk9UE2cyRnELmriIgjgUOA/csknZIkSX1VRyfnfrmrIuJA4Fhgn8z8Uw31N0KvyehMZiepHX83CGhsCHmlc3Jacled3VL8b8BmwAURcVVEnFxlGyRJ0ngaRO6qR1RZpyRJ6pIjOZIkSaPDtA6SJI0718mRJEkaHbXnrsrMD5bvHwO8H1iUmb+uqh1N0Wu0g9ESGhdGC3Vn2K6Nn9+ANHROTmWdnMz8MbAbQEQsBG6myF1FRCwGDgBurKp+SZI03gaRuwrgAxRr5ZxbU/2SJGkmDV3xuPbcVRGxHLg5M6+ebQdzV0mSpF7UmrsqIjYF3kxxq2pW5q6SJKkm6xcOugWVqGMkpzV31cOBnYGrI+J6YEfgyoh4SA3tkCRJY6TW3FWZ+QNg28k3yo7OMqOr5qddFAIYiaDx5vd/tPn5qZ8q7eS05K56eZX1SJKk+VswMVFjbfXdGqs9d9WU93eqsn5JkjS+TOsgSdKYi/Xra6ytvpEc0zpIkqRGciRHkqQxV+9ITn3s5IwwoxAkdWMc80KN4znrPpXdroqIR0XEVS2POyLi9eV7r4mIH0XEDyPivVW1QZIkzW3BxERtjzrVnqAzIvYDlgO7ZuZdEbHtLIeRJEmal9oTdEbE+4ATMvMugMy8raY2SJKkNpo6J6f2BJ3ALsBTIuKyiPhWROzRbgcTdEqSpF7UmqCzpc6tgb2APYAzI+JhmXm/JJwm6JQkqR5NHcmp43ZVa4JOgJuAs8tOzeURMQFsA6ytoS0jywgBSb1q0u+MTn8nNumc1b1aE3SWvgjsB3wzInYBNgJM0ClJ0oDUHfVUl0rn5LQk6Dy7pfhU4GERcQ1wBnDE1FtVkiRJvao9fCLFWwAAFUxJREFUQWdm3g28sMp6JUlS55o6J8fcVZIkqZHs5EiSpEYyd5UkSWOuqberKuvkRMSjgM+1FD0MeCtwEXAysAmwDjgqMy+vqh3tjGI49rC3T/cZxe+XhlMv36Vh+x72uz3+TKkTteeuAj4KvCMzz4+Ig4H3AvtW1Q5JkjQ7Q8h7c2/uKiCBzcvyLYBbamqDJEkaI3XNyWnNXfV64GsR8X6KTtaT2u0QESuAFQBLlhzGokVPraOdkiSNnabOyal8JKcld9VZZdErgaMz///27jxcjqrM4/j3RxIgAdk0KEsUlH1QEa8MgiwGF8B5BGZgQESUZVAcBOLoKMMMm+OMisqMOsrEsCmIgqDiBnFDAo+ENYEETEATWYLCqIDsJnnnj3OuKW6qqqsu9L3pvr/P8/STvtX19jlVOX369KnljSnANOCcsriImB4RAxEx4AGOmZmZtTUauaveDZyQn18KzBiBOpiZmVmFfp3JGY3cVUuAPUhXWU0F7hqBOjyLz8q3bnL7sudLP119NFr1uXfXXVdaNuW660ahJjYaujrIKeSuem9h8T8A/y1pPPAU+bwbMzMzGx39enXVaOSuuhZ4bTfLNTMzM/Mdj83MzMa4fj0nx7mrzMzMrC95JsfMzGyM69eZnG6feDwNOJp0l+PbgSOAjYCvk87VuRl4V0Q80816mNnIW9VyJ9nYVHYlldvm2NG1w1WSNgGOBwYiYntgHOnOx58EzoqILYA/Akd1qw5mZmbW2WrLl4/YY0S3q8vvPx6YmC8XnwQ8QLo3zjfz6xcA+3e5DmZmZjYGdW2QExH3A58G7iENbh4hHZ56OCKW5tXuAzYpi5d0jKSbJN300EPXdKuaZmZm1qe6dk6OpPWB/YDNgYdJKRz2bhofEdOB6QADA9OjG3U0MzOz/j3xuJuHq94ELIqIhyLiz8DlwK7AevnwFcCmwP1drIOZmZmNUd28uuoeYGdJk4Angb2Am4CfAQeSrrB6N/CdLtbBzEaJr1axVVVZ2xzrV1x5JqeliJhNOsH4FtLl46uRDj99BPigpLtJl5Gf0606mJmZ2djV7dxVpwKnDln8a2CnbpZrZmZmzfVrgk6ndTAzM7O+5LQOZmZmY5zPyTEzMzPrIZ7JMTOzMa/pFVdV6/Y6z+QMg6RpkuZLmifpYklrSrpI0oK87FxJE7pZBzMzs+dLPw5w+tloJOi8CNgGeCUwkZSl3MzMzEZJvybo7PbhqsEEnX8mJehcEhEzB1+UdAPprsdmZmZmz6sRTdA5ZIAzAXgXcGVZvBN0mpmZjQwtWzZij5HUzcNVxQSdGwNrSTqssMoXgWsiYlZZfERMj4iBiBiYPHn3blXTzMzM+lQ3D1f9JUEngKTLgV2ACyWdCkwG3tvF8s3MzIZtLJ1k3K9XV414gk5JRwNvBfaKiP68j7SZmZmNuq4NciJitqTBBJ1LgVtJCTofB34D/EISwOURcUa36mFmZmZj02gk6PQNCM3MzFYhTtBpZmZm1kM8q2JmZjbG9euJx57JMTMzs77U1ZkcSdNIaRsCuB04IiKeyq99DjgyItbuZh1GQlkSt7F06aGZmfU2z+S0VJO7CkkDwPrdKtvMzMxsxHNXSRoHnAkcChzQ5fLNzMysA19d1VJN7qrjgCsi4oG6eOeuMjMzs+eiazM5Q3JXPQxcKulw4CBgz07xETGddPNABgamR7fqaWZmNtb16zk5I5276nRgInB3vtvxJEl3R8QWXayHmZmZjUEjnbvqsxHx+cEVJD3WDwMcX0llZma9rF9ncrp5Ts5sYDB31e25rOndKs/MzMysaDRyVxVf7/l75JiZmfU6X11lZmZm1kOcu8rMzGyM8zk5ZmZmZj3EMzn2LM7DZWZm/WLEE3QCTwP/Trop4DLgSxHxuW7Ww8zMzKr16+Gqbt7xeDBB53YR8aSkS0gJOgVMAbaJiOWSNuxWHczMzGzsGvEEnaRZnEMjYjlARDzY5TqYmZlZDV9C3lJNgs5XAAfn5Js/lLRlWbwTdJqZmdlz0bVBzpAEnRsDa0k6DFgDeCoiBoAvA+eWxUfE9IgYiIiByZN371Y1zczMxjwtWzZij5E00gk6dwHuAy7P63wLOK+LdbCWfCWVjTRf0Wdm3TLSCTpvAh4F3ggsAvYAFnaxDmZmZtaBr65qKSJmSxpM0LkUuJWUoHMicFG+vPwx0iXmZmZmZs+r0UjQ+TTwtm6Wa2ZmZs356iozMzOzESBpb0kLJN0t6aPDfR+ndTAzMxvjVqVzciSNA/4HeDPpYqUbJV0REXe0fS/P5JiZmdmqZCfg7oj4dUQ8A3yddEua9iKiZx7AMaMV34uxvVpv7y9v86oa26v19jaPjf3VKw/gGNLV1oOPY4a8fiAwo/D3u4AvDKesXpvJOWYU43sxdjTL7sXY0Szb29wbsaNZtre5N2JHu+xVXhRu9psf07tVVq8NcszMzKy/3U9K5D1o07ysNQ9yzMzMbFVyI7ClpM0lrQ4cAlwxnDfqtaurnuuU1nOJ78XY0Sy7F2NHs2xvc2/EjmbZ3ubeiB3tsnteRCyVdBxwFTAOODci5g/nvZRP6jEzMzPrKz5cZWZmZn3JgxwzMzPrSz0zyBnuLZ4lrSnpBklzJc2XdHrLcteT9E1Jv5R0p6TXt4w/QdK8XPaJHdY9V9KDkuYVlp2Zy75N0rckrdci9jRJ90uakx/7tix7B0nX59ibJO1UEjdF0s8k3ZG38YS8/KD893JJAzXllsYXXv8nSSHpRS3K/kZhmxdLmlMSW9ou8olus3M7+0Y+6a1p7Dl52W25zaxdsc1V8ZL0cUkLc1s7vkXsVEm35LZ2gaTK8+0kjZN0q6Tv5b8vyp+tebkdTGgRe76kRYX9vUOL2L1ynedIulbSFjWxiyXdPtgW87KmbWyl2MJrle2rptyO7Suvt1LfIWkDST+SdFf+d/2aepfFfyy3rzmSZkrauGlsXv6BvGy+pE+1KPfVkn6R98V3Ja1TErd1Yb/MkfSopBPVoA+riW3Uh9XEd+zDcvy0vE/mSbpY6XN2nFI/UNk+qmILr31O0mNtYiXNKmzHEknfroq3Bkb7pkANbxw0DvgV8HJgdWAusF3DWAFr5+cTgNnAzi3KvgA4Oj9fHVivRez2wDxgEukk7x8DW9SsvzuwIzCvsOwtwPj8/JPAJ1vEngZ8qGFdy+JnAvvk5/sCV5fEbQTsmJ+/AFgIbAdsC2wNXA0M1JRbGp//nkI68ew3wIvaxBbW+QxwStN2AVwCHJKXnw0c2yJ2ncI6nwU+2qZNAkcAXwFWy69t2DB2F+BeYKu8/AzgqJp9/kHga8D3Cv+3yo+Ly7a5JvZ84MCGbWxo7EJg2/z8/cD5NbGLh7aBFm1spdgm7asutlP7yq+t1HcAnxpsF8BHqfg818QX29jxwNktYt9I6oPWqGpfNbE3AnvkZUcCH+vwfz0O+C3wMhr2YRWxp9GwD6uIb9KHbQIsAibmvy8B3gO8Btisrg1UxebnA8BXgcfaxhbWuQw4vM32+/HsR6/M5Az7Fs+RDI6kJ+RHo7OtJa1L+vI/J7/XMxHxcIt6bwvMjognImIp8HPgb2vqeg3whyHLZuZYgOtJ9wtoFNtGRXwAg7/Y1gWWlMQ9EBG35Od/Au4ENomIOyNiQYNyS+Pzy2cB/0zF/1eHWCQJ+HvSF/fQ2Kp2MRX4Zl5+AbB/09iIeLRQ7sSaeleVfSxwRkQsz+s92DB2GfBMRCzMy38E/F1Z2ZI2Bd4GzCi85w/y+wZwAxVtrCy2qYrYju2rTtM2VqO2fXVS175q+o79SO0KKtpXXfxgG8vWKqt7TdnHAp+IiKfz8pXaV03sVsA1ebXK9lWwF/CriPhN0z6sLLbDek3im7ax8cBEpRnQScCSiLg1IhY3KG+lWKXcS2eS2ler2MEX8mzZVMAzOc9BrwxyNiH9Uh10H4Uvs06UpsnnAA8CP4qI2Q1DNwceAs5TmmafIWmtpuWSZnF2k/RCSZNIvySmdIipcyTww5Yxx+Vp4nPrpsYrnAicKele4NPASXUrS9qM9Oun6f6tjJe0H3B/RMxtG1tYvBvwu4i4qyLmWe2CNFv4cKFDrmxnVW1K0nmkX5HbAJ+vqW9Z/CuAg/O0+g8lbdmw3jcA47XikM2BVLez/yJ1vMtL3ncC6fbpV7aM/XhuY2dJWqNF7NHADyTdl8v9REUspC+rmZJultT2jrArxbZoX3Xl1rWvqr7jxRHxQF7nt8CLK8qt7HuUDmneC7wTOKVF7Fak/mi2pJ9Lel2L2Pms+GF5EJ37sUMoGfzRrA8bGtu2DyvGd+zDIuL+/No9wAPAIxExs0E5dbHHAVcU/q/bxA7aH/jJkIGttdQrg5znJCKWRcQOpF8QO0navmHoeNIhnC9FxGuAx0lTzE3LvZM0PTuT9MUxh/SruzVJJwNLgYtahH2J9MW5A+lD9JmWxR4LTIuIKcA08q+7ivqtTZpaPXE4H8piPGk7/4XyDrxN2e+gvKMFVm4XpIFJI1VtKiKOADYmzSod3DJ+DeCpiBgAvgyc27Def0Xq2M+SdAPwJ0ramaS/AR6MiJsrqvVF4JqImNUi9iTSfnsdsAHwkRax04B9I2JT4DzSIb4qb4iIHYF9gH+UtHvNuk1im7avunLr2lfHviPPnFXNIlXGR8TJ+TN5EenLtGnseNL/0c7Ah4FL8mxUk9gjgfdLupl0aPiZinqjdB7b24FLhyzv2IeVxLbqw0riO/ZheeC0H2mAtzGwlqTD6srpEHs4aSBY+SOnYbm1/Zc1FKvAMbNOD+D1wFWFv08CThrme51C8/NUXgIsLvy9G/D957Ad/wG8v8M6m1E4LyYvew/wC2BS29gmr1WtAzzCinspCXi0Im4C6dyGD5a8djU150uUxQOvJM1SLM6PpaRfOy9pWjaps/4dsGmLdvFh4P9Ycf7As9pdmzZFmvL/Xps2CfwS2Lywvx8ZZtlvAS4pWfc/SbNTi0mzCE8AF+bXTiVNi69WUU5lbGGdPcu2uSL2+6RDCoPrvBS4o+H+Oq24zU3a2JDYf2vavqrK7dS+qOg7gAXARnnZRsCCNvFD1nkpJZ/pmrKvBN5YWP4rYPIwyt0KuKFmP+0HzByy7D0068NWii28tlnZ9tbF06APIw1Izin8fTjwxcLfi6k+J6csdlFu54PtaznpdIvG5QIvAn4PrNmkXftR/eiVmZxh3+JZ0mTls/klTQTeTPoy6SgifgvcK2nrvGgv4I42FZe0Yf73paTzcb7WMn5v0jT/2yPiiZaxGxX+PIB0+KyNJcAe+flUYKVp+fxL8Bzgzoio+yVeVceV4iPi9ojYMCI2i4jNSF+QO+b/j6Zlvwn4ZUTcV1FuWbu4E/gZ6XAPwLuB7zSMXaB8dVCu19upaGc1bfLbpJNDIe33hU1jC+1sDdJsytlDYyPipIjYNO/TQ4CfRsRhko4G3gq8I/L5QC1iNyps8/6UtLGyWNKX0bqStsqrDe7/sv21lqQXDD4nDeIateWK2Bsbtq+6cmvbV03fcQWpXUFF+6qLH3IIcz9K2lhN2X9pX3m/r04a1Dcpd7B9rQb8KyXtq+BZMxAt+7ChsW37sKGzHx37MNIAd2dJk3I73ouKttgw9rMR8ZJC+3oiIsquHKwr90DSD4anGtbDqoz2KKvpg3Q+y0LSr4+TW8S9CrgVuI30ASm9EqImfgdSKvjbSJ3E+i3jZ5E6mLnAXh3WvZg0JftnUsd7FHA36XykOflRdTVFWexXgdtz3a8g/4JsEf8G4OZc99nAa0vi3kCacr+tUMd9SR3SfcDTpF+8pTMiVfFD1llM+dUxlbGkq37e17ZdkK7guyHv90vJV6J0iiUd+r0u7+95pCn5dVqWvR7pF/ftpF+9r24Reyapg1xAOmzXqV3uyYqrnJaSPleD+7D2MzIk9qeFbb6QfOVXw9gDcuxc0mzMyytiXp7XmUs6N+TkQnxtG6uKbdi+KmM7ta+qvgN4IfAT0pftj4ENWsZflvf1bcB3SSf5N41dPf8fzQNuAaa2iD2B1P8uJJ07pYrYtUgzEOsWljXtw8pi2/RhZfEd+7C83umkAeO8XOYapKvX7iN9PpYAM5rGDnm99OqquljS52HvTp9jPzo/nNbBzMzM+lKvHK4yMzMza8WDHDMzM+tLHuSYmZlZX/Igx8zMzPqSBzlmZmbWlzzIMVuFSFqmlH14nqRLldKBDPe9zpd0YH4+Q9J2NevuKWmXYZSxWOUZ4kuXD1mnMjtzxfqnSfpQ2zqa2djlQY7ZquXJiNghIrYn3Tr/fcUXlRL5tRYRR0dE3Y0s9yRlMzcz6xse5JitumYBW+RZllmSriDdfXacpDMl3ZgTF74X0l2HJX1B0gJJPwY2HHwjSVcrJ/CUtLekWyTNlfQTpeSm7wOm5Vmk3fKdlS/LZdwoadcc+0JJMyXNlzSDdKv8WpK+rZTgcr6GJLlUSuo5P9djcl72CklX5phZkhrnFDMzKxrWr0Iz6648Y7MPKzKC7whsHxGL8kDhkYh4XU7jcJ2kmaQs7FsD25GyW9/BkCSfeSDxZWD3/F4bRMQfJJ1NujPrp/N6XwPOiohrlVKSXAVsS8pxdW1EnCHpbaQ7Y3dyZC5jInCjpMsi4vekO9TeFBHTJJ2S3/s4YDrpbsJ3SfprUuLQqcPYjWY2xnmQY7ZqmShpTn4+i5SbaxdSQsRFeflbgFcNnm8DrAtsSUoKenFELAOWSPppyfvvTMo0vgggIv5QUY83AdtpRZLqdZSyve9OysFGRHxf0h8bbNPxkg7Iz6fkuv6elLjwG3n5hcDluYxdgEsLZa/RoAwzs5V4kGO2ankyInYoLshf9o8XFwEfiIirhqy37/NYj9WAnWNIgsDCwKMRSXuSBkyvj4gnJF0NrFmxeuRyHx66D8zMhsPn5Jj1nquAYyVNgJRROmfJvgY4OJ+zsxErMpoXXQ/sLmnzHLtBXv4n4AWF9WYCHxj8Q9LgoOMa4NC8bB9S8sY66wJ/zAOcbUgzSYNWY0XG90NJh8EeBRZJOiiXIUmv7lCGmVkpD3LMes8M0vk2t0iaB/wvaVb2W6Ts1ncAXyFlMn+WiHgIOIZ0aGguKw4XfRc4YPDEY1IG5oF8YvMdrLjK63TSIGk+6bDVPR3qeiUwXtKdpOzV1xdeexzYKW/DVOCMvPydwFG5fvOB/RrsEzOzlTgLuZmZmfUlz+SYmZlZX/Igx8zMzPqSBzlmZmbWlzzIMTMzs77kQY6ZmZn1JQ9yzMzMrC95kGNmZmZ96f8Bm10H1CpsiAEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGxWw4fyzyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}