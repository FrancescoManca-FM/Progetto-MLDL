{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_icarl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/exemplars/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "outputId": "18838cf3-285f-43b5-9c77-5c8941749398",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "outputId": "83cc7227-07bd-4dc0-f5a5-d44adf9fe7d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cc734f05-ec7c-4d2d-ee4a-d5179bb5594a"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e3da978a-9372-49e0-a444-fb844d5aea54"
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "#!rm -r $DATA_DIR\n",
        "#!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone -b exemplars https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 235, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/235)\u001b[K\rremote: Counting objects:   1% (3/235)\u001b[K\rremote: Counting objects:   2% (5/235)\u001b[K\rremote: Counting objects:   3% (8/235)\u001b[K\rremote: Counting objects:   4% (10/235)\u001b[K\rremote: Counting objects:   5% (12/235)\u001b[K\rremote: Counting objects:   6% (15/235)\u001b[K\rremote: Counting objects:   7% (17/235)\u001b[K\rremote: Counting objects:   8% (19/235)\u001b[K\rremote: Counting objects:   9% (22/235)\u001b[K\rremote: Counting objects:  10% (24/235)\u001b[K\rremote: Counting objects:  11% (26/235)\u001b[K\rremote: Counting objects:  12% (29/235)\u001b[K\rremote: Counting objects:  13% (31/235)\u001b[K\rremote: Counting objects:  14% (33/235)\u001b[K\rremote: Counting objects:  15% (36/235)\u001b[K\rremote: Counting objects:  16% (38/235)\u001b[K\rremote: Counting objects:  17% (40/235)\u001b[K\rremote: Counting objects:  18% (43/235)\u001b[K\rremote: Counting objects:  19% (45/235)\u001b[K\rremote: Counting objects:  20% (47/235)\u001b[K\rremote: Counting objects:  21% (50/235)\u001b[K\rremote: Counting objects:  22% (52/235)\u001b[K\rremote: Counting objects:  23% (55/235)\u001b[K\rremote: Counting objects:  24% (57/235)\u001b[K\rremote: Counting objects:  25% (59/235)\u001b[K\rremote: Counting objects:  26% (62/235)\u001b[K\rremote: Counting objects:  27% (64/235)\u001b[K\rremote: Counting objects:  28% (66/235)\u001b[K\rremote: Counting objects:  29% (69/235)\u001b[K\rremote: Counting objects:  30% (71/235)\u001b[K\rremote: Counting objects:  31% (73/235)\u001b[K\rremote: Counting objects:  32% (76/235)\u001b[K\rremote: Counting objects:  33% (78/235)\u001b[K\rremote: Counting objects:  34% (80/235)\u001b[K\rremote: Counting objects:  35% (83/235)\u001b[K\rremote: Counting objects:  36% (85/235)\u001b[K\rremote: Counting objects:  37% (87/235)\u001b[K\rremote: Counting objects:  38% (90/235)\u001b[K\rremote: Counting objects:  39% (92/235)\u001b[K\rremote: Counting objects:  40% (94/235)\u001b[K\rremote: Counting objects:  41% (97/235)\u001b[K\rremote: Counting objects:  42% (99/235)\u001b[K\rremote: Counting objects:  43% (102/235)\u001b[K\rremote: Counting objects:  44% (104/235)\u001b[K\rremote: Counting objects:  45% (106/235)\u001b[K\rremote: Counting objects:  46% (109/235)\u001b[K\rremote: Counting objects:  47% (111/235)\u001b[K\rremote: Counting objects:  48% (113/235)\u001b[K\rremote: Counting objects:  49% (116/235)\u001b[K\rremote: Counting objects:  50% (118/235)\u001b[K\rremote: Counting objects:  51% (120/235)\u001b[K\rremote: Counting objects:  52% (123/235)\u001b[K\rremote: Counting objects:  53% (125/235)\u001b[K\rremote: Counting objects:  54% (127/235)\u001b[K\rremote: Counting objects:  55% (130/235)\u001b[K\rremote: Counting objects:  56% (132/235)\u001b[K\rremote: Counting objects:  57% (134/235)\u001b[K\rremote: Counting objects:  58% (137/235)\u001b[K\rremote: Counting objects:  59% (139/235)\u001b[K\rremote: Counting objects:  60% (141/235)\u001b[K\rremote: Counting objects:  61% (144/235)\u001b[K\rremote: Counting objects:  62% (146/235)\u001b[K\rremote: Counting objects:  63% (149/235)\u001b[K\rremote: Counting objects:  64% (151/235)\u001b[K\rremote: Counting objects:  65% (153/235)\u001b[K\rremote: Counting objects:  66% (156/235)\u001b[K\rremote: Counting objects:  67% (158/235)\u001b[K\rremote: Counting objects:  68% (160/235)\u001b[K\rremote: Counting objects:  69% (163/235)\u001b[K\rremote: Counting objects:  70% (165/235)\u001b[K\rremote: Counting objects:  71% (167/235)\u001b[K\rremote: Counting objects:  72% (170/235)\u001b[K\rremote: Counting objects:  73% (172/235)\u001b[K\rremote: Counting objects:  74% (174/235)\u001b[K\rremote: Counting objects:  75% (177/235)\u001b[K\rremote: Counting objects:  76% (179/235)\u001b[K\rremote: Counting objects:  77% (181/235)\u001b[K\rremote: Counting objects:  78% (184/235)\u001b[K\rremote: Counting objects:  79% (186/235)\u001b[K\rremote: Counting objects:  80% (188/235)\u001b[K\rremote: Counting objects:  81% (191/235)\u001b[K\rremote: Counting objects:  82% (193/235)\u001b[K\rremote: Counting objects:  83% (196/235)\u001b[K\rremote: Counting objects:  84% (198/235)\u001b[K\rremote: Counting objects:  85% (200/235)\u001b[K\rremote: Counting objects:  86% (203/235)\u001b[K\rremote: Counting objects:  87% (205/235)\u001b[K\rremote: Counting objects:  88% (207/235)\u001b[K\rremote: Counting objects:  89% (210/235)\u001b[K\rremote: Counting objects:  90% (212/235)\u001b[K\rremote: Counting objects:  91% (214/235)\u001b[K\rremote: Counting objects:  92% (217/235)\u001b[K\rremote: Counting objects:  93% (219/235)\u001b[K\rremote: Counting objects:  94% (221/235)\u001b[K\rremote: Counting objects:  95% (224/235)\u001b[K\rremote: Counting objects:  96% (226/235)\u001b[K\rremote: Counting objects:  97% (228/235)\u001b[K\rremote: Counting objects:  98% (231/235)\u001b[K\rremote: Counting objects:  99% (233/235)\u001b[K\rremote: Counting objects: 100% (235/235)\u001b[K\rremote: Counting objects: 100% (235/235), done.\u001b[K\n",
            "remote: Compressing objects:   0% (1/167)\u001b[K\rremote: Compressing objects:   1% (2/167)\u001b[K\rremote: Compressing objects:   2% (4/167)\u001b[K\rremote: Compressing objects:   3% (6/167)\u001b[K\rremote: Compressing objects:   4% (7/167)\u001b[K\rremote: Compressing objects:   5% (9/167)\u001b[K\rremote: Compressing objects:   6% (11/167)\u001b[K\rremote: Compressing objects:   7% (12/167)\u001b[K\rremote: Compressing objects:   8% (14/167)\u001b[K\rremote: Compressing objects:   9% (16/167)\u001b[K\rremote: Compressing objects:  10% (17/167)\u001b[K\rremote: Compressing objects:  11% (19/167)\u001b[K\rremote: Compressing objects:  12% (21/167)\u001b[K\rremote: Compressing objects:  13% (22/167)\u001b[K\rremote: Compressing objects:  14% (24/167)\u001b[K\rremote: Compressing objects:  15% (26/167)\u001b[K\rremote: Compressing objects:  16% (27/167)\u001b[K\rremote: Compressing objects:  17% (29/167)\u001b[K\rremote: Compressing objects:  18% (31/167)\u001b[K\rremote: Compressing objects:  19% (32/167)\u001b[K\rremote: Compressing objects:  20% (34/167)\u001b[K\rremote: Compressing objects:  21% (36/167)\u001b[K\rremote: Compressing objects:  22% (37/167)\u001b[K\rremote: Compressing objects:  23% (39/167)\u001b[K\rremote: Compressing objects:  24% (41/167)\u001b[K\rremote: Compressing objects:  25% (42/167)\u001b[K\rremote: Compressing objects:  26% (44/167)\u001b[K\rremote: Compressing objects:  27% (46/167)\u001b[K\rremote: Compressing objects:  28% (47/167)\u001b[K\rremote: Compressing objects:  29% (49/167)\u001b[K\rremote: Compressing objects:  30% (51/167)\u001b[K\rremote: Compressing objects:  31% (52/167)\u001b[K\rremote: Compressing objects:  32% (54/167)\u001b[K\rremote: Compressing objects:  33% (56/167)\u001b[K\rremote: Compressing objects:  34% (57/167)\u001b[K\rremote: Compressing objects:  35% (59/167)\u001b[K\rremote: Compressing objects:  36% (61/167)\u001b[K\rremote: Compressing objects:  37% (62/167)\u001b[K\rremote: Compressing objects:  38% (64/167)\u001b[K\rremote: Compressing objects:  39% (66/167)\u001b[K\rremote: Compressing objects:  40% (67/167)\u001b[K\rremote: Compressing objects:  41% (69/167)\u001b[K\rremote: Compressing objects:  42% (71/167)\u001b[K\rremote: Compressing objects:  43% (72/167)\u001b[K\rremote: Compressing objects:  44% (74/167)\u001b[K\rremote: Compressing objects:  45% (76/167)\u001b[K\rremote: Compressing objects:  46% (77/167)\u001b[K\rremote: Compressing objects:  47% (79/167)\u001b[K\rremote: Compressing objects:  48% (81/167)\u001b[K\rremote: Compressing objects:  49% (82/167)\u001b[K\rremote: Compressing objects:  50% (84/167)\u001b[K\rremote: Compressing objects:  51% (86/167)\u001b[K\rremote: Compressing objects:  52% (87/167)\u001b[K\rremote: Compressing objects:  53% (89/167)\u001b[K\rremote: Compressing objects:  54% (91/167)\u001b[K\rremote: Compressing objects:  55% (92/167)\u001b[K\rremote: Compressing objects:  56% (94/167)\u001b[K\rremote: Compressing objects:  57% (96/167)\u001b[K\rremote: Compressing objects:  58% (97/167)\u001b[K\rremote: Compressing objects:  59% (99/167)\u001b[K\rremote: Compressing objects:  60% (101/167)\u001b[K\rremote: Compressing objects:  61% (102/167)\u001b[K\rremote: Compressing objects:  62% (104/167)\u001b[K\rremote: Compressing objects:  63% (106/167)\u001b[K\rremote: Compressing objects:  64% (107/167)\u001b[K\rremote: Compressing objects:  65% (109/167)\u001b[K\rremote: Compressing objects:  66% (111/167)\u001b[K\rremote: Compressing objects:  67% (112/167)\u001b[K\rremote: Compressing objects:  68% (114/167)\u001b[K\rremote: Compressing objects:  69% (116/167)\u001b[K\rremote: Compressing objects:  70% (117/167)\u001b[K\rremote: Compressing objects:  71% (119/167)\u001b[K\rremote: Compressing objects:  72% (121/167)\u001b[K\rremote: Compressing objects:  73% (122/167)\u001b[K\rremote: Compressing objects:  74% (124/167)\u001b[K\rremote: Compressing objects:  75% (126/167)\u001b[K\rremote: Compressing objects:  76% (127/167)\u001b[K\rremote: Compressing objects:  77% (129/167)\u001b[K\rremote: Compressing objects:  78% (131/167)\u001b[K\rremote: Compressing objects:  79% (132/167)\u001b[K\rremote: Compressing objects:  80% (134/167)\u001b[K\rremote: Compressing objects:  81% (136/167)\u001b[K\rremote: Compressing objects:  82% (137/167)\u001b[K\rremote: Compressing objects:  83% (139/167)\u001b[K\rremote: Compressing objects:  84% (141/167)\u001b[K\rremote: Compressing objects:  85% (142/167)\u001b[K\rremote: Compressing objects:  86% (144/167)\u001b[K\rremote: Compressing objects:  87% (146/167)\u001b[K\rremote: Compressing objects:  88% (147/167)\u001b[K\rremote: Compressing objects:  89% (149/167)\u001b[K\rremote: Compressing objects:  90% (151/167)\u001b[K\rremote: Compressing objects:  91% (152/167)\u001b[K\rremote: Compressing objects:  92% (154/167)\u001b[K\rremote: Compressing objects:  93% (156/167)\u001b[K\rremote: Compressing objects:  94% (157/167)\u001b[K\rremote: Compressing objects:  95% (159/167)\u001b[K\rremote: Compressing objects:  96% (161/167)\u001b[K\rremote: Compressing objects:  97% (162/167)\u001b[K\rremote: Compressing objects:  98% (164/167)\u001b[K\rremote: Compressing objects:  99% (166/167)\u001b[K\rremote: Compressing objects: 100% (167/167)\u001b[K\rremote: Compressing objects: 100% (167/167), done.\u001b[K\n",
            "Receiving objects:   0% (1/1214)   \rReceiving objects:   1% (13/1214)   \rReceiving objects:   2% (25/1214)   \rReceiving objects:   3% (37/1214)   \rReceiving objects:   4% (49/1214)   \rReceiving objects:   5% (61/1214)   \rReceiving objects:   6% (73/1214)   \rReceiving objects:   7% (85/1214)   \rReceiving objects:   8% (98/1214)   \rReceiving objects:   9% (110/1214)   \rReceiving objects:  10% (122/1214)   \rReceiving objects:  11% (134/1214)   \rReceiving objects:  12% (146/1214)   \rReceiving objects:  13% (158/1214)   \rReceiving objects:  14% (170/1214)   \rReceiving objects:  15% (183/1214)   \rReceiving objects:  16% (195/1214)   \rReceiving objects:  17% (207/1214)   \rReceiving objects:  18% (219/1214)   \rReceiving objects:  19% (231/1214)   \rReceiving objects:  20% (243/1214)   \rReceiving objects:  21% (255/1214)   \rReceiving objects:  22% (268/1214)   \rReceiving objects:  23% (280/1214)   \rReceiving objects:  24% (292/1214)   \rReceiving objects:  25% (304/1214)   \rReceiving objects:  26% (316/1214)   \rReceiving objects:  27% (328/1214)   \rReceiving objects:  28% (340/1214)   \rReceiving objects:  29% (353/1214)   \rReceiving objects:  30% (365/1214)   \rReceiving objects:  31% (377/1214)   \rReceiving objects:  32% (389/1214)   \rReceiving objects:  33% (401/1214)   \rReceiving objects:  34% (413/1214)   \rReceiving objects:  35% (425/1214)   \rReceiving objects:  36% (438/1214)   \rReceiving objects:  37% (450/1214)   \rReceiving objects:  38% (462/1214)   \rReceiving objects:  39% (474/1214)   \rReceiving objects:  40% (486/1214)   \rReceiving objects:  41% (498/1214)   \rReceiving objects:  42% (510/1214)   \rReceiving objects:  43% (523/1214)   \rReceiving objects:  44% (535/1214)   \rReceiving objects:  45% (547/1214)   \rReceiving objects:  46% (559/1214)   \rReceiving objects:  47% (571/1214)   \rReceiving objects:  48% (583/1214)   \rReceiving objects:  49% (595/1214)   \rReceiving objects:  50% (607/1214)   \rReceiving objects:  51% (620/1214)   \rReceiving objects:  52% (632/1214)   \rReceiving objects:  53% (644/1214)   \rReceiving objects:  54% (656/1214)   \rReceiving objects:  55% (668/1214)   \rReceiving objects:  56% (680/1214)   \rReceiving objects:  57% (692/1214)   \rReceiving objects:  58% (705/1214)   \rReceiving objects:  59% (717/1214)   \rReceiving objects:  60% (729/1214)   \rReceiving objects:  61% (741/1214)   \rReceiving objects:  62% (753/1214)   \rReceiving objects:  63% (765/1214)   \rReceiving objects:  64% (777/1214)   \rReceiving objects:  65% (790/1214)   \rReceiving objects:  66% (802/1214)   \rReceiving objects:  67% (814/1214)   \rReceiving objects:  68% (826/1214)   \rReceiving objects:  69% (838/1214)   \rReceiving objects:  70% (850/1214)   \rReceiving objects:  71% (862/1214)   \rReceiving objects:  72% (875/1214)   \rReceiving objects:  73% (887/1214)   \rReceiving objects:  74% (899/1214)   \rReceiving objects:  75% (911/1214)   \rReceiving objects:  76% (923/1214)   \rReceiving objects:  77% (935/1214)   \rReceiving objects:  78% (947/1214)   \rReceiving objects:  79% (960/1214)   \rReceiving objects:  80% (972/1214)   \rReceiving objects:  81% (984/1214)   \rReceiving objects:  82% (996/1214)   \rReceiving objects:  83% (1008/1214)   \rReceiving objects:  84% (1020/1214)   \rReceiving objects:  85% (1032/1214)   \rReceiving objects:  86% (1045/1214)   \rReceiving objects:  87% (1057/1214)   \rReceiving objects:  88% (1069/1214)   \rReceiving objects:  89% (1081/1214)   \rReceiving objects:  90% (1093/1214)   \rremote: Total 1214 (delta 140), reused 157 (delta 68), pack-reused 979\n",
            "Receiving objects:  91% (1105/1214)   \rReceiving objects:  92% (1117/1214)   \rReceiving objects:  93% (1130/1214)   \rReceiving objects:  94% (1142/1214)   \rReceiving objects:  95% (1154/1214)   \rReceiving objects:  96% (1166/1214)   \rReceiving objects:  97% (1178/1214)   \rReceiving objects:  98% (1190/1214)   \rReceiving objects:  99% (1202/1214)   \rReceiving objects: 100% (1214/1214)   \rReceiving objects: 100% (1214/1214), 6.89 MiB | 21.99 MiB/s, done.\n",
            "Resolving deltas:   0% (0/733)   \rResolving deltas:  33% (244/733)   \rResolving deltas:  35% (261/733)   \rResolving deltas:  36% (270/733)   \rResolving deltas:  37% (275/733)   \rResolving deltas:  38% (281/733)   \rResolving deltas:  39% (289/733)   \rResolving deltas:  57% (419/733)   \rResolving deltas:  65% (480/733)   \rResolving deltas:  66% (491/733)   \rResolving deltas:  67% (492/733)   \rResolving deltas:  73% (538/733)   \rResolving deltas:  75% (556/733)   \rResolving deltas:  76% (560/733)   \rResolving deltas:  78% (579/733)   \rResolving deltas:  79% (583/733)   \rResolving deltas:  80% (588/733)   \rResolving deltas:  86% (637/733)   \rResolving deltas:  90% (665/733)   \rResolving deltas:  92% (678/733)   \rResolving deltas:  93% (682/733)   \rResolving deltas:  95% (700/733)   \rResolving deltas: 100% (733/733)   \rResolving deltas: 100% (733/733), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "71327bc3-fd43-42a1-996f-3b76089da742"
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-28 21:44:57--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz.24’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  72.4MB/s    in 2.2s    \n",
            "\n",
            "2020-05-28 21:45:00 (72.4 MB/s) - ‘cifar-100-python.tar.gz.24’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4dcbbe5a-4223-4f0a-dce5-888ac8d3f483"
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 42, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "aba79231-d68a-4d4f-a040-7a0b0ba8223a"
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NAwyvHxdiGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exemplar_transform = transforms.Compose([\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.Pad(4),\n",
        "                                      transforms.RandomCrop(32),\n",
        "                                      transforms.ToTensor()\n",
        "])\n",
        "\n",
        "#exemplar_transform = train_transform"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55490124-3bbe-4eac-aad6-671b1b6e92d1"
      },
      "source": [
        "from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, exemplar_transform, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPJXeMLt-m_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        preds = net.classify(images)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "  accuracy = correct/len(dataset)\n",
        "  if method == 'test':\n",
        "    all_preds_cm.extend(preds.tolist())\n",
        "    all_labels_cm.extend(labels.data.tolist())\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "\n",
        "    # exemplars new params => embedded in the model\n",
        "    #classes_seen = 0 # number of classes seen so far (useful in the computation of the exemplar sets cardinality)\n",
        "    #tot_num_classes = 100 \n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "        else:\n",
        "          test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(K/icarl.n_classes)\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          #print(type(class_train_subset))\n",
        "          icarl.construct_exemplar_set(class_train_subset,m, eval_transform,y) # why eval? ref: https://github.com/donlee90/icarl/blob/master/main.py\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier ...\n",
        "        icarl.computeMeans()\n",
        "        \n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3b76e48-0e11-45f9-91e2-8711bee698d2"
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.3271869122982025\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.31977811455726624\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.32024985551834106\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.3132197856903076\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.2992495000362396\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.28257399797439575\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.2792026698589325\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.2631855607032776\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.26100656390190125\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.2346944808959961\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.241450235247612\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.21887929737567902\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.22994740307331085\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.2126535028219223\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.24025700986385345\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.21149258315563202\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.20387446880340576\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.1801387369632721\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.1808517724275589\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.19843874871730804\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.1657547652721405\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.17400775849819183\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.1615581214427948\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.15076930820941925\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.16667775809764862\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.1649375706911087\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.15105721354484558\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.1673133820295334\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1550399363040924\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12167244404554367\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.15550479292869568\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.1336732655763626\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.11897450685501099\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.11440562456846237\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12303458899259567\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.12029135227203369\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.13051992654800415\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.13925158977508545\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.11767946928739548\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.10979648679494858\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12505215406417847\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.10823755711317062\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1150217279791832\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.09365735203027725\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12069547176361084\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.11201474815607071\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.10398516803979874\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.10014219582080841\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.11983957141637802\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07080313563346863\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07444585859775543\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07894565910100937\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.056575533002614975\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.06673165410757065\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.04553821682929993\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.04960758239030838\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.0676528587937355\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.05511554703116417\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07123565673828125\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.05985875055193901\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.058562494814395905\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.06259103864431381\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.03484578803181648\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0617099404335022\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.04470429569482803\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.04894183203577995\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.0372171588242054\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.034335650503635406\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.03914864733815193\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.030764415860176086\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 92.56\n",
            "\n",
            "Test Accuracy (all groups seen so far): 79.00\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1413743495941162\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12789872288703918\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.12232059240341187\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.11903482675552368\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12843595445156097\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.10901669412851334\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.11663611978292465\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.10484027862548828\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.11534183472394943\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.11978123337030411\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.09491551667451859\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.09199114143848419\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.10297176986932755\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.09698554873466492\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.10737446695566177\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.0914849042892456\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.07971953600645065\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.10348811000585556\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.09425535053014755\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0923784151673317\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08631948381662369\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08550695329904556\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.0924474224448204\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08420725911855698\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.07766725867986679\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08614128082990646\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.09537720680236816\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08574464917182922\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08560549467802048\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08392471075057983\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07987793534994125\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08532146364450455\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07312730699777603\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07082398980855942\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07227566093206406\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08299025148153305\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08168815821409225\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08018089830875397\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.0770818367600441\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07652100175619125\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.0742710754275322\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08378586918115616\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07540158182382584\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.0747297927737236\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.06728535145521164\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07943516969680786\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07338302582502365\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07408592849969864\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07304389774799347\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.06319833546876907\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.057114481925964355\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.05424037203192711\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.05151546001434326\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.05150851234793663\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.05488966777920723\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.055721916258335114\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.058740951120853424\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.05666368082165718\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.05309896543622017\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.05465755611658096\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.053699444979429245\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.04792517051100731\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.04821118339896202\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0523037426173687\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.05001120641827583\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.05307639390230179\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.04778282344341278\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.051254548132419586\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.04998669773340225\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.04851063713431358\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 86.54\n",
            "\n",
            "Test Accuracy (all groups seen so far): 65.90\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1227630078792572\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.11600825935602188\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.11803193390369415\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.11553303897380829\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.11033209413290024\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.10795342922210693\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.10285912454128265\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.10568249225616455\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.10259877145290375\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.09900662302970886\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.10728636384010315\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.09699125587940216\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.0967336893081665\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.09484244138002396\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.0973191186785698\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.09083999693393707\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08498452603816986\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.09856082499027252\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.09698101878166199\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.1007394790649414\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.09570598602294922\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08515483886003494\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08997120708227158\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08973344415426254\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.0883646160364151\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.09256843477487564\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.09669610112905502\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.0925060510635376\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.0821504071354866\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08348479121923447\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08634711056947708\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08318091183900833\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08189786970615387\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08660447597503662\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08094374090433121\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.0800383985042572\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.0826960951089859\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.09441657364368439\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08624361455440521\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08282237499952316\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.09453611820936203\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08914375305175781\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08685193210840225\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.0819331705570221\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.085696280002594\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08542700856924057\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.09449943155050278\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0750146135687828\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.0801798403263092\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07072573155164719\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.06561000645160675\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06879554688930511\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.066279336810112\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.059896320104599\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06063714250922203\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06821390986442566\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06098557636141777\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06416657567024231\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06331617385149002\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.06234293431043625\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06443729251623154\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0677867904305458\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.06269632279872894\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.06212561950087547\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06054607778787613\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.05989127606153488\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06098831444978714\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06431621313095093\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.05782585218548775\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06715922057628632\n",
            "Reducing each exemplar set to size: 66\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 61.62\n",
            "\n",
            "Test Accuracy (all groups seen so far): 52.60\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.11641176044940948\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09790471941232681\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.09579294174909592\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08872871845960617\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09449268877506256\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08596827834844589\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.09101972728967667\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09104827046394348\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.09165948629379272\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0887264758348465\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.09349875897169113\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08376692980527878\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.09176125377416611\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08384969085454941\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08682476729154587\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.0903589054942131\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08600682020187378\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.0875459760427475\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08939682692289352\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.09321971982717514\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.0835651084780693\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08440671861171722\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08592069149017334\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08642294257879257\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08881797641515732\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08228247612714767\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.07842747122049332\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07741137593984604\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08291802555322647\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08746404945850372\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08678256720304489\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08113518357276917\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07967962324619293\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08706154674291611\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07903357595205307\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.080634206533432\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08268338441848755\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08034240454435349\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07998062670230865\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08388135582208633\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08206886053085327\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07775073498487473\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07970882207155228\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.0861327275633812\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07948451489210129\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07827501744031906\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08504749089479446\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07989185303449631\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08255256712436676\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07110462337732315\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.0722678080201149\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06947674602270126\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07252230495214462\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.06726180762052536\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06749307364225388\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07042232900857925\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06690583378076553\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06962238997220993\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07050676643848419\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07022969424724579\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06928632408380508\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.06621590256690979\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.06943481415510178\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.06959457695484161\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06803315877914429\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.0635787844657898\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06994714587926865\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06930321455001831\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06412448734045029\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06908702105283737\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 70.00\n",
            "\n",
            "Test Accuracy (all groups seen so far): 48.50\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.11566305160522461\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.1001492440700531\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.10291186720132828\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.09707820415496826\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09393548965454102\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.09408529102802277\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.09277639538049698\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09463029354810715\n",
            "NUM_EPOCHS:  8 / 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "method = 'iCaRL'\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}