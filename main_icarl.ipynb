{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_icarl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/master/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "outputId": "aba50ad6-08fe-479d-c31f-03cb910d98a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "outputId": "df2a410f-80aa-4cb1-b9c5-e4c9bd557997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "outputId": "9e597a41-2d96-4270-8292-cd6cfc101f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "outputId": "7cdd7ac0-ce33-4c2b-b997-1bcb93586eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/76)\u001b[K\rremote: Counting objects:   2% (2/76)\u001b[K\rremote: Counting objects:   3% (3/76)\u001b[K\rremote: Counting objects:   5% (4/76)\u001b[K\rremote: Counting objects:   6% (5/76)\u001b[K\rremote: Counting objects:   7% (6/76)\u001b[K\rremote: Counting objects:   9% (7/76)\u001b[K\rremote: Counting objects:  10% (8/76)\u001b[K\rremote: Counting objects:  11% (9/76)\u001b[K\rremote: Counting objects:  13% (10/76)\u001b[K\rremote: Counting objects:  14% (11/76)\u001b[K\rremote: Counting objects:  15% (12/76)\u001b[K\rremote: Counting objects:  17% (13/76)\u001b[K\rremote: Counting objects:  18% (14/76)\u001b[K\rremote: Counting objects:  19% (15/76)\u001b[K\rremote: Counting objects:  21% (16/76)\u001b[K\rremote: Counting objects:  22% (17/76)\u001b[K\rremote: Counting objects:  23% (18/76)\u001b[K\rremote: Counting objects:  25% (19/76)\u001b[K\rremote: Counting objects:  26% (20/76)\u001b[K\rremote: Counting objects:  27% (21/76)\u001b[K\rremote: Counting objects:  28% (22/76)\u001b[K\rremote: Counting objects:  30% (23/76)\u001b[K\rremote: Counting objects:  31% (24/76)\u001b[K\rremote: Counting objects:  32% (25/76)\u001b[K\rremote: Counting objects:  34% (26/76)\u001b[K\rremote: Counting objects:  35% (27/76)\u001b[K\rremote: Counting objects:  36% (28/76)\u001b[K\rremote: Counting objects:  38% (29/76)\u001b[K\rremote: Counting objects:  39% (30/76)\u001b[K\rremote: Counting objects:  40% (31/76)\u001b[K\rremote: Counting objects:  42% (32/76)\u001b[K\rremote: Counting objects:  43% (33/76)\u001b[K\rremote: Counting objects:  44% (34/76)\u001b[K\rremote: Counting objects:  46% (35/76)\u001b[K\rremote: Counting objects:  47% (36/76)\u001b[K\rremote: Counting objects:  48% (37/76)\u001b[K\rremote: Counting objects:  50% (38/76)\u001b[K\rremote: Counting objects:  51% (39/76)\u001b[K\rremote: Counting objects:  52% (40/76)\u001b[K\rremote: Counting objects:  53% (41/76)\u001b[K\rremote: Counting objects:  55% (42/76)\u001b[K\rremote: Counting objects:  56% (43/76)\u001b[K\rremote: Counting objects:  57% (44/76)\u001b[K\rremote: Counting objects:  59% (45/76)\u001b[K\rremote: Counting objects:  60% (46/76)\u001b[K\rremote: Counting objects:  61% (47/76)\u001b[K\rremote: Counting objects:  63% (48/76)\u001b[K\rremote: Counting objects:  64% (49/76)\u001b[K\rremote: Counting objects:  65% (50/76)\u001b[K\rremote: Counting objects:  67% (51/76)\u001b[K\rremote: Counting objects:  68% (52/76)\u001b[K\rremote: Counting objects:  69% (53/76)\u001b[K\rremote: Counting objects:  71% (54/76)\u001b[K\rremote: Counting objects:  72% (55/76)\u001b[K\rremote: Counting objects:  73% (56/76)\u001b[K\rremote: Counting objects:  75% (57/76)\u001b[K\rremote: Counting objects:  76% (58/76)\u001b[K\rremote: Counting objects:  77% (59/76)\u001b[K\rremote: Counting objects:  78% (60/76)\u001b[K\rremote: Counting objects:  80% (61/76)\u001b[K\rremote: Counting objects:  81% (62/76)\u001b[K\rremote: Counting objects:  82% (63/76)\u001b[K\rremote: Counting objects:  84% (64/76)\u001b[K\rremote: Counting objects:  85% (65/76)\u001b[K\rremote: Counting objects:  86% (66/76)\u001b[K\rremote: Counting objects:  88% (67/76)\u001b[K\rremote: Counting objects:  89% (68/76)\u001b[K\rremote: Counting objects:  90% (69/76)\u001b[K\rremote: Counting objects:  92% (70/76)\u001b[K\rremote: Counting objects:  93% (71/76)\u001b[K\rremote: Counting objects:  94% (72/76)\u001b[K\rremote: Counting objects:  96% (73/76)\u001b[K\rremote: Counting objects:  97% (74/76)\u001b[K\rremote: Counting objects:  98% (75/76)\u001b[K\rremote: Counting objects: 100% (76/76)\u001b[K\rremote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 1517 (delta 40), reused 54 (delta 20), pack-reused 1441\n",
            "Receiving objects: 100% (1517/1517), 7.43 MiB | 35.38 MiB/s, done.\n",
            "Resolving deltas: 100% (931/931), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "outputId": "c39382f1-8499-4f73-c6a5-2eba442042b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-30 17:18:24--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  50.0MB/s    in 3.6s    \n",
            "\n",
            "2020-05-30 17:18:28 (45.0 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "outputId": "c8207706-f4b1-4513-c424-e73e641f3bd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]\n",
        "\n",
        "# icarl params\n",
        "herding = True # if false random exemplars, if true nme (herding)\n",
        "classifier = \"NCM\" # NCM, FCC ..."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 88, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "outputId": "9bea2f81-46ff-41f3-a051-166b4e741715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "outputId": "b35c8df8-94be-4e92-ae5a-cd8ab6f26bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, herding, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        \n",
        "        # add other classifiers\n",
        "        if classifier == 'NCM':\n",
        "          preds = net.classify(images)\n",
        "        elif classifier == 'FCC':\n",
        "          preds = net.FCC_classify(images)\n",
        "\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "  accuracy = correct/len(dataset)\n",
        "  if method == 'test':\n",
        "    all_preds_cm.extend(preds.tolist())\n",
        "    all_labels_cm.extend(labels.data.tolist())\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "    train_set_big = None # train subsets so far (needed for exemplars)\n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "          train_set_big = train_subset\n",
        "        else:\n",
        "          test_set = utils.joinSubsets(test_dataset, [test_set, test_subset])\n",
        "          train_set_big = utils.joinSubsets(train_dataset, [train_set_big, train_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, train_dataset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(math.ceil(K/icarl.n_classes))\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          #print(type(class_train_subset))\n",
        "          icarl.construct_exemplar_set(class_train_subset,m,y) # why eval? ref: https://github.com/donlee90/icarl/blob/master/main.py\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier ...\n",
        "        icarl.computeMeans()\n",
        "        \n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "outputId": "154ab83d-16cf-434b-be42-0c249732a9e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.33419695496559143\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.32229432463645935\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.3151329457759857\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.32251298427581787\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.29753682017326355\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.26687827706336975\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.28451070189476013\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.28582072257995605\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.24588258564472198\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.27721819281578064\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.25670313835144043\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.25239142775535583\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.23574884235858917\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.25767937302589417\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.2331186980009079\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.23614268004894257\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.24232125282287598\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.20938341319561005\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.22988362610340118\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.2114684134721756\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.2398044615983963\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.2169158011674881\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.2480475902557373\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.20649118721485138\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.21384313702583313\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.21961426734924316\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.19461864233016968\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.1872277706861496\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1964915245771408\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.20372839272022247\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.17697007954120636\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.2071038782596588\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.18411509692668915\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.16667217016220093\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.2290162593126297\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.17001603543758392\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.2083207219839096\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.16482646763324738\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.15439243614673615\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.15521296858787537\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.1397780030965805\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.1686299592256546\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1553337126970291\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.14586737751960754\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.15535716712474823\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.1549396812915802\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.12382540851831436\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.147420272231102\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12398796528577805\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.0979280024766922\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.11123603582382202\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.12301421165466309\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.13339795172214508\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11835937947034836\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.10424480587244034\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.08613874018192291\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.09841746091842651\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.10382365435361862\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.0751219317317009\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.1020544096827507\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.08519037812948227\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07545718550682068\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.0932716354727745\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0907757431268692\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.10732938349246979\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.09206243604421616\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.0639350563287735\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07442543655633926\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06977159529924393\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.05563286691904068\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 86.74\n",
            "\n",
            "Test Accuracy (all groups seen so far): 79.10\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.14675597846508026\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.15735089778900146\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.1504213660955429\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.15750694274902344\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.13067756593227386\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.13913153111934662\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.13312458992004395\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.1374906301498413\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12571465969085693\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.13197112083435059\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12926800549030304\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.12279119342565536\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12144122272729874\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.13226862251758575\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12208127975463867\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.128713920712471\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.11244528740644455\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.11795442551374435\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.10720638185739517\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.10791327804327011\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.11958219856023788\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.10485285520553589\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.10248222202062607\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.10578706115484238\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.09519591182470322\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.11556744575500488\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.10710077732801437\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.11343374103307724\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.10256283730268478\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.11430343240499496\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.10807513445615768\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.09978969395160675\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.10539450496435165\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.10040322691202164\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.09659136831760406\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.1069101095199585\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.09727588295936584\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.09857841581106186\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.09889707714319229\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.09447480738162994\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.09630506485700607\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.09715446084737778\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08665493875741959\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.09205688536167145\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.09216087311506271\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.09416785091161728\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.09428801387548447\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.09105783700942993\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08799515664577484\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07198429107666016\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.06933499127626419\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07585432380437851\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07114605605602264\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07418695837259293\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07129917293787003\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06893973052501678\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07334291189908981\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06778010725975037\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06889941543340683\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07974245399236679\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06710044294595718\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07010294497013092\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07345125824213028\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0629730224609375\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.0710451602935791\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.06729497015476227\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07147791981697083\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.0634303018450737\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06607251614332199\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06525252014398575\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 92.40\n",
            "\n",
            "Test Accuracy (all groups seen so far): 72.90\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.12010765820741653\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.11827816069126129\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.11921051144599915\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.10798130929470062\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.11030304431915283\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.1124744787812233\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.1142447218298912\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09907355904579163\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.10056810081005096\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.09995486587285995\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.0982452780008316\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.09766822308301926\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.10413531213998795\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.10370011627674103\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.10275845974683762\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.09895190596580505\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.10127762705087662\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.09154293686151505\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.09496570378541946\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.09823578596115112\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.09459452331066132\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.09301084280014038\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.09089412540197372\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.090463787317276\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.0922551229596138\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.09028548002243042\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.09887094050645828\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08455543220043182\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08516483753919601\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.09524402022361755\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.0919012576341629\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.09240513294935226\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.0912591964006424\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08332640677690506\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08140771836042404\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08762600272893906\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.09444943070411682\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08617876470088959\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08974247425794601\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08432886004447937\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08925776928663254\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08694174885749817\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08425558358430862\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.08361198753118515\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.0899951308965683\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08335547894239426\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08932773768901825\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0833946093916893\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08396473526954651\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.0685139074921608\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07076588273048401\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.06843481212854385\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07019605487585068\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.06805381923913956\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06866830587387085\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06964901089668274\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07173065096139908\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06565215438604355\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06832927465438843\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.06782109290361404\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06770770251750946\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.06641741842031479\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.069447822868824\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0674685463309288\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06651057302951813\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.06735847145318985\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06786704063415527\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06352154910564423\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06668829172849655\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06504414975643158\n",
            "Reducing each exemplar set to size: 67\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 87.54\n",
            "\n",
            "Test Accuracy (all groups seen so far): 67.23\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.10269633680582047\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.10261982679367065\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.10162806510925293\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.0956452414393425\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09743320941925049\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.09487204253673553\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.09650986641645432\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.09421619772911072\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.09587958455085754\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.09026831388473511\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08981865644454956\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.0885041132569313\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.09322097152471542\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08997861295938492\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08795246481895447\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08632149547338486\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08902515470981598\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08379961550235748\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08860307931900024\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08942325413227081\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08365502208471298\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08560550212860107\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08775395900011063\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.09288149327039719\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08688937872648239\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08561040461063385\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08445390313863754\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08789544552564621\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08667003363370895\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.0831492468714714\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08554806560277939\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08268644660711288\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08443361520767212\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08421430736780167\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07802640646696091\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.0815146416425705\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.0836908221244812\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08121903985738754\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07532639056444168\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08141390234231949\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07992281764745712\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08041521161794662\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08148342370986938\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07822483032941818\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08156075328588486\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08203645795583725\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08337539434432983\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07946731150150299\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07950200885534286\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07130622118711472\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07076568901538849\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.0696425586938858\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.06612228602170944\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.0683935359120369\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06928320974111557\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06877085566520691\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07164830714464188\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.06822717934846878\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.06637604534626007\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.06835010647773743\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06639861315488815\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.06571930646896362\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.06590045988559723\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.06709245592355728\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06679388880729675\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.0679173544049263\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.0679757297039032\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06547492742538452\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06487243622541428\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06787136197090149\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 81.08\n",
            "\n",
            "Test Accuracy (all groups seen so far): 61.75\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.09844581037759781\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09754964709281921\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.09266816824674606\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08834678679704666\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.0860668197274208\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08989933133125305\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.0904267281293869\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.07964619249105453\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08666852861642838\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0844515711069107\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.0847601369023323\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08012239634990692\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.0797640010714531\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08387993276119232\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08618985861539841\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08104635030031204\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08148568868637085\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.07487678527832031\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08517303317785263\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0771021842956543\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.0827401876449585\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08005405962467194\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08114181458950043\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08004539459943771\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.0829426571726799\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.0761532112956047\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08517787605524063\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08107805997133255\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08089887350797653\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07680997997522354\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07947471737861633\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08068341761827469\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07881780713796616\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07841138541698456\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08113566040992737\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07796462625265121\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08103901892900467\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07984843850135803\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07904145866632462\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07940120995044708\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.07653488963842392\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07768458127975464\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07764511555433273\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07730467617511749\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08100953698158264\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07554477453231812\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07734877616167068\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07885997742414474\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08098037540912628\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07341624796390533\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07208805531263351\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07225925475358963\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07399295270442963\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07067874819040298\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06989908963441849\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.06924061477184296\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06932526081800461\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07182031124830246\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07041646540164948\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.06873749196529388\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.06702011078596115\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0699799433350563\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07034217566251755\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07038228213787079\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.06771616637706757\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.0651395246386528\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.06729861348867416\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.06878191232681274\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.06861749291419983\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.06763263791799545\n",
            "Reducing each exemplar set to size: 40\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 83.66\n",
            "\n",
            "Test Accuracy (all groups seen so far): 59.60\n",
            "\n",
            "the model knows 50 classes:\n",
            " \n",
            "GROUP:  6\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08930723369121552\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08678553998470306\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08669622242450714\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.09011801332235336\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08241511881351471\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08185750246047974\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08500850945711136\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08722814172506332\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08856960386037827\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0803203284740448\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08567604422569275\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08090101927518845\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08503600209951401\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08061452209949493\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08047688752412796\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08128665387630463\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08230230212211609\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08201143890619278\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08033334463834763\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08401858806610107\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08312586694955826\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08473918586969376\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.0815013125538826\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08070768415927887\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08199519664049149\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08438266813755035\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08197157084941864\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08383844792842865\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.07812725752592087\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.0813252180814743\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07946305721998215\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08186188340187073\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.0817561000585556\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07571069151163101\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.0775911882519722\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.07810166478157043\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07722948491573334\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07845279574394226\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07652027904987335\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07981663942337036\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08250859379768372\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07671359926462173\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07809384912252426\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.08054585754871368\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08040601760149002\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07922006398439407\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07708853483200073\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07756701111793518\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08141160756349564\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07542219012975693\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07095521688461304\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07030659914016724\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07026230543851852\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07285137474536896\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.06926720589399338\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.0720323771238327\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.06953265517950058\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07267636060714722\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07379583269357681\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07128911465406418\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07137233763933182\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07183852791786194\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.0690169706940651\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0711018294095993\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07026534527540207\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07029587775468826\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07268466055393219\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07027987390756607\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07123605161905289\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07029729336500168\n",
            "Reducing each exemplar set to size: 34\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 76.48\n",
            "\n",
            "Test Accuracy (all groups seen so far): 56.42\n",
            "\n",
            "the model knows 60 classes:\n",
            " \n",
            "GROUP:  7\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08921996504068375\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09140955656766891\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08759204298257828\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.0874292254447937\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.0841665044426918\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08464565873146057\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08469649404287338\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08480795472860336\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.0870317742228508\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08338893949985504\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08224008977413177\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08008159697055817\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08396168053150177\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.0818428173661232\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08366353064775467\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08153180032968521\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08025392144918442\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08197809010744095\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08165230602025986\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08091443032026291\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08243389427661896\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08293028920888901\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08169005066156387\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08009077608585358\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08285626024007797\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08017489314079285\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08287417888641357\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08074846118688583\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08083854615688324\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07901683449745178\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.0789576917886734\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08085492253303528\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.081642284989357\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07917694747447968\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08251819759607315\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08075905591249466\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08193200081586838\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.07972507178783417\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08121081441640854\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08280465006828308\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.079996258020401\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08078619092702866\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.07806768268346786\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.07861634343862534\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.07937932759523392\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07788478583097458\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07959401607513428\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.08006845414638519\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.07937192171812057\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07325280457735062\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07388417422771454\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07308422029018402\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07280869036912918\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07134965062141418\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07315709441900253\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07426435500383377\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07361066341400146\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07246457785367966\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07511167228221893\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07549784332513809\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.0735749676823616\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.06957440823316574\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07227402925491333\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07279539108276367\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07310442626476288\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07290606200695038\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07117403298616409\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07147987931966782\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07355489581823349\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07204174995422363\n",
            "Reducing each exemplar set to size: 29\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 77.48\n",
            "\n",
            "Test Accuracy (all groups seen so far): 54.49\n",
            "\n",
            "the model knows 70 classes:\n",
            " \n",
            "GROUP:  8\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.09103138744831085\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.08988594263792038\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08858975023031235\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.09128645807504654\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08676128834486008\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08652230352163315\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08628875017166138\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08437734842300415\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08421751111745834\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08774536848068237\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08630050718784332\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.0856461375951767\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08452127873897552\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.0855553150177002\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08363647758960724\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08310890197753906\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08066948503255844\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08489706367254257\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08190455287694931\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.08293434232473373\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08004217594861984\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08501704037189484\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08168935775756836\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08184865862131119\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08135724812746048\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.0835951715707779\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08178901672363281\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.08140011131763458\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08343625068664551\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.0826139748096466\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08288580179214478\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08000832051038742\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07992327213287354\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08154188841581345\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08059214800596237\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08120931684970856\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08104424923658371\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08173256367444992\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08124332875013351\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08064236491918564\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08135855942964554\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.07841736823320389\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08084777742624283\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.08056732267141342\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08341441303491592\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.07993441820144653\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08155804127454758\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.0812092199921608\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08037595450878143\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07582852989435196\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.0760556012392044\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07350074499845505\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07389093935489655\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07595788687467575\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07449724525213242\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07369305938482285\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.0738021656870842\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07702857255935669\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07655560970306396\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.0746142789721489\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07512494176626205\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07414685934782028\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07295327633619308\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07378014922142029\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07400156557559967\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07356139272451401\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07214458286762238\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07538887113332748\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07183605432510376\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07316114008426666\n",
            "Reducing each exemplar set to size: 25\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 67.10\n",
            "\n",
            "Test Accuracy (all groups seen so far): 50.41\n",
            "\n",
            "the model knows 80 classes:\n",
            " \n",
            "GROUP:  9\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.08802175521850586\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09056010097265244\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.08833940327167511\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08625227957963943\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.08565565943717957\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08514565229415894\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08675191551446915\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08967474102973938\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08407489210367203\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.08289329707622528\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08443962037563324\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08500052988529205\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08358094841241837\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.08429651707410812\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.0825173631310463\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08244301378726959\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.08144178241491318\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.0811086967587471\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08200641721487045\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0826549232006073\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08264373987913132\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08180234581232071\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08129585534334183\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08265520632266998\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08242280036211014\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08329585939645767\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08246839791536331\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07977280020713806\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08107078820466995\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08355777710676193\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08255988359451294\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08222606033086777\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08239800482988358\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08386006206274033\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.08095414936542511\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08389769494533539\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08334390819072723\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08376123011112213\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08008616417646408\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.0808512419462204\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08146236091852188\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08288693428039551\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08042926341295242\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.0800991877913475\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.0831688717007637\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08005917817354202\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08225748687982559\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.08066990226507187\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08084248006343842\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07791747897863388\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07669554650783539\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.07574038207530975\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.07857125997543335\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.077579565346241\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07494042813777924\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.07905013114213943\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07538164407014847\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07745987921953201\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07660470902919769\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07603021711111069\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07560182362794876\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.07357378304004669\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07695696502923965\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07591548562049866\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07522382587194443\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07613518089056015\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.07544630765914917\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07591740787029266\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07571849226951599\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07403987646102905\n",
            "Reducing each exemplar set to size: 23\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 66.28\n",
            "\n",
            "Test Accuracy (all groups seen so far): 48.71\n",
            "\n",
            "the model knows 90 classes:\n",
            " \n",
            "GROUP:  10\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.09179630130529404\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.09053746610879898\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.09190848469734192\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.08737756311893463\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.09075052291154861\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.08735700696706772\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.08972934633493423\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.08854223042726517\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.08539985120296478\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.09008296579122543\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08851079642772675\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.08680535852909088\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.08676771074533463\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.09091455489397049\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08958561718463898\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.08647281676530838\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.0867539793252945\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.087732695043087\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.08477755635976791\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0859622061252594\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08482736349105835\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08436191082000732\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08429358899593353\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08643611520528793\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08413812518119812\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.08571469038724899\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.08444585651159286\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.0862002745270729\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.08462665975093842\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.08343690633773804\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.08567121624946594\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.08531495928764343\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.08505092561244965\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.08582566678524017\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.0841018408536911\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.08522548526525497\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.08204545825719833\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08602286130189896\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.08442627638578415\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.08470596373081207\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.08353301882743835\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.08499438315629959\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.08183948695659637\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.08235158026218414\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.08501686155796051\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.08323848247528076\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.08239902555942535\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.08375301212072372\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.08171285688877106\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.07883187383413315\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.07846999913454056\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.08006160706281662\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.08110509812831879\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.07732554525136948\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.07873348891735077\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.0770995244383812\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.07835210859775543\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.07936225086450577\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.07901700586080551\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.07931449264287949\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.07846412062644958\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.0776798352599144\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.07830803841352463\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.07736625522375107\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.07852187752723694\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.07926489412784576\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.0762806162238121\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.07833650708198547\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.07708815485239029\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.07805531471967697\n",
            "Reducing each exemplar set to size: 20\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 67.76\n",
            "\n",
            "Test Accuracy (all groups seen so far): 45.85\n",
            "\n",
            "the model knows 100 classes:\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "outputId": "0330defb-5795-49c7-90b6-8a921dd05171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if herding:\n",
        "  method = 'iCaRL_{}_herding'.format(classifier)\n",
        "else:\n",
        "  method = 'iCaRL_{}_random'.format(classifier)\n",
        "\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics iCaRL for seed 88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}