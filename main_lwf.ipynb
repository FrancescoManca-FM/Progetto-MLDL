{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_lwf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/lwf/main_lwf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Cpfl5JiceY",
        "colab_type": "code",
        "outputId": "d34130f9-8c91-4f32-ae19-3b4af35a529e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications,\n",
        "  LwF is implemented simirality to iCaRL itself.\n",
        "  The differences are:\n",
        "  - No exemplars management\n",
        "  - For classification, it is used the network output values themselves\n",
        "  (ref. iCaRL paper section 4.1)\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications,\\n  LwF is implemented simirality to iCaRL itself.\\n  The differences are:\\n  - No exemplars management\\n  - For classification, it is used the network output values themselves\\n  (ref. iCaRL paper section 4.1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhl89o9xjPsT",
        "colab_type": "code",
        "outputId": "35630f22-2a04-4532-926c-583cd186fc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvZ4mFgjZ8t",
        "colab_type": "code",
        "outputId": "863d8714-7762-48b3-97a5-006da56e8b9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone -b lwf https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 484 (delta 130), reused 136 (delta 62), pack-reused 269\u001b[K\n",
            "Receiving objects: 100% (484/484), 3.97 MiB | 2.50 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqlWIxXjldh",
        "colab_type": "code",
        "outputId": "95d2fa0c-fbe0-44f5-8488-0f92b2dc891e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-26 18:15:07--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  14.4MB/s    in 13s     \n",
            "\n",
            "2020-05-26 18:15:21 (12.7 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-LUNWGgSYwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5a49b3a6-8ce0-4982-94a3-1dc66d59a1c6"
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 30, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXMsUQ2oPS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xM_JUJoP-l",
        "colab_type": "code",
        "outputId": "137c512a-5310-4ab8-debd-5adfcc795246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owLONCmOoZUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ONNgtszoa26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlDVHznwocr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2J8c2SaohuJ",
        "colab_type": "text"
      },
      "source": [
        "**LWF implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7fwVxB7oefq",
        "colab_type": "code",
        "outputId": "88cfc8d9-3292-4cbc-e1ab-8c4dc6b849ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# default params\n",
        "from Cifar100.lwf_model import LWF\n",
        "\n",
        "feature_size = 2048\n",
        "n_classes = 0\n",
        "lwf = LWF(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, outputs_labels_mapping)\n",
        "lwf.cuda()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LWF(\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=2048, bias=True)\n",
              "  )\n",
              "  (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "  (ReLU): ReLU()\n",
              "  (fc): Linear(in_features=2048, out_features=0, bias=False)\n",
              "  (cls_loss): BCEWithLogitsLoss()\n",
              "  (dist_loss): BCEWithLogitsLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4l0ELnTxI2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H60xMfbW2R3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(net, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index):\n",
        "    #groups_accuracies=[] not used right now, use it if you want test on single groups\n",
        "    all_accuracies=[]\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      print(\"GROUP: \",group_id)\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "      train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "      val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "      test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "      \n",
        "      net.train()\n",
        "\n",
        "      new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "\n",
        "      # update representation\n",
        "      net.update_representation(train_subset, new_classes_examined)\n",
        "\n",
        "      net.n_known = net.n_classes\n",
        "      print (\"the model knows %d classes:\\n \" % net.n_known)\n",
        "\n",
        "      # evaluation on the train set\n",
        "      net.eval()\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      for indices, images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        preds = net.classify(images)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      # Train Accuracy\n",
        "      print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * correct / len(train_subset)))\n",
        "\n",
        "      # evaluation on all the previous groups\n",
        "      net.eval()\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      for indices, images, labels in test_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        preds = net.classify(images)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      accuracy = correct / len(test_set)\n",
        "      all_accuracies.append(accuracy)\n",
        "      # Train Accuracy\n",
        "      print ('Test Accuracy (all groupds seen so far): %.2f\\n' % (100.0 * accuracy))\n",
        "\n",
        "      group_id+=1\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TXVwGceM6gN",
        "colab_type": "code",
        "outputId": "64768081-fbef-4e70-ebe8-167347ad4521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "incrementalTraining(lwf, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  tensor(2.4223, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  tensor(2.4044, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  tensor(2.1034, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  tensor(1.9272, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  tensor(1.9924, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  tensor(1.9603, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  tensor(1.8054, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  tensor(1.6038, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  tensor(1.7772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  tensor(1.7842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  tensor(1.6801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  tensor(1.7558, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  tensor(1.6945, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  tensor(1.4909, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  tensor(1.3187, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  tensor(1.2539, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  tensor(1.4491, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  tensor(1.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  tensor(1.3985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  tensor(1.1941, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  tensor(1.1849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  tensor(1.0963, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  tensor(1.2369, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  tensor(1.2498, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  tensor(1.2820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  tensor(1.0172, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  tensor(1.1618, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  tensor(1.2550, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  tensor(1.1546, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  tensor(1.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  tensor(1.1003, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  tensor(0.8920, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  tensor(1.0086, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  tensor(0.8290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  tensor(1.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  tensor(0.7067, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  tensor(0.9778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  tensor(0.7783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  tensor(0.6691, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  tensor(0.9223, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  tensor(0.7291, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  tensor(0.8760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  tensor(0.7335, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  tensor(0.8229, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  tensor(0.8019, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  tensor(0.5458, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  tensor(0.6084, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  tensor(0.9096, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  tensor(0.6252, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  tensor(0.5356, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  tensor(0.4818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  tensor(0.3782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  tensor(0.3676, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  tensor(0.4441, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  tensor(0.2165, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  tensor(0.2319, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  tensor(0.3494, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  tensor(0.3674, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  tensor(0.2651, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  tensor(0.3573, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  tensor(0.3068, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  tensor(0.3464, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  tensor(0.3109, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  tensor(0.1984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  tensor(0.1905, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  tensor(0.2472, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  tensor(0.2149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  tensor(0.2031, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  tensor(0.1837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  tensor(0.2401, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "the model knows 10 classes:\n",
            " \n",
            "Train Accuracy (on current group): 98.16\n",
            "\n",
            "Test Accuracy (all groupds seen so far): 81.40\n",
            "\n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  tensor(17.3215, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  tensor(17.1074, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  tensor(25.7368, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  tensor(-3.6290, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  tensor(-1.8738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  tensor(12.4055, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  tensor(2.6839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  tensor(10.3922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  tensor(19.6638, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  tensor(7.1508, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  tensor(-44.5936, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  tensor(5.2798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  tensor(4.3434, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  tensor(16.1775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  tensor(8.5944, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  tensor(1.1988, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  tensor(9.5890, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  tensor(6.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  tensor(10.6631, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  tensor(-6.9301, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  tensor(8.2635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  tensor(52.7353, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  tensor(1.4594, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  tensor(9.3375, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  tensor(11.2635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  tensor(29.5723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  tensor(6.7758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  tensor(11.4451, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  tensor(-7.9996, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  tensor(10.8499, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  tensor(-3.2544, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  tensor(7.5527, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  tensor(-22.5144, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  tensor(10.5292, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  tensor(-9.2846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  tensor(6.0935, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  tensor(-21.1713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  tensor(63.7602, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  tensor(-1.1115, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  tensor(10.2018, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  tensor(-388.8552, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  tensor(-120.9766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  tensor(18.7047, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  tensor(-48.2985, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  tensor(-174.9912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  tensor(15.0180, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  tensor(6.2709, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  tensor(384.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  tensor(131.8974, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  tensor(-67.5166, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  tensor(-189.1921, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  tensor(-2308.8635, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  tensor(-58.3414, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  tensor(-252.0587, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  tensor(-586.3090, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  tensor(1191.4858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  tensor(1183.3544, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  tensor(5925.6294, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  tensor(46799.3047, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  tensor(-3351.7639, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  tensor(110812.1250, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  tensor(-6.1369e+09, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "the model knows 20 classes:\n",
            " \n",
            "Train Accuracy (on current group): 0.00\n",
            "\n",
            "Test Accuracy (all groupds seen so far): 5.00\n",
            "\n",
            "GROUP:  3\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  tensor(nan, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "NUM_EPOCHS:  3 / 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpsSTfGNM1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}