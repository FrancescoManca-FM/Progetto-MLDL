{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_lwf.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/lwf/main_lwf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Cpfl5JiceY",
        "colab_type": "code",
        "outputId": "e0af65ac-d38b-4ab6-ac2d-cba9be3fbec5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications,\n",
        "  LwF is implemented simirality to iCaRL itself.\n",
        "  The differences are:\n",
        "  - No exemplars management\n",
        "  - For classification, it is used the network output values themselves\n",
        "  (ref. iCaRL paper section 4.1)\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications,\\n  LwF is implemented simirality to iCaRL itself.\\n  The differences are:\\n  - No exemplars management\\n  - For classification, it is used the network output values themselves\\n  (ref. iCaRL paper section 4.1)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhl89o9xjPsT",
        "colab_type": "code",
        "outputId": "153f3cde-e314-4415-f053-8bfb07fe654b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juvZ4mFgjZ8t",
        "colab_type": "code",
        "outputId": "f0812fc2-221d-4271-f5a0-422835925270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "#!rm -r $DATA_DIR\n",
        "#!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone -b lwf https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 24, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 526 (delta 14), reused 18 (delta 8), pack-reused 502\u001b[K\n",
            "Receiving objects: 100% (526/526), 3.98 MiB | 1.37 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqlWIxXjldh",
        "colab_type": "code",
        "outputId": "c4053d34-d033-4bbc-d4d4-2c54e6e27352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}'.format(\"$DATA_DIR/cifar-100-python\")):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-26 20:20:47--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  14.1MB/s    in 13s     \n",
            "\n",
            "2020-05-26 20:21:01 (12.7 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "mv: cannot move 'cifar-100-python' to 'DATA/cifar-100-python/cifar-100-python': Directory not empty\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-LUNWGgSYwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6ad5e293-a0a5-4fa2-8968-f8361b661762"
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = dictHyperparams[\"NUM_EPOCHS\"]     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = dictHyperparams[\"SEED\"]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 30, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXMsUQ2oPS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8xM_JUJoP-l",
        "colab_type": "code",
        "outputId": "2d10c9e8-47cb-4bf2-c50a-be5c49da8c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owLONCmOoZUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ONNgtszoa26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=0.99, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlDVHznwocr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10):\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2J8c2SaohuJ",
        "colab_type": "text"
      },
      "source": [
        "**LWF implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7fwVxB7oefq",
        "colab_type": "code",
        "outputId": "53926de5-93ed-476c-e642-3c39096410a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# default params\n",
        "from Cifar100.lwf_model import LWF\n",
        "\n",
        "feature_size = 2048\n",
        "n_classes = 0\n",
        "lwf = LWF(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, outputs_labels_mapping)\n",
        "lwf.cuda()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LWF(\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=2048, bias=True)\n",
              "  )\n",
              "  (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
              "  (ReLU): ReLU()\n",
              "  (fc): Linear(in_features=2048, out_features=0, bias=False)\n",
              "  (cls_loss): BCEWithLogitsLoss()\n",
              "  (dist_loss): BCEWithLogitsLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4l0ELnTxI2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joinSubsets(dataset, subsets):\n",
        "    indices = []\n",
        "    for s in subsets:\n",
        "        indices += s.indices\n",
        "    return Subset(dataset, indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H60xMfbW2R3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(net, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index):\n",
        "    #groups_accuracies=[] not used right now, use it if you want test on single groups\n",
        "    all_accuracies = []\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "    for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "      print(\"GROUP: \",group_id)\n",
        "      if test_set is None:\n",
        "        test_set = test_subset\n",
        "      else:\n",
        "        test_set = joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "      train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "      val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "      test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False, num_workers=4)\n",
        "      \n",
        "      #net.train()\n",
        "\n",
        "      new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "\n",
        "      # update representation\n",
        "      net.update_representation(train_subset, new_classes_examined)\n",
        "\n",
        "      # evaluation on the train set\n",
        "      net.eval()\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      for indices, images, labels in train_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        preds = net.classify(images)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "      # Train Accuracy\n",
        "      print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * correct / len(train_subset)))\n",
        "\n",
        "      # evaluation on all the previous groups\n",
        "      #net.eval()\n",
        "      total = 0.0\n",
        "      correct = 0.0\n",
        "\n",
        "      for indices, images, labels in test_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        labels = reverse_index.getNodes(labels)\n",
        "        preds = net.classify(images)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "      \n",
        "      all_preds_cm.extend(preds.tolist())\n",
        "      all_labels_cm.extend(labels.data.tolist())\n",
        "\n",
        "      accuracy = correct / len(test_set)\n",
        "      all_accuracies.append(accuracy)\n",
        "      # Train Accuracy\n",
        "      print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * accuracy))\n",
        "\n",
        "      net.n_known = net.n_classes\n",
        "      print (\"the model knows %d classes:\\n \" % net.n_known)\n",
        "\n",
        "      group_id+=1\n",
        "    \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)\n",
        "\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TXVwGceM6gN",
        "colab_type": "code",
        "outputId": "6d429576-f6fd-4b6d-85a1-14e74e540ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(lwf, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  tensor(0.2839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  tensor(0.2436, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  tensor(0.2342, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  tensor(0.2225, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  tensor(0.2032, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  tensor(0.2205, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  tensor(0.2028, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  tensor(0.2079, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  tensor(0.1750, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  tensor(0.1926, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  tensor(0.1461, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  tensor(0.1623, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  tensor(0.1695, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  tensor(0.1513, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  tensor(0.1647, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  tensor(0.1585, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  tensor(0.1485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  tensor(0.1469, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  tensor(0.1177, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  tensor(0.1305, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  tensor(0.1064, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  tensor(0.1127, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  tensor(0.1129, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  tensor(0.1133, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  tensor(0.1089, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  tensor(0.1168, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  tensor(0.1266, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  tensor(0.1144, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  tensor(0.1049, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  tensor(0.1188, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  tensor(0.0839, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  tensor(0.0834, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  tensor(0.1005, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  tensor(0.1021, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  tensor(0.1037, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  tensor(0.0818, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  tensor(0.0964, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  tensor(0.0935, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  tensor(0.0777, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  tensor(0.1008, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  tensor(0.1014, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  tensor(0.0727, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  tensor(0.0676, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  tensor(0.0666, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  tensor(0.0733, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  tensor(0.0640, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  tensor(0.0737, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  tensor(0.0645, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  tensor(0.0743, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  tensor(0.0485, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  tensor(0.0531, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  tensor(0.0607, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  tensor(0.0515, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  tensor(0.0361, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  tensor(0.0439, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  tensor(0.0411, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  tensor(0.0335, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  tensor(0.0437, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  tensor(0.0346, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  tensor(0.0224, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  tensor(0.0262, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  tensor(0.0396, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  tensor(0.0338, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  tensor(0.0293, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  tensor(0.0241, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  tensor(0.0275, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  tensor(0.0340, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  tensor(0.0216, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  tensor(0.0233, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  tensor(0.0167, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "Train Accuracy (on current group): 98.14\n",
            "\n",
            "Test Accuracy (all groups seen so far): 83.30\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  tensor(0.5245, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  tensor(0.5034, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  tensor(0.4967, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  tensor(0.4881, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  tensor(0.4861, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  tensor(0.4770, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  tensor(0.4681, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  tensor(0.4648, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n",
            "NUM_EPOCHS:  8 / 70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpsSTfGNM1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# metrics\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "for id in range(0,10):\n",
        "    data_plot_line.append(((id+1)*10, accuracies[id]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(data_plot_line)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(confusionMatrixData)\n",
        "\n",
        "# write to file\n",
        "print(\"metrics FINETUNING for seed {}\".format(RANDOM_SEED))\n",
        "utils.writeMetrics('lwf', RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}