{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_icarl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/classifiers/main_icarl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Tkq4Z64NfD",
        "colab_type": "code",
        "outputId": "82e00dac-45df-4f65-9033-73ed5ccaeee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "  Following the iCaRL paper specifications.\n",
        "  ...documentation ...\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  Following the iCaRL paper specifications.\\n  ...documentation ...\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMSxfKS2gIKU",
        "colab_type": "code",
        "outputId": "542463eb-6436-4a36-c4a3-f00d36fe2f9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\"\"\"\n",
        "# !pip install --upgrade wandb"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!pip3 install 'torch==1.3.1'\\n!pip3 install 'torchvision==0.5.0'\\n!pip3 install 'Pillow-SIMD'\\n!pip3 install 'tqdm'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiz6sjyFgQFs",
        "colab_type": "code",
        "outputId": "78827eaa-dffa-49e7-d847-a0b8c5a1c932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BToWlSKc4km7",
        "colab_type": "code",
        "outputId": "b151d920-d675-4ebc-c278-b2412b8d5298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "source": [
        "DATA_DIR = 'DATA' # here the dataset will be downloaded\n",
        "\n",
        "# Clone github repository with dataset handler\n",
        "!rm -r Cifar100/\n",
        "!rm -r $DATA_DIR\n",
        "!mkdir \"DATA\"\n",
        "if not os.path.isdir('./Cifar100'):\n",
        "  !git clone https://github.com/danielegenta/Progetto-MLDL.git\n",
        "  !mv 'Progetto-MLDL' 'Cifar100'\n",
        "  !rm -r Cifar100/Theoretical-Sources\n",
        "  !rm -rf Cifar100/ProjectMLDL.ipynb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'Cifar100/': No such file or directory\n",
            "rm: cannot remove 'DATA': No such file or directory\n",
            "Cloning into 'Progetto-MLDL'...\n",
            "remote: Enumerating objects: 184, done.\u001b[K\n",
            "remote: Counting objects: 100% (184/184), done.\u001b[K\n",
            "remote: Compressing objects: 100% (130/130), done.\u001b[K\n",
            "remote: Total 1916 (delta 95), reused 131 (delta 47), pack-reused 1732\u001b[K\n",
            "Receiving objects: 100% (1916/1916), 12.47 MiB | 15.64 MiB/s, done.\n",
            "Resolving deltas: 100% (1150/1150), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Raa-DyJgUwV",
        "colab_type": "text"
      },
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTxhdzcVgWmO",
        "colab_type": "code",
        "outputId": "56e7d65d-4db3-4c59-e78f-63475c7f2565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "# Download dataset from the official source and save it into DATA/cifar-100-pyhton\n",
        "\n",
        "if not os.path.isdir('./{}/cifar-100-python'.format(DATA_DIR)):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !mkdir $DATA_DIR\n",
        "    !mv 'cifar-100-python' \"$DATA_DIR/cifar-100-python\"\n",
        "    !rm -rf 'cifar-100-python.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-11 13:19:01--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  31.9MB/s    in 5.6s    \n",
            "\n",
            "2020-06-11 13:19:07 (28.5 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "mkdir: cannot create directory ‘DATA’: File exists\n",
            "/bin/bash: -c: line 0: unexpected EOF while looking for matching `''\n",
            "/bin/bash: -c: line 1: syntax error: unexpected end of file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjIXkQbKgZH3",
        "colab_type": "code",
        "outputId": "ce9323a8-a712-4cec-8911-176593560508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from Cifar100 import utils\n",
        "\n",
        "\n",
        "dictHyperparams = utils.getHyperparams()\n",
        "print(dictHyperparams)\n",
        "\n",
        "DEVICE = dictHyperparams[\"DEVICE\"] # 'cuda' or 'cpu'\n",
        "NUM_CLASSES = dictHyperparams[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = dictHyperparams[\"BATCH_SIZE\"]     # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                     # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = dictHyperparams[\"LR\"]          # The initial Learning Rate\n",
        "MOMENTUM = dictHyperparams[\"MOMENTUM\"]       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = dictHyperparams[\"WEIGHT_DECAY\"] # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = 70     # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = dictHyperparams[\"GAMMA\"]         # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = dictHyperparams[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = dictHyperparams[\"MILESTONES\"]\n",
        "RANDOM_SEED = 66\n",
        "\n",
        "# icarl params\n",
        "herding = True # if false random exemplars, if true nme (herding)\n",
        "classifier = \"NCM\" # NCM, FCC, KNN, SVC, COS"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'LR': 2, 'MOMENTUM': 0.9, 'WEIGHT_DECAY': 1e-05, 'NUM_EPOCHS': 70, 'MILESTONES': [49, 63], 'BATCH_SIZE': 128, 'DEVICE': 'cuda', 'GAMMA': 0.2, 'SEED': 42, 'LOG_FREQUENCY': 10, 'NUM_CLASSES': 100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOcQlG_ga8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform, eval_transform = utils.getTransformations()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHXbDzgjgk_B",
        "colab_type": "code",
        "outputId": "5b0dbd7f-c051-458b-c38b-23dd62d8b6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "\n",
        "# Import dataset\n",
        "train_dataset = CIFAR100(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = CIFAR100(DATA_DIR, split='test', transform=eval_transform)\n",
        "\n",
        "# check if datasets have been correctly loaded\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m85q6ZMLgsC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from Cifar100.reverse_index import ReverseIndex\n",
        "\n",
        "def build_test_splits(dataset, reverse_index):\n",
        "    splits = dict()\n",
        "    groups = list(reverse_index.getGroups())\n",
        "    for g in groups:\n",
        "        labels_of_groups = reverse_index.getLabelsOfGroup(g)\n",
        "        indices = list(dataset.df[dataset.df['labels'].isin(labels_of_groups)].index)\n",
        "        splits[g] = indices\n",
        "    return splits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgY-syfF3WRw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# performing the train/val split\n",
        "train_splits = train_dataset.split_in_train_val_groups(ratio=1, seed=RANDOM_SEED)\n",
        "outputs_labels_mapping = ReverseIndex(train_dataset, train_splits)\n",
        "\n",
        "# performing the test split (coherent with train/val)\n",
        "test_splits = build_test_splits(test_dataset, outputs_labels_mapping)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsFyMkAyguQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_subsets = []\n",
        "val_subsets = []\n",
        "test_subsets = []\n",
        "\n",
        "for v in train_splits.values():\n",
        "    train_subs = Subset(train_dataset, v['train'])\n",
        "    #val_subs = Subset(train_dataset, v['val'])\n",
        "    train_subsets.append(train_subs)\n",
        "    #val_subsets.append(val_subs)\n",
        "\n",
        "for i in range(0,10): # for each group of classes\n",
        "    v=test_splits[i]\n",
        "    test_subs = Subset(test_dataset, v)\n",
        "    test_subsets.append(test_subs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAn-c_mMrMB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    This class implements the main model of iCaRL \n",
        "    and all the methods regarding the exemplars\n",
        "    from delivery: iCaRL is made up of 2 components\n",
        "    - feature extractor (a convolutional NN) => resnet32 optimized on cifar100\n",
        "    - classifier => a FC layer OR a non-parametric classifier (NME)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torch.autograd import Variable\n",
        "import copy\n",
        "import gc #extensive use in order to manage memory issues\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToPILImage \n",
        "\n",
        "from Cifar100 import utils\n",
        "from Cifar100.resnet import resnet32\n",
        "from Cifar100.Dataset.cifar100 import CIFAR100\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# new classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def auto_loss_rebalancing(n_known, n_classes, loss_type):\n",
        "  alpha = n_known/n_classes \n",
        "\n",
        "  if loss_type == 'class':\n",
        "    return 1-alpha\n",
        "  return alpha\n",
        "\n",
        "def get_rebalancing(rebalancing=None):\n",
        "  if rebalancing is None:\n",
        "    return lambda n_known, n_classes, loss_type: 1\n",
        "  if rebalancing in ['auto', 'AUTO']:\n",
        "    return auto_loss_rebalancing\n",
        "  if callable(rebalancing):\n",
        "    return rebalancing\n",
        "\n",
        "# feature_size: 2048, why?\n",
        "# n_classes: 10 => 100\n",
        "class ICaRL(nn.Module):\n",
        "  def __init__(self, feature_size, n_classes,\\\n",
        "      BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE, MILESTONES, MOMENTUM, K,\\\n",
        "      herding, reverse_index = None, class_loss_criterion='bce', dist_loss_criterion='bce', loss_rebalancing='auto', lambda0=1):\n",
        "    super(ICaRL, self).__init__()\n",
        "    self.net = resnet32()\n",
        "    self.net.fc = nn.Linear(self.net.fc.in_features, n_classes)\n",
        "\n",
        "    self.feature_extractor = resnet32()\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    self.n_classes = n_classes\n",
        "    self.n_known = 0\n",
        "\n",
        "    # Hyper-parameters from iCaRL\n",
        "    self.BATCH_SIZE = BATCH_SIZE\n",
        "    self.WEIGHT_DECAY  = WEIGHT_DECAY\n",
        "    self.LR = LR\n",
        "    self.GAMMA = GAMMA # this allow LR to become 1/5 LR after MILESTONES epochs\n",
        "    self.NUM_EPOCHS = NUM_EPOCHS\n",
        "    self.DEVICE = DEVICE\n",
        "    self.MILESTONES = MILESTONES # when the LR decreases, according to icarl\n",
        "    self.MOMENTUM = MOMENTUM\n",
        "    self.K = K\n",
        "    \n",
        "    self.reverse_index=reverse_index\n",
        "\n",
        "    self.optimizer, self.scheduler = utils.getOptimizerScheduler(self.LR, self.MOMENTUM, self.WEIGHT_DECAY, self.MILESTONES, self.GAMMA, self.parameters())\n",
        "\n",
        "    gc.collect()\n",
        "    \n",
        "    # List containing exemplar_sets\n",
        "    # Each exemplar_set is a np.array of N images\n",
        "    self.exemplar_sets = []\n",
        "    self.exemplar_sets_indices = []\n",
        "\n",
        "    \n",
        "    # for the classification/distillation loss we have two alternatives\n",
        "    # 1- BCE loss with Logits (reduction could be mean or sum)\n",
        "    # 2- BCE loss + sigmoid\n",
        "    # actually we use just one loss as explained on the forum\n",
        "\n",
        "    self.class_loss, self.dist_loss = self.build_loss(class_loss_criterion, dist_loss_criterion, loss_rebalancing, lambda0=lambda0)\n",
        "\n",
        "    # Means of exemplars (cntroids)\n",
        "    self.compute_means = True\n",
        "    self.exemplar_means = []\n",
        "    self.exemplar_mean_nn = [] # means not normalized\n",
        "\n",
        "    self.herding = herding # random choice of exemplars or icarl exemplars strategy?\n",
        "\n",
        "    # this is used as explained in the forum to compute the exemplar mean in a more accurate way\n",
        "    # populated during construct exemplar set and used in the classify step\n",
        "    self.data_from_classes = []\n",
        "    self.means_from_classes = []\n",
        "\n",
        "    # Knn, svc classification\n",
        "    self.model = None\n",
        "\n",
        "    # QUA! #\n",
        "    ###############################################################################################################################################################################################\n",
        "    self.oldNet= None\n",
        "    self.moreOldNet= None\n",
        "\n",
        "    ###############################################################################################################################################################################################\n",
        "  \n",
        "  # increment the number of classes considered by the net\n",
        "  # incremental learning approach, 0,10..100\n",
        "  def increment_classes(self, n):\n",
        "        gc.collect()\n",
        "\n",
        "        in_features = self.net.fc.in_features\n",
        "        out_features = self.net.fc.out_features\n",
        "        weights = self.net.fc.weight.data\n",
        "        bias = self.net.fc.bias.data\n",
        "\n",
        "        self.net.fc = nn.Linear(in_features, out_features + n) #add 10 classes to the fc last layer\n",
        "        self.net.fc.weight.data[:out_features] = weights\n",
        "        self.net.fc.bias.data[:out_features] = bias\n",
        "        self.n_classes += n #icrement #classes considered\n",
        "\n",
        "  # computes the mean of each exemplar set\n",
        "  def computeMeans(self):\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # new mean mgmt\n",
        "    tensors_mean = []\n",
        "    exemplar_mean_nn=[]\n",
        "    with torch.no_grad():\n",
        "      for tensor_set in self.data_from_classes:\n",
        "        features = []\n",
        "        for tensor, _ in tensor_set:\n",
        "          \n",
        "          tensor = tensor.to(self.DEVICE)\n",
        "          feature = feature_extractor(tensor)\n",
        "\n",
        "          feature.data = feature.data / feature.data.norm() # Normalize\n",
        "          features.append(feature)\n",
        "\n",
        "          # cleaning \n",
        "          torch.no_grad()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        features = torch.stack(features) #(num_exemplars,num_features)\n",
        "        mean_tensor = features.mean(0) \n",
        "        exemplar_mean_nn.append(mean_tensor.to('cpu'))\n",
        "        mean_tensor.data = mean_tensor.data / mean_tensor.data.norm() # Re-normalize\n",
        "        mean_tensor = mean_tensor.to('cpu')\n",
        "        tensors_mean.append(mean_tensor)\n",
        "\n",
        "    self.exemplar_means = tensors_mean  # nb the mean is computed over all the imgs\n",
        "    self.exemplar_mean_nn= exemplar_mean_nn # exemplars means not normalized\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()  \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # train procedure common for KNN and SVC classifier (save a lot of training time)\n",
        "  def modelTrain(self, method, K_nn = None):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    # -- train a SVC classifier\n",
        "    X_train, y_train = [], []\n",
        "\n",
        "    for exemplar_set in self.exemplar_sets:\n",
        "          for exemplar, label in  exemplar_set:\n",
        "            exemplar = exemplar.to(self.DEVICE)\n",
        "            feature = feature_extractor(exemplar)\n",
        "            feature = feature.squeeze()\n",
        "            feature.data = feature.data / feature.data.norm() # Normalize\n",
        "            X_train.append(feature.cpu().detach().numpy())\n",
        "            y_train.append(label)\n",
        "    \n",
        "    if method == 'KNN':\n",
        "      model = KNeighborsClassifier(n_neighbors = K_nn)\n",
        "    elif method == 'SVC':\n",
        "      model = LinearSVC()\n",
        "    self.model = model.fit(X_train, y_train)\n",
        "\n",
        "  # common classify function\n",
        "  def KNN_SVC_classify(self, images):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # --- prediction\n",
        "    X_pred = []\n",
        "    images = images.to(self.DEVICE)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    features = feature_extractor(images)\n",
        "    for feature in features:\n",
        "      feature = feature.squeeze()\n",
        "      feature.data = feature.data / feature.data.norm() # Normalize\n",
        "      X_pred.append(feature.cpu().detach().numpy())\n",
        "    \n",
        "    preds = self.model.predict(X_pred)\n",
        "    # --- end prediction\n",
        "    return torch.tensor(preds)\n",
        "    \n",
        "  # classify base on cosine similarity\n",
        "  def COS_classify(self, batch_imgs):\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    batch_imgs_size = batch_imgs.size(0)\n",
        "    feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "    feature_extractor.train(False)\n",
        "\n",
        "    means_exemplars = torch.cat(self.exemplar_mean_nn, dim=0)\n",
        "    means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "    means_exemplars = means_exemplars.transpose(1, 2) # means no normalized\n",
        "\n",
        "    feature = feature_extractor(batch_imgs) # features no normalized\n",
        "    \n",
        "    feature=feature.to('cpu')\n",
        "    means_exemplars = means_exemplars.to('cpu')\n",
        "\n",
        "    preds=[]\n",
        "    for a in feature:\n",
        "      a=a.detach().numpy()\n",
        "      aa=np.linalg.norm(a)\n",
        "      res=[]\n",
        "      for b in means_exemplars:\n",
        "        b=b.detach().numpy()\n",
        "        bb=np.linalg.norm(b)\n",
        "        dot = np.dot(a, b)\n",
        "        cos = dot / (aa * bb)\n",
        "        res.append(cos)\n",
        "      preds.append(np.argmax(np.array(res)))\n",
        "\n",
        "    # cleaning\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return torch.FloatTensor(preds).to(self.DEVICE)\n",
        "\n",
        "  # classification via fc layer (similar to lwf approach)\n",
        "  def FCC_classify(self, images):\n",
        "    _, preds = torch.max(torch.softmax(self.net(images), dim=1), dim=1, keepdim=False)\n",
        "    return preds\n",
        "  # NME classification from iCaRL paper\n",
        "  def classify(self, batch_imgs):\n",
        "      \"\"\"Classify images by nearest-mean-of-exemplars\n",
        "      Args:\n",
        "          batch_imgs: input image batch\n",
        "      Returns:\n",
        "          preds: Tensor of size (batch_size,)\n",
        "      \"\"\"\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      batch_imgs_size = batch_imgs.size(0)\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # update exemplar_means with the mean\n",
        "      # of all the train data for a given class\n",
        "\n",
        "      means_exemplars = torch.cat(self.exemplar_means, dim=0)\n",
        "      means_exemplars = torch.stack([means_exemplars] * batch_imgs_size)\n",
        "      means_exemplars = means_exemplars.transpose(1, 2) \n",
        "\n",
        "      feature = feature_extractor(batch_imgs) \n",
        "      aus_normalized_features = []\n",
        "      for el in feature: # Normalize\n",
        "          el.data = el.data / el.data.norm()\n",
        "          aus_normalized_features.append(el)\n",
        "\n",
        "      feature = torch.stack(aus_normalized_features,dim=0)\n",
        "\n",
        "      feature = feature.unsqueeze(2) \n",
        "      feature = feature.expand_as(means_exemplars) \n",
        "\n",
        "      means_exemplars = means_exemplars.to(self.DEVICE)\n",
        "\n",
        "      # Nearest prototype\n",
        "      preds = torch.argmin((feature - means_exemplars).pow(2).sum(1),dim=1)\n",
        "\n",
        "      # cleaning\n",
        "      torch.no_grad()\n",
        "      torch.cuda.empty_cache()\n",
        "      gc.collect()\n",
        "\n",
        "      return preds\n",
        "\n",
        "  # implementation of alg. 4 of icarl paper\n",
        "  # iCaRL ConstructExemplarSet\n",
        "  def construct_exemplar_set(self, tensors, m, label):\n",
        "    \"\"\"\n",
        "      Args:\n",
        "          tensors: train_subset containing a single label\n",
        "          m: number of exemplars allowed/exemplar set (class)\n",
        "          label: considered class\n",
        "    \"\"\"\n",
        "    torch.no_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    exemplar_set_indices = set()\n",
        "    exemplar_list_indices = []\n",
        "    exemplar_set = []\n",
        "    if self.herding:\n",
        "\n",
        "      feature_extractor = self.feature_extractor.to(self.DEVICE)\n",
        "      feature_extractor.train(False)\n",
        "\n",
        "      # Compute and cache features for each example\n",
        "      features = []\n",
        "\n",
        "      loader = DataLoader(tensors,batch_size=self.BATCH_SIZE,shuffle=True,drop_last=False,num_workers = 4)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for _, images, labels in loader:\n",
        "          images = images.to(self.DEVICE)\n",
        "          labels = labels.to(self.DEVICE)\n",
        "          feature = feature_extractor(images) \n",
        "\n",
        "          feature = feature / np.linalg.norm(feature.cpu()) # Normalize\n",
        "          \n",
        "          features.append(feature)\n",
        "\n",
        "      features_s = torch.cat(features)\n",
        "      \n",
        "      class_mean = features_s.mean(0)\n",
        "      class_mean = class_mean / np.linalg.norm(class_mean.cpu()) # Normalize\n",
        "      class_mean = torch.stack([class_mean]*features_s.size()[0])\n",
        "\n",
        "      summon = torch.zeros(1,features_s.size()[1]).to(self.DEVICE) #(1,num_features)\n",
        "      for k in range(1, (m + 1)):\n",
        "          S = torch.cat([summon]*features_s.size()[0]) # second addend, features in the exemplar set\n",
        "          results = pd.DataFrame((class_mean-(1/k)*(features_s + S)).pow(2).sum(1).cpu(), columns=['result']).sort_values('result')\n",
        "          results['index'] = results.index\n",
        "          results = results.to_numpy()\n",
        "\n",
        "          # select argmin not included in exemplar_set_indices\n",
        "          for i in range(results.shape[0]):\n",
        "            index = results[i, 1]\n",
        "            exemplar_k_index = tensors[index][0]\n",
        "            if exemplar_k_index not in exemplar_set_indices:\n",
        "              exemplar_k = tensors[index][1].unsqueeze(dim = 0) # take the image from the tuple (index, img, label)\n",
        "              exemplar_set.append((exemplar_k, label))\n",
        "              exemplar_k_index = tensors[index][0] # index of the img on the real dataset\n",
        "              \n",
        "              exemplar_list_indices.append(exemplar_k_index)\n",
        "              exemplar_set_indices.add(exemplar_k_index)\n",
        "              break\n",
        "\n",
        "          # features of the exemplar k\n",
        "          phi = feature_extractor(exemplar_k.to(self.DEVICE)) #feature_extractor(exemplar_k.to(self.DEVICE))\n",
        "          summon += phi # update sum of features\n",
        "    else:\n",
        "      tensors_size = len(tensors)\n",
        "      unique_random_indexes = random.sample(range(0, tensors_size), m) # random sample without replacement k exemplars\n",
        "      i = 0\n",
        "      for k in range(1, (m + 1)):\n",
        "        index = unique_random_indexes[i]\n",
        "        exemplar_k = tensors[index][1].unsqueeze(dim = 0)\n",
        "        exemplar_k_index = tensors[index][0]\n",
        "        exemplar_set.append((exemplar_k, label))\n",
        "        exemplar_set_indices.add(exemplar_k_index)\n",
        "        i = i + 1\n",
        "\n",
        "    # --- new ---\n",
        "    tensor_set = []\n",
        "    for i in range(0, len(tensors)):\n",
        "      t = tensors[i][1].unsqueeze(dim = 0)\n",
        "      tensor_set.append((t, label))\n",
        "    \n",
        "    self.exemplar_sets.append(exemplar_set) #update exemplar sets with the updated exemplars images\n",
        "    self.exemplar_sets_indices.append(exemplar_list_indices)\n",
        "\n",
        "    # this is used to compute more accurately the means of the exemplar (see also computeMeans and classify)\n",
        "    self.data_from_classes.append(tensor_set)\n",
        "\n",
        "    # cleaning\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  # build a exemplar dataset as a subset of the train dataset\n",
        "  def build_exemplars_dataset(self, train_dataset): #complete train dataset\n",
        "    all_exemplars_indices = []\n",
        "    for exemplar_set_indices in self.exemplar_sets_indices:\n",
        "        all_exemplars_indices.extend(exemplar_set_indices)\n",
        "\n",
        "    exemplars_dataset = Subset(train_dataset, all_exemplars_indices)\n",
        "    return exemplars_dataset\n",
        "\n",
        "  def update_representation(self, dataset, train_dataset_big, new_classes):\n",
        "    # 1 - retrieve the classes from the dataset (which is the current train_subset)\n",
        "    # 2 - retrieve the new classes\n",
        "    # 1,2 are done in the main_icarl\n",
        "    #gc.collect()\n",
        "\n",
        "    # 3 - increment classes\n",
        "    #          (add output nodes)\n",
        "    #          (update n_classes)\n",
        "    # 5        store network outputs with pre-update parameters\n",
        "    self.increment_classes(len(new_classes))\n",
        "\n",
        "    # 4 - combine current train_subset (dataset) with exemplars\n",
        "    #     to form a new augmented train dataset\n",
        "    # join the datasets\n",
        "    exemplars_dataset = self.build_exemplars_dataset(train_dataset_big)\n",
        "    #\n",
        "    if len(exemplars_dataset) > 0:\n",
        "      augmented_dataset = ConcatDataset(dataset, exemplars_dataset)\n",
        "      #augmented_dataset = utils.joinSubsets(train_dataset_big, [dataset, exemplars_dataset])\n",
        "    else: \n",
        "      augmented_dataset = dataset # first iteration\n",
        "\n",
        "    # 6 - run network training, with loss function\n",
        "\n",
        "    net = self.net\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=self.LR, weight_decay=self.WEIGHT_DECAY, momentum=self.MOMENTUM)\n",
        "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.MILESTONES, gamma=self.GAMMA, last_epoch=-1)\n",
        "\n",
        "    criterion = utils.getLossCriterion()\n",
        "\n",
        "    cudnn.benchmark # Calling this optimizes runtime\n",
        "    net = net.to(self.DEVICE)\n",
        "\n",
        "    # define the loader for the augmented_dataset\n",
        "    loader = DataLoader(augmented_dataset, batch_size=self.BATCH_SIZE,shuffle=True, num_workers=4, drop_last = True)\n",
        "\n",
        "    if len(self.exemplar_sets) > 0:\n",
        "      # QUA! #\n",
        "      #########################################################################################\n",
        "\n",
        "      #   print(self.oldNetTeachers[1].size())\n",
        "      if self.oldNet!=None:\n",
        "        self.moreOldNet=self.oldNet\n",
        "      self.oldNet= copy.deepcopy(net)\n",
        "\n",
        "      #########################################################################################\n",
        "    for epoch in range(self.NUM_EPOCHS):\n",
        "        print(\"NUM_EPOCHS: \",epoch,\"/\", self.NUM_EPOCHS)\n",
        "        for _, images, labels in loader:\n",
        "            # Bring data over the device of choice\n",
        "            images = images.to(self.DEVICE)\n",
        "            labels = labels.to(self.DEVICE)\n",
        "            net.train()\n",
        "\n",
        "            # PyTorch, by default, accumulates gradients after each backward pass\n",
        "            # We need to manually set the gradients to zero before starting a new iteration\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            \n",
        "\n",
        "            # QUA! #\n",
        "            #############################################################################################################\n",
        "            # Forward pass to the network\n",
        "            outputs = net(images)\n",
        "            # Loss = only classification on new classes\n",
        "            loss = self.class_loss(outputs, labels, col_start=self.n_known)\n",
        "            class_loss = loss.item() # Used for logging for debugging purposes\n",
        "\n",
        "            # Distilation loss for old classes, class loss on new classes\n",
        "            dist_loss = None\n",
        "            older_dist_loss=None\n",
        "            if len(self.exemplar_sets) > 0:\n",
        "              old_net=self.oldNet\n",
        "              out_old = torch.sigmoid(old_net(images))\n",
        "              dist_loss = self.dist_loss(outputs, out_old, col_end=self.n_known)\n",
        "\n",
        "              if self.moreOldNet!=None:\n",
        "                older_net=self.moreOldNet # old old net\n",
        "                older_out = torch.sigmoid(older_net(images))\n",
        "                older_dist_loss = self.double_dist_loss(outputs[0:128,0:-20], older_out[0:128,0:-10], col_end=self.n_known)\n",
        "                loss += older_dist_loss\n",
        "\n",
        "              loss += dist_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            ##############################################################################################################\n",
        "\n",
        "        scheduler.step()\n",
        "        print(\"LOSS: \", loss.item(), 'class loss', class_loss, 'dist loss', dist_loss.item() if dist_loss is not None else dist_loss, 'older dist loss',older_dist_loss.item() if older_dist_loss is not None else older_dist_loss)\n",
        "\n",
        "    self.net = copy.deepcopy(net)\n",
        "    self.feature_extractor = copy.deepcopy(net)\n",
        "    self.feature_extractor.fc = nn.Sequential()\n",
        "\n",
        "    #cleaning\n",
        "    del net\n",
        "    torch.cuda.empty_cache()\n",
        " # QUA! #\n",
        " ###################################################################################################################\n",
        "  def double_dist_loss(self,outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    rebalancing=None\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    dist_loss_func = self.bce_dist_loss\n",
        "    alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "    return 0.5*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        " ##################################################################################################################\n",
        "\n",
        "  def build_loss(self, class_loss_criterion, dist_loss_criterion, rebalancing=None, lambda0=1):\n",
        "    class_loss_func = None\n",
        "    dist_loss_func = None\n",
        "\n",
        "    if class_loss_criterion in ['l2', 'L2']:\n",
        "      class_loss_func = self.l2_class_loss\n",
        "    elif class_loss_criterion in ['bce', 'BCE']:\n",
        "      class_loss_func = self.bce_class_loss\n",
        "    elif class_loss_criterion in ['ce', 'CE']:\n",
        "      class_loss_func = self.ce_class_loss\n",
        "\n",
        "    if dist_loss_criterion in ['l2', 'L2']:\n",
        "      dist_loss_func = self.l2_dist_loss\n",
        "    elif dist_loss_criterion in ['bce', 'BCE']:\n",
        "      dist_loss_func = self.bce_dist_loss\n",
        "    elif dist_loss_criterion in ['ce', 'CE']:\n",
        "      dist_loss_func = self.ce_dist_loss\n",
        "\n",
        "    rebalancing = get_rebalancing(rebalancing)\n",
        "    \n",
        "    def class_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'class')\n",
        "      return alpha*class_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    def dist_loss(outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "      alpha = rebalancing(self.n_known, self.n_classes, 'dist')\n",
        "      return lambda0*alpha*dist_loss_func(outputs, labels, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "    \n",
        "    return class_loss, dist_loss\n",
        "\n",
        "  def bce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def bce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.bce_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def ce_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, self.reverse_index.getNodes(labels), decode=False, row_start=row_start, row_end=row_end, col_start=None, col_end=col_end)\n",
        "    \n",
        "  def ce_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.ce_loss(outputs, labels, decode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_class_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=True, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "  def l2_dist_loss(self, outputs, labels, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    return self.l2_loss(outputs, labels, encode=False, row_start=row_start, row_end=row_end, col_start=col_start, col_end=col_end)\n",
        "\n",
        "\n",
        "  def bce_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
        "\n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "\n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  def ce_loss(self, outputs, labels, decode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if decode:\n",
        "      labels = torch.argmax(labels, dim=1)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end])\n",
        "\n",
        "\n",
        "  def l2_loss(self, outputs, labels, encode=False, row_start=None, row_end=None, col_start=None, col_end=None):\n",
        "    criterion = nn.MSELoss(reduction = 'mean')\n",
        "    \n",
        "    if encode:\n",
        "      labels = utils._one_hot_encode(labels, self.n_classes, self.reverse_index, device=self.DEVICE)\n",
        "      labels = labels.type_as(outputs)\n",
        "    \n",
        "    return criterion(outputs[row_start:row_end, col_start:col_end], labels[row_start:row_end, col_start:col_end])\n",
        "\n",
        "\n",
        "  # implementation of alg. 5 of icarl paper\n",
        "  # iCaRL ReduceExemplarSet\n",
        "  def reduce_exemplar_sets(self, m):\n",
        "  \t    # i keep only the first m exemplar images\n",
        "        # where m is the UPDATED K/number_classes_seen\n",
        "        # the number of images per each exemplar set (class) progressively decreases\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:m] \n",
        "        for x, P_x in enumerate(self.exemplar_sets_indices):\n",
        "            self.exemplar_sets_indices[x] = P_x[:m] \n",
        "\n",
        "\n",
        "# ---------- \n",
        "from torch.utils.data import Dataset\n",
        "\"\"\"\n",
        "  Merge two different datasets (train and exemplars in our case)\n",
        "  format:\n",
        "  train\n",
        "  --------\n",
        "  exemplars\n",
        "  train leans on cifar100\n",
        "  exemplars is managed here (exemplar_transform is performed) => changed\n",
        "\"\"\"\n",
        "class ConcatDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, dataset1, dataset2):\n",
        "        self.dataset1 = dataset1\n",
        "        self.dataset2 = dataset2\n",
        "        self.l1 = len(dataset1)\n",
        "        self.l2 = len(dataset2)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        if index < self.l1:\n",
        "            _, image,label = self.dataset1[index] #here it leans on cifar100 get item\n",
        "            return _, image,label\n",
        "        else:\n",
        "            _, image, label = self.dataset2[index - self.l1]\n",
        "            return _, image,label\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.l1 + self.l2)\n",
        "#------------"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axANZYKXg6wn",
        "colab_type": "text"
      },
      "source": [
        "**Exemplars management**<br>\n",
        "From iCaRL. We have an exemplar set for each class that we have seen so far. The cardinality of each exemplar set is constant and it is equal, at any time, to m = K/t. Where K is a constraint equal to the amount of memory we're allocating for the exemplars and t is the number of classes that has been seen so far. Implementing iCaRL, whenever a group of (10) classes is trained, it is trained on the train data for those classes (as before) + the current exemplars sets.*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx0Woq8uhXyR",
        "colab_type": "code",
        "outputId": "b4cdc8e2-03a9-40d1-d9e0-b1db540df4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#from Cifar100.icarl_model import ICaRL\n",
        "\n",
        "# default params\n",
        "\n",
        "K = 2000\n",
        "n_classes = 0\n",
        "feature_size = 2048\n",
        "\n",
        "icarl = ICaRL(feature_size, n_classes, BATCH_SIZE, WEIGHT_DECAY, LR, GAMMA, NUM_EPOCHS, DEVICE,MILESTONES,MOMENTUM, K, herding, outputs_labels_mapping)\n",
        "icarl.cuda() "
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ICaRL(\n",
              "  (net): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Linear(in_features=64, out_features=0, bias=True)\n",
              "  )\n",
              "  (feature_extractor): ResNet(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (4): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
              "    (fc): Sequential()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppBh08iGBARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def computeAccuracy(method, net, loader, reverse_index, dataset, all_preds_cm, all_labels_cm):\n",
        "  total = 0.0\n",
        "  correct = 0.0\n",
        "  for indices, images, labels in loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "     \n",
        "        # add other classifiers\n",
        "        if classifier == 'NCM':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.classify(images)\n",
        "        elif classifier == 'FCC':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.FCC_classify(images)\n",
        "        elif classifier == 'KNN' or classifier == 'SVC':\n",
        "          preds = net.KNN_SVC_classify(images)\n",
        "          preds = preds.to(DEVICE)\n",
        "        elif classifier == 'COS':\n",
        "          labels = reverse_index.getNodes(labels)\n",
        "          preds = net.COS_classify(images)\n",
        "\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "  accuracy = correct/len(dataset)\n",
        "  if method == 'test':\n",
        "    all_preds_cm.extend(preds.tolist())\n",
        "    all_labels_cm.extend(labels.data.tolist())\n",
        "  return accuracy, all_preds_cm, all_labels_cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wupANuY0g1pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, reverse_index, K):\n",
        "    \n",
        "    all_accuracies = []\n",
        "    all_preds_cm = []\n",
        "    all_labels_cm = []\n",
        "    group_id=1\n",
        "    test_set = None\n",
        "\n",
        "    #for train_subset, val_subset, test_subset in zip(train_subsets, val_subsets, test_subsets):\n",
        "    for train_subset, test_subset in zip(train_subsets, test_subsets):\n",
        "        print(\"GROUP: \",group_id)\n",
        "        if test_set is None:\n",
        "          test_set = test_subset\n",
        "          train_set_big = train_subset\n",
        "        else:\n",
        "          test_set = utils.joinSubsets(test_dataset, [test_set, test_subset])\n",
        "\n",
        "        train_dataloader = DataLoader(train_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        #val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "        test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=True, num_workers=4)\n",
        "\n",
        "        ####### iCaRL implementation(following alg. 2,3,4,5 on icarl paper) ##################\n",
        "        \n",
        "        new_classes_examined = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        # 1 - update representation of the net \n",
        "        #  alg. 3 icarl\n",
        "        # (here the trainset will be augmented with the exemplars too)\n",
        "        # (here the classes are incremented too)\n",
        "        icarl.update_representation(train_subset, train_dataset, new_classes_examined)\n",
        "\n",
        "        # 2 - update m (number of images per class in the exemplar set corresponding to that class)\n",
        "        m = int(math.ceil(K/icarl.n_classes))\n",
        "\n",
        "        print(\"Reducing each exemplar set to size: {}\".format(m))\n",
        "\n",
        "        # 3 - reduce exemplar set for all the previously seen classes\n",
        "        # alg.5 icarl\n",
        "        icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "        # retrieve the 10 classes in the current subset\n",
        "        # NB. Here there will be exemplars too! (if i do not want that, use new_classes_examined)\n",
        "        classes_current_subset = list(train_dataset.df.loc[train_subset.indices, 'labels'].value_counts().index)\n",
        "        \n",
        "        print(\"Constructing exemplar sets class...\")\n",
        "        \n",
        "        # 4 - construct the exemplar set for the new classes\n",
        "        for y in new_classes_examined: # for each class in the current subset\n",
        "          \n",
        "          \n",
        "          # extract all the imgs in the train subset that are linked to this class\n",
        "          images_current_class = train_subset.dataset.df.loc[train_dataset.df['labels'] == y, 'data'] #they're TENSORS NOT IMAGES (the conversion will be done later)         \n",
        "          imgs_idxs = images_current_class.index # the indexes of all the images in the current classe being considered 0...49k\n",
        "          class_train_subset = Subset(train_dataset, imgs_idxs)#subset of the train dataset where i have all the imgs of class y\n",
        "\n",
        "          # alg. 4 icarl\n",
        "          icarl.construct_exemplar_set(class_train_subset,m,y)\n",
        "\n",
        "        # update the num classes seen so far\n",
        "        icarl.n_known = icarl.n_classes #n_classes is incremented in 1: updateRepresentation\n",
        "\n",
        "        print(\"Performing classification...\")\n",
        "\n",
        "        # start classifier\n",
        "        icarl.computeMeans()\n",
        "\n",
        "        # common training on exemplars for KNN and SVC classifier\n",
        "        if classifier == 'KNN':\n",
        "          K_nn = 5\n",
        "          icarl.modelTrain(classifier, K_nn)\n",
        "        elif classifier == 'SVC':\n",
        "          icarl.modelTrain(classifier)\n",
        "\n",
        "        #train accuracy\n",
        "        train_accuracy, _, _ = computeAccuracy('train',icarl, train_dataloader, reverse_index, train_subset,all_preds_cm, all_labels_cm)\n",
        "        print ('Train Accuracy (on current group): %.2f\\n' % (100.0 * train_accuracy))\n",
        "\n",
        "        # --- not used\n",
        "        #val_accuracy, _, _ = computeAccuracy('val',icarl, val_dataloader, reverse_index, val_subset)\n",
        "        #print ('Val Accuracy (on current group): %.2f\\n' % (100.0 * val_accuracy))\n",
        "\n",
        "        #test\n",
        "        test_accuracy, all_preds_cm, all_labels_cm = computeAccuracy('test',icarl, test_dataloader, reverse_index, test_set, all_preds_cm, all_labels_cm)\n",
        "        all_accuracies.append(test_accuracy)\n",
        "        print ('Test Accuracy (all groups seen so far): %.2f\\n' % (100.0 * test_accuracy))\n",
        "\n",
        "        print (\"the model knows %d classes:\\n \" % icarl.n_known)\n",
        "\n",
        "        group_id+=1\n",
        "        \n",
        "    return all_accuracies, np.array(all_preds_cm), np.array(all_labels_cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bmxtCL8AvYD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9c7cce0-8881-40d6-dd55-1017bfd66644"
      },
      "source": [
        "accuracies, all_preds_cm, all_labels_cm = incrementalTraining(icarl, train_subsets, val_subsets, test_subsets,eval_transform, outputs_labels_mapping, K)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GROUP:  1\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.3240870237350464 class loss 0.3240870237350464 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.3216666281223297 class loss 0.3216666281223297 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.31903570890426636 class loss 0.31903570890426636 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.314587265253067 class loss 0.314587265253067 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.3178633153438568 class loss 0.3178633153438568 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.3196324110031128 class loss 0.3196324110031128 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.30564337968826294 class loss 0.30564337968826294 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.2746237516403198 class loss 0.2746237516403198 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.2683528959751129 class loss 0.2683528959751129 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.25156673789024353 class loss 0.25156673789024353 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.2589839696884155 class loss 0.2589839696884155 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.25529783964157104 class loss 0.25529783964157104 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.25039955973625183 class loss 0.25039955973625183 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.22634994983673096 class loss 0.22634994983673096 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.22699077427387238 class loss 0.22699077427387238 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.22985582053661346 class loss 0.22985582053661346 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.23418226838111877 class loss 0.23418226838111877 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.2169012874364853 class loss 0.2169012874364853 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.22185257077217102 class loss 0.22185257077217102 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.2102571725845337 class loss 0.2102571725845337 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.19020403921604156 class loss 0.19020403921604156 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.2282930463552475 class loss 0.2282930463552475 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.21050743758678436 class loss 0.21050743758678436 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.20546360313892365 class loss 0.20546360313892365 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.18850205838680267 class loss 0.18850205838680267 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.1896398365497589 class loss 0.1896398365497589 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.1656825989484787 class loss 0.1656825989484787 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.17463086545467377 class loss 0.17463086545467377 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1663445085287094 class loss 0.1663445085287094 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.1551351547241211 class loss 0.1551351547241211 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.1891760677099228 class loss 0.1891760677099228 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.1267145425081253 class loss 0.1267145425081253 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12297296524047852 class loss 0.12297296524047852 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.1369365155696869 class loss 0.1369365155696869 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.10208306461572647 class loss 0.10208306461572647 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.1335008293390274 class loss 0.1335008293390274 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12956520915031433 class loss 0.12956520915031433 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.12375475466251373 class loss 0.12375475466251373 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.09986694902181625 class loss 0.09986694902181625 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.14606161415576935 class loss 0.14606161415576935 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.09754731506109238 class loss 0.09754731506109238 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.09740062803030014 class loss 0.09740062803030014 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.10040692239999771 class loss 0.10040692239999771 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.11066289246082306 class loss 0.11066289246082306 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.09932037442922592 class loss 0.09932037442922592 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.0945892259478569 class loss 0.0945892259478569 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07031076401472092 class loss 0.07031076401472092 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.09499340504407883 class loss 0.09499340504407883 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.06136387586593628 class loss 0.06136387586593628 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.05059505254030228 class loss 0.05059505254030228 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.05754561349749565 class loss 0.05754561349749565 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.04733278602361679 class loss 0.04733278602361679 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.06138409301638603 class loss 0.06138409301638603 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.0382576547563076 class loss 0.0382576547563076 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.03841186687350273 class loss 0.03841186687350273 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.02619354799389839 class loss 0.02619354799389839 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.03808718919754028 class loss 0.03808718919754028 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.04755885526537895 class loss 0.04755885526537895 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.04099380970001221 class loss 0.04099380970001221 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.05642525479197502 class loss 0.05642525479197502 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.033107858151197433 class loss 0.033107858151197433 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.035760339349508286 class loss 0.035760339349508286 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.030459214001893997 class loss 0.030459214001893997 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.013653377071022987 class loss 0.013653377071022987 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.03248700127005577 class loss 0.03248700127005577 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.02115808241069317 class loss 0.02115808241069317 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.014021637849509716 class loss 0.014021637849509716 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.03452315926551819 class loss 0.03452315926551819 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.01890651136636734 class loss 0.01890651136636734 dist loss None older dist loss None\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.01808309368789196 class loss 0.01808309368789196 dist loss None older dist loss None\n",
            "Reducing each exemplar set to size: 200\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 97.16\n",
            "\n",
            "Test Accuracy (all groups seen so far): 81.90\n",
            "\n",
            "the model knows 10 classes:\n",
            " \n",
            "GROUP:  2\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.14226152002811432 class loss 0.09196241945028305 dist loss 0.05029909685254097 older dist loss None\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12702907621860504 class loss 0.0774063840508461 dist loss 0.04962269216775894 older dist loss None\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.1099139004945755 class loss 0.05914356932044029 dist loss 0.05077033117413521 older dist loss None\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.10961487144231796 class loss 0.060873378068208694 dist loss 0.04874149337410927 older dist loss None\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.11532940715551376 class loss 0.0627143532037735 dist loss 0.052615053951740265 older dist loss None\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.10555679351091385 class loss 0.05392073467373848 dist loss 0.05163605883717537 older dist loss None\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.09830985963344574 class loss 0.04503348097205162 dist loss 0.05327637866139412 older dist loss None\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.10247015953063965 class loss 0.04913436248898506 dist loss 0.05333579704165459 older dist loss None\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.10784744471311569 class loss 0.047988954931497574 dist loss 0.05985848978161812 older dist loss None\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.0966644138097763 class loss 0.04543207213282585 dist loss 0.05123234540224075 older dist loss None\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.08511237055063248 class loss 0.04006253555417061 dist loss 0.04504983499646187 older dist loss None\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.09333761781454086 class loss 0.04796474799513817 dist loss 0.045372869819402695 older dist loss None\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.10016060620546341 class loss 0.04290086776018143 dist loss 0.05725973844528198 older dist loss None\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.09364999830722809 class loss 0.03660815954208374 dist loss 0.05704184249043465 older dist loss None\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.08457135409116745 class loss 0.033769164234399796 dist loss 0.050802189856767654 older dist loss None\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.09827224910259247 class loss 0.0443599633872509 dist loss 0.05391228199005127 older dist loss None\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.0917130559682846 class loss 0.03415274620056152 dist loss 0.057560306042432785 older dist loss None\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.08480684459209442 class loss 0.033371008932590485 dist loss 0.051435839384794235 older dist loss None\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.10156740248203278 class loss 0.040757786482572556 dist loss 0.06080961227416992 older dist loss None\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.0847008004784584 class loss 0.03380345180630684 dist loss 0.050897348672151566 older dist loss None\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.08821919560432434 class loss 0.03558728098869324 dist loss 0.052631910890340805 older dist loss None\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.08045902103185654 class loss 0.028500908985733986 dist loss 0.0519581101834774 older dist loss None\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.08654342591762543 class loss 0.03362894430756569 dist loss 0.05291447788476944 older dist loss None\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.08171183615922928 class loss 0.027190405875444412 dist loss 0.054521430283784866 older dist loss None\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.08524031192064285 class loss 0.025905871763825417 dist loss 0.05933443829417229 older dist loss None\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.0770498514175415 class loss 0.025618037208914757 dist loss 0.0514318123459816 older dist loss None\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.09234209358692169 class loss 0.035017311573028564 dist loss 0.057324785739183426 older dist loss None\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.07857222855091095 class loss 0.025638431310653687 dist loss 0.052933793514966965 older dist loss None\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.0822334736585617 class loss 0.032642662525177 dist loss 0.049590814858675 older dist loss None\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.07872448861598969 class loss 0.027064507827162743 dist loss 0.05165997892618179 older dist loss None\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.07532806694507599 class loss 0.023594627156853676 dist loss 0.05173344165086746 older dist loss None\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.0774562880396843 class loss 0.01778198778629303 dist loss 0.059674300253391266 older dist loss None\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.07794877141714096 class loss 0.027232063934206963 dist loss 0.05071670934557915 older dist loss None\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.07150531560182571 class loss 0.021587004885077477 dist loss 0.04991831257939339 older dist loss None\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.07552371174097061 class loss 0.02561849169433117 dist loss 0.04990522190928459 older dist loss None\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.0717780739068985 class loss 0.019332557916641235 dist loss 0.05244551971554756 older dist loss None\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.07552897930145264 class loss 0.02089177630841732 dist loss 0.05463720113039017 older dist loss None\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.08536034822463989 class loss 0.022748928517103195 dist loss 0.0626114159822464 older dist loss None\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.07209781557321548 class loss 0.02224625274538994 dist loss 0.049851562827825546 older dist loss None\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.07207859307527542 class loss 0.015915943309664726 dist loss 0.056162651628255844 older dist loss None\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.0735163539648056 class loss 0.01797414757311344 dist loss 0.05554220825433731 older dist loss None\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.0650864839553833 class loss 0.013406115584075451 dist loss 0.051680367439985275 older dist loss None\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.06997311860322952 class loss 0.020713889971375465 dist loss 0.04925922676920891 older dist loss None\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.06493014097213745 class loss 0.017447324469685555 dist loss 0.04748281463980675 older dist loss None\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.06525465101003647 class loss 0.017957745119929314 dist loss 0.047296907752752304 older dist loss None\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.06089089438319206 class loss 0.012496828101575375 dist loss 0.04839406535029411 older dist loss None\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.07376916706562042 class loss 0.02359389141201973 dist loss 0.05017527937889099 older dist loss None\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.07875223457813263 class loss 0.020919194445014 dist loss 0.05783304199576378 older dist loss None\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.06447891145944595 class loss 0.012063266709446907 dist loss 0.052415646612644196 older dist loss None\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.047726865857839584 class loss 0.006429316010326147 dist loss 0.0412975512444973 older dist loss None\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.05101950466632843 class loss 0.00788706075400114 dist loss 0.043132442981004715 older dist loss None\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.049280405044555664 class loss 0.007482010405510664 dist loss 0.04179839417338371 older dist loss None\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.05199428275227547 class loss 0.006057679187506437 dist loss 0.04593660309910774 older dist loss None\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.04671068862080574 class loss 0.004007162060588598 dist loss 0.042703527957201004 older dist loss None\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.04819350317120552 class loss 0.0032754316926002502 dist loss 0.04491807147860527 older dist loss None\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.05084555223584175 class loss 0.004692007787525654 dist loss 0.04615354537963867 older dist loss None\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.04606234282255173 class loss 0.004597064107656479 dist loss 0.04146527871489525 older dist loss None\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.043684929609298706 class loss 0.004143647383898497 dist loss 0.03954128175973892 older dist loss None\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.04968496412038803 class loss 0.003436131402850151 dist loss 0.04624883085489273 older dist loss None\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.04569421336054802 class loss 0.002977175172418356 dist loss 0.042717039585113525 older dist loss None\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.044794898480176926 class loss 0.003384878160431981 dist loss 0.04141002148389816 older dist loss None\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.04912479594349861 class loss 0.005606208927929401 dist loss 0.043518587946891785 older dist loss None\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.043375879526138306 class loss 0.0034173280000686646 dist loss 0.03995855152606964 older dist loss None\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.04831705242395401 class loss 0.0044972761534154415 dist loss 0.04381977766752243 older dist loss None\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.04772976040840149 class loss 0.0051029641181230545 dist loss 0.042626794427633286 older dist loss None\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.04387441650032997 class loss 0.001786468201316893 dist loss 0.04208794981241226 older dist loss None\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.04552055522799492 class loss 0.0036708940751850605 dist loss 0.041849661618471146 older dist loss None\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.048413414508104324 class loss 0.002795607317239046 dist loss 0.045617807656526566 older dist loss None\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.04575634375214577 class loss 0.0022571298759430647 dist loss 0.043499212712049484 older dist loss None\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.04183513671159744 class loss 0.002230835845693946 dist loss 0.039604302495718 older dist loss None\n",
            "Reducing each exemplar set to size: 100\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 91.58\n",
            "\n",
            "Test Accuracy (all groups seen so far): 74.75\n",
            "\n",
            "the model knows 20 classes:\n",
            " \n",
            "GROUP:  3\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.17020654678344727 class loss 0.05021755024790764 dist loss 0.06094850227236748 older dist loss 0.05904048681259155\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.16692885756492615 class loss 0.0454445444047451 dist loss 0.06281542778015137 older dist loss 0.05866888165473938\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.16506843268871307 class loss 0.05080729350447655 dist loss 0.05604719743132591 older dist loss 0.05821394547820091\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.14017699658870697 class loss 0.03199983388185501 dist loss 0.05551035702228546 older dist loss 0.0526668019592762\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.14242112636566162 class loss 0.03136000409722328 dist loss 0.0589955598115921 older dist loss 0.052065569907426834\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.15047092735767365 class loss 0.03927988559007645 dist loss 0.06156532093882561 older dist loss 0.049625713378190994\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.1304025948047638 class loss 0.02381792478263378 dist loss 0.0560934916138649 older dist loss 0.05049118027091026\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.13055859506130219 class loss 0.023597754538059235 dist loss 0.057392530143260956 older dist loss 0.049568310379981995\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.13349619507789612 class loss 0.0275954008102417 dist loss 0.0553605854511261 older dist loss 0.05054020881652832\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.1329193264245987 class loss 0.022021688520908356 dist loss 0.056421320885419846 older dist loss 0.05447631701827049\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.14702032506465912 class loss 0.027147557586431503 dist loss 0.062305040657520294 older dist loss 0.05756772682070732\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.14960239827632904 class loss 0.03368880972266197 dist loss 0.06089204177260399 older dist loss 0.05502155050635338\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.13824880123138428 class loss 0.025703448802232742 dist loss 0.0531143918633461 older dist loss 0.05943096801638603\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.13735446333885193 class loss 0.02369016222655773 dist loss 0.059240587055683136 older dist loss 0.05442371591925621\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12898477911949158 class loss 0.02235584706068039 dist loss 0.055629294365644455 older dist loss 0.05099964141845703\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.13830655813217163 class loss 0.025913534685969353 dist loss 0.05843394249677658 older dist loss 0.053959090262651443\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.13088226318359375 class loss 0.02369881421327591 dist loss 0.05664071440696716 older dist loss 0.050542738288640976\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.1279817670583725 class loss 0.02047816663980484 dist loss 0.05654211342334747 older dist loss 0.05096149072051048\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12384618818759918 class loss 0.018872180953621864 dist loss 0.05651235207915306 older dist loss 0.048461657017469406\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.13112816214561462 class loss 0.0234421007335186 dist loss 0.05738845095038414 older dist loss 0.05029761418700218\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12138509750366211 class loss 0.02209058403968811 dist loss 0.05413772165775299 older dist loss 0.04515678808093071\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.13944494724273682 class loss 0.02196453884243965 dist loss 0.06119292229413986 older dist loss 0.0562874935567379\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12756337225437164 class loss 0.019900845363736153 dist loss 0.056181322783231735 older dist loss 0.051481205970048904\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.1270942986011505 class loss 0.017990920692682266 dist loss 0.059312306344509125 older dist loss 0.04979106783866882\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12472667545080185 class loss 0.02101975679397583 dist loss 0.05547626316547394 older dist loss 0.04823065549135208\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.13256829977035522 class loss 0.01728808879852295 dist loss 0.062183927744627 older dist loss 0.05309627577662468\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.13362279534339905 class loss 0.017559105530381203 dist loss 0.06326302886009216 older dist loss 0.052800651639699936\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12559005618095398 class loss 0.01799491047859192 dist loss 0.05480101332068443 older dist loss 0.052794136106967926\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1216767206788063 class loss 0.018067343160510063 dist loss 0.05385689437389374 older dist loss 0.04975248500704765\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12942664325237274 class loss 0.023770004510879517 dist loss 0.05577061325311661 older dist loss 0.04988602548837662\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.12968847155570984 class loss 0.017971953377127647 dist loss 0.05844235420227051 older dist loss 0.05327416583895683\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.12484407424926758 class loss 0.018354780972003937 dist loss 0.05699307844042778 older dist loss 0.04949621111154556\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.13402047753334045 class loss 0.018968094140291214 dist loss 0.060819461941719055 older dist loss 0.054232921451330185\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.127979576587677 class loss 0.020311478525400162 dist loss 0.0569629929959774 older dist loss 0.05070510134100914\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.1223439872264862 class loss 0.016387589275836945 dist loss 0.057914670556783676 older dist loss 0.048041731119155884\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.126206174492836 class loss 0.014207578264176846 dist loss 0.0609002485871315 older dist loss 0.05109834671020508\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12227697670459747 class loss 0.019500771537423134 dist loss 0.056916333734989166 older dist loss 0.04585987329483032\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.13137072324752808 class loss 0.015613730065524578 dist loss 0.05923070013523102 older dist loss 0.0565263032913208\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.13072718679904938 class loss 0.01686081662774086 dist loss 0.057693224400281906 older dist loss 0.05617314577102661\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12037794291973114 class loss 0.013376647606492043 dist loss 0.05559549480676651 older dist loss 0.05140579864382744\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.1174803227186203 class loss 0.01269887387752533 dist loss 0.05523575842380524 older dist loss 0.049545690417289734\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.13056668639183044 class loss 0.013520810753107071 dist loss 0.06299076229333878 older dist loss 0.05405512452125549\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1124601662158966 class loss 0.013172964565455914 dist loss 0.05368756875395775 older dist loss 0.04559962823987007\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12345921248197556 class loss 0.017035802826285362 dist loss 0.059327855706214905 older dist loss 0.04709555208683014\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12147170305252075 class loss 0.0127714267000556 dist loss 0.05619455501437187 older dist loss 0.052505720406770706\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.11447738111019135 class loss 0.010895231738686562 dist loss 0.05496395379304886 older dist loss 0.048618193715810776\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.12628936767578125 class loss 0.012215541675686836 dist loss 0.062415219843387604 older dist loss 0.051658596843481064\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.12357062101364136 class loss 0.011118898168206215 dist loss 0.06012723594903946 older dist loss 0.052324485033750534\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12365464866161346 class loss 0.013454183004796505 dist loss 0.0587209016084671 older dist loss 0.05147956684231758\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11168656498193741 class loss 0.006987657397985458 dist loss 0.05639396607875824 older dist loss 0.04830494150519371\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.09781235456466675 class loss 0.0039671314880251884 dist loss 0.04945727437734604 older dist loss 0.044387947767972946\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.10973027348518372 class loss 0.006030097138136625 dist loss 0.05561913549900055 older dist loss 0.048081036657094955\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.10396507382392883 class loss 0.003720899811014533 dist loss 0.05287529528141022 older dist loss 0.04736888036131859\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.10525251924991608 class loss 0.0043837809935212135 dist loss 0.05259004980325699 older dist loss 0.04827868938446045\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.10049004852771759 class loss 0.004971582908183336 dist loss 0.05096140131354332 older dist loss 0.04455706477165222\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.09593288600444794 class loss 0.0036278129555284977 dist loss 0.05013468116521835 older dist loss 0.04217039421200752\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.09396831691265106 class loss 0.003707937663421035 dist loss 0.047930940985679626 older dist loss 0.04232943430542946\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.09926500916481018 class loss 0.004319812171161175 dist loss 0.05125534161925316 older dist loss 0.04368985444307327\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.09392061829566956 class loss 0.002557306084781885 dist loss 0.0487632229924202 older dist loss 0.04260009154677391\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.08953969180583954 class loss 0.004539940506219864 dist loss 0.04767606779932976 older dist loss 0.03732367977499962\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.09865128993988037 class loss 0.004002581816166639 dist loss 0.05147218331694603 older dist loss 0.04317652806639671\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.09331350028514862 class loss 0.0028295654337853193 dist loss 0.04827164113521576 older dist loss 0.042212288826704025\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.097148597240448 class loss 0.005938700865954161 dist loss 0.05069764703512192 older dist loss 0.04051225259900093\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.0913318395614624 class loss 0.0036585635971277952 dist loss 0.04761333018541336 older dist loss 0.04005994647741318\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.09797161817550659 class loss 0.0044364482164382935 dist loss 0.05121435225009918 older dist loss 0.042320821434259415\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.09918798506259918 class loss 0.003391896141692996 dist loss 0.05217555910348892 older dist loss 0.043620526790618896\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.10331776738166809 class loss 0.005020029842853546 dist loss 0.05074603110551834 older dist loss 0.0475517101585865\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.1017536073923111 class loss 0.003043417353183031 dist loss 0.053790703415870667 older dist loss 0.044919487088918686\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.09953218698501587 class loss 0.00436454638838768 dist loss 0.05069786310195923 older dist loss 0.04446977749466896\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.095380499958992 class loss 0.002327561378479004 dist loss 0.049038227647542953 older dist loss 0.04401471093297005\n",
            "Reducing each exemplar set to size: 67\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 79.66\n",
            "\n",
            "Test Accuracy (all groups seen so far): 67.10\n",
            "\n",
            "the model knows 30 classes:\n",
            " \n",
            "GROUP:  4\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.15000109374523163 class loss 0.04549393430352211 dist loss 0.058566704392433167 older dist loss 0.04594045877456665\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.14577916264533997 class loss 0.04029044881463051 dist loss 0.059990182518959045 older dist loss 0.04549853876233101\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.13624164462089539 class loss 0.038899507373571396 dist loss 0.05408821254968643 older dist loss 0.04325392469763756\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.13250261545181274 class loss 0.031975168734788895 dist loss 0.05784621089696884 older dist loss 0.042681243270635605\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.13712523877620697 class loss 0.03469296917319298 dist loss 0.058722637593746185 older dist loss 0.04370963200926781\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.1343456357717514 class loss 0.03304894268512726 dist loss 0.056587979197502136 older dist loss 0.04470871016383171\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.13000361621379852 class loss 0.02831008844077587 dist loss 0.058554165065288544 older dist loss 0.04313936457037926\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.127470463514328 class loss 0.02764296531677246 dist loss 0.058105386793613434 older dist loss 0.041722122579813004\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12659843266010284 class loss 0.027459358796477318 dist loss 0.0564006082713604 older dist loss 0.04273846372961998\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.1293647736310959 class loss 0.026041880249977112 dist loss 0.059279266744852066 older dist loss 0.044043634086847305\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.13740386068820953 class loss 0.03076973557472229 dist loss 0.06196652725338936 older dist loss 0.044667597860097885\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.13529938459396362 class loss 0.0265401192009449 dist loss 0.0642690509557724 older dist loss 0.04449021443724632\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12899193167686462 class loss 0.025873208418488503 dist loss 0.06046295911073685 older dist loss 0.04265577346086502\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.1263638436794281 class loss 0.025621607899665833 dist loss 0.057320959866046906 older dist loss 0.04342127591371536\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12560901045799255 class loss 0.019508810713887215 dist loss 0.06137615442276001 older dist loss 0.04472404718399048\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.12534604966640472 class loss 0.02404021844267845 dist loss 0.05970757082104683 older dist loss 0.04159826040267944\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12439531087875366 class loss 0.021202649921178818 dist loss 0.05967356264591217 older dist loss 0.043519098311662674\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.12774361670017242 class loss 0.026356065645813942 dist loss 0.05834857374429703 older dist loss 0.043038975447416306\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12771473824977875 class loss 0.025504393503069878 dist loss 0.06116132810711861 older dist loss 0.04104902222752571\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.128718763589859 class loss 0.025748858228325844 dist loss 0.059660784900188446 older dist loss 0.04330912232398987\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12472164630889893 class loss 0.022346172481775284 dist loss 0.059092696756124496 older dist loss 0.04328277334570885\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.11943060904741287 class loss 0.020443875342607498 dist loss 0.05777473747730255 older dist loss 0.04121199622750282\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12406836450099945 class loss 0.016908524557948112 dist loss 0.06275459378957748 older dist loss 0.044405248016119\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.13628347218036652 class loss 0.0220683254301548 dist loss 0.06670203804969788 older dist loss 0.04751310870051384\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.125234454870224 class loss 0.016227511689066887 dist loss 0.0639757439494133 older dist loss 0.04503120854496956\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12314244359731674 class loss 0.018442610278725624 dist loss 0.060813382267951965 older dist loss 0.043886449187994\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.13093312084674835 class loss 0.02439052239060402 dist loss 0.06362473964691162 older dist loss 0.04291785508394241\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.1316433548927307 class loss 0.0218647588044405 dist loss 0.06478019058704376 older dist loss 0.0449984073638916\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1210038959980011 class loss 0.018071850761771202 dist loss 0.06010317802429199 older dist loss 0.042828869074583054\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.11761824041604996 class loss 0.01931820623576641 dist loss 0.057010844349861145 older dist loss 0.04128918796777725\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.1189575344324112 class loss 0.016231629997491837 dist loss 0.0602387934923172 older dist loss 0.042487114667892456\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.12497775256633759 class loss 0.019093217328190804 dist loss 0.061475709080696106 older dist loss 0.044408828020095825\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12008778750896454 class loss 0.015343256294727325 dist loss 0.061324864625930786 older dist loss 0.04341966658830643\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12121536582708359 class loss 0.011319582350552082 dist loss 0.06260573863983154 older dist loss 0.04729004576802254\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12527048587799072 class loss 0.020474249497056007 dist loss 0.06055513024330139 older dist loss 0.044241100549697876\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.1257559359073639 class loss 0.015116835944354534 dist loss 0.06426945328712463 older dist loss 0.04636964946985245\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12350744754076004 class loss 0.01255969237536192 dist loss 0.06468778848648071 older dist loss 0.04625996574759483\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.11752523481845856 class loss 0.010482762940227985 dist loss 0.061375148594379425 older dist loss 0.04566732048988342\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12193825095891953 class loss 0.013116150163114071 dist loss 0.06139595806598663 older dist loss 0.047426141798496246\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12223231792449951 class loss 0.01411765068769455 dist loss 0.06365207582712173 older dist loss 0.04446259140968323\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.1276564747095108 class loss 0.015657097101211548 dist loss 0.0652540847659111 older dist loss 0.046745289117097855\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.1159721240401268 class loss 0.011918515898287296 dist loss 0.059661224484443665 older dist loss 0.044392384588718414\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.11687630414962769 class loss 0.014155978336930275 dist loss 0.06087503582239151 older dist loss 0.04184529185295105\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12531515955924988 class loss 0.01539642084389925 dist loss 0.06485304236412048 older dist loss 0.04506570100784302\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12300147861242294 class loss 0.012421831488609314 dist loss 0.06570228934288025 older dist loss 0.04487735778093338\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.12027536332607269 class loss 0.013310039415955544 dist loss 0.06160089746117592 older dist loss 0.04536442086100578\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.11674746870994568 class loss 0.012935449369251728 dist loss 0.060343701392412186 older dist loss 0.04346831515431404\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.11868928372859955 class loss 0.011386360041797161 dist loss 0.06252794712781906 older dist loss 0.04477497562766075\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.1290069967508316 class loss 0.012199347838759422 dist loss 0.06768445670604706 older dist loss 0.04912319406867027\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.10220605880022049 class loss 0.005755147431045771 dist loss 0.05724175274372101 older dist loss 0.03920915722846985\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.10597006231546402 class loss 0.0063622863963246346 dist loss 0.05877181515097618 older dist loss 0.04083596169948578\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.10233090817928314 class loss 0.00646701455116272 dist loss 0.05607614666223526 older dist loss 0.03978775069117546\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.1050645112991333 class loss 0.004009507596492767 dist loss 0.05838986858725548 older dist loss 0.04266513139009476\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.10169415920972824 class loss 0.004810791928321123 dist loss 0.05576816201210022 older dist loss 0.041115205734968185\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.1022270992398262 class loss 0.005744345486164093 dist loss 0.056007251143455505 older dist loss 0.040475502610206604\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.1005273386836052 class loss 0.005762902554124594 dist loss 0.054935142397880554 older dist loss 0.03982929512858391\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.1026543453335762 class loss 0.0038264275062829256 dist loss 0.05662723630666733 older dist loss 0.04220068082213402\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.10145281255245209 class loss 0.0056761205196380615 dist loss 0.05528680980205536 older dist loss 0.04048987850546837\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.1040663942694664 class loss 0.007373794913291931 dist loss 0.05717424675822258 older dist loss 0.03951835259795189\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.10710640251636505 class loss 0.005773394834250212 dist loss 0.058117255568504333 older dist loss 0.04321575164794922\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.10203202813863754 class loss 0.003883583936840296 dist loss 0.05724986642599106 older dist loss 0.04089857637882233\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.10282397270202637 class loss 0.003914583474397659 dist loss 0.05867570638656616 older dist loss 0.040233686566352844\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.1015457808971405 class loss 0.0031019661109894514 dist loss 0.05930546298623085 older dist loss 0.03913835436105728\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.09899652004241943 class loss 0.004377167206257582 dist loss 0.05493268370628357 older dist loss 0.039686672389507294\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.09600884467363358 class loss 0.0032371636480093002 dist loss 0.05549202114343643 older dist loss 0.03727966174483299\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.10142931342124939 class loss 0.004710295703262091 dist loss 0.057105161249637604 older dist loss 0.03961385414004326\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.10266190767288208 class loss 0.003781296778470278 dist loss 0.05808834731578827 older dist loss 0.04079226404428482\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.09718811511993408 class loss 0.0032569325994700193 dist loss 0.055104851722717285 older dist loss 0.03882632777094841\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.10000298917293549 class loss 0.004632917232811451 dist loss 0.056503549218177795 older dist loss 0.038866519927978516\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.10469569265842438 class loss 0.003847603453323245 dist loss 0.058977462351322174 older dist loss 0.04187062382698059\n",
            "Reducing each exemplar set to size: 50\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 72.78\n",
            "\n",
            "Test Accuracy (all groups seen so far): 60.30\n",
            "\n",
            "the model knows 40 classes:\n",
            " \n",
            "GROUP:  5\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1355718970298767 class loss 0.029120970517396927 dist loss 0.06344430893659592 older dist loss 0.04300662502646446\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.1261795461177826 class loss 0.024191129952669144 dist loss 0.06052888184785843 older dist loss 0.04145953804254532\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.12619498372077942 class loss 0.023503856733441353 dist loss 0.06241319701075554 older dist loss 0.040277931839227676\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.1261483132839203 class loss 0.02058563567698002 dist loss 0.06403987109661102 older dist loss 0.041522808372974396\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12323012948036194 class loss 0.019785890355706215 dist loss 0.06301159411668777 older dist loss 0.0404326431453228\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.12178713083267212 class loss 0.021200960502028465 dist loss 0.0604463592171669 older dist loss 0.040139809250831604\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12662942707538605 class loss 0.023364149034023285 dist loss 0.06254556775093079 older dist loss 0.040719714015722275\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.12242239713668823 class loss 0.017664862796664238 dist loss 0.06430921703577042 older dist loss 0.04044831544160843\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.126918762922287 class loss 0.018011165782809258 dist loss 0.06554242223501205 older dist loss 0.04336516931653023\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.12345648556947708 class loss 0.01846647448837757 dist loss 0.06426643580198288 older dist loss 0.04072357714176178\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12078282237052917 class loss 0.018101703375577927 dist loss 0.06185932084918022 older dist loss 0.04082179442048073\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.1216510608792305 class loss 0.01679394207894802 dist loss 0.06395868957042694 older dist loss 0.04089843109250069\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12149032950401306 class loss 0.016432488337159157 dist loss 0.06543328613042831 older dist loss 0.03962455689907074\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.1194973886013031 class loss 0.015199792571365833 dist loss 0.06457826495170593 older dist loss 0.03971932828426361\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12116579711437225 class loss 0.014417096972465515 dist loss 0.06550977379083633 older dist loss 0.0412389300763607\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.11926954984664917 class loss 0.013240747153759003 dist loss 0.06480958312749863 older dist loss 0.04121921956539154\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12424427270889282 class loss 0.015198825858533382 dist loss 0.06689535826444626 older dist loss 0.04215008765459061\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.12040357291698456 class loss 0.011930437758564949 dist loss 0.06689917296171188 older dist loss 0.041573964059352875\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.11957112699747086 class loss 0.010650110431015491 dist loss 0.06729929149150848 older dist loss 0.041621726006269455\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12004617601633072 class loss 0.014968069270253181 dist loss 0.06539172679185867 older dist loss 0.039686378091573715\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.11907918751239777 class loss 0.012036642991006374 dist loss 0.0644206628203392 older dist loss 0.042621880769729614\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.11714263260364532 class loss 0.009665380232036114 dist loss 0.0666133239865303 older dist loss 0.04086393117904663\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12285810708999634 class loss 0.010544105432927608 dist loss 0.06946402043104172 older dist loss 0.04284997656941414\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.12438547611236572 class loss 0.01318798866122961 dist loss 0.06913759559392929 older dist loss 0.04205989092588425\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12008719146251678 class loss 0.01250181533396244 dist loss 0.06608637422323227 older dist loss 0.041499003767967224\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.11840157210826874 class loss 0.010556408204138279 dist loss 0.06683079898357391 older dist loss 0.04101436212658882\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.12276554852724075 class loss 0.01408347487449646 dist loss 0.0670144110918045 older dist loss 0.04166766256093979\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12021049857139587 class loss 0.011851301416754723 dist loss 0.0677565261721611 older dist loss 0.0406026691198349\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.11618707329034805 class loss 0.010925207287073135 dist loss 0.06492380797863007 older dist loss 0.04033805802464485\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12066957354545593 class loss 0.009655415080487728 dist loss 0.0693373903632164 older dist loss 0.04167677089571953\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.11876649409532547 class loss 0.009171360172331333 dist loss 0.06798188388347626 older dist loss 0.041613250970840454\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.11825557053089142 class loss 0.01178357657045126 dist loss 0.06548863649368286 older dist loss 0.04098336026072502\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12386319786310196 class loss 0.01174070779234171 dist loss 0.0685291513800621 older dist loss 0.04359333962202072\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12129972875118256 class loss 0.013245189562439919 dist loss 0.06622558832168579 older dist loss 0.041828952729701996\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12118940055370331 class loss 0.010595482774078846 dist loss 0.06781477481126785 older dist loss 0.04277914762496948\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.11503853648900986 class loss 0.0071105374954640865 dist loss 0.0663544163107872 older dist loss 0.04157358407974243\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12414263188838959 class loss 0.011103346012532711 dist loss 0.06822292506694794 older dist loss 0.044816359877586365\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.11875873804092407 class loss 0.0085570914670825 dist loss 0.06649164855480194 older dist loss 0.043709997087717056\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12247463315725327 class loss 0.009120888076722622 dist loss 0.06918241828680038 older dist loss 0.044171325862407684\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12445437908172607 class loss 0.010390530340373516 dist loss 0.06947368383407593 older dist loss 0.044590163975954056\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.11743444949388504 class loss 0.008975477889180183 dist loss 0.06576234102249146 older dist loss 0.04269662871956825\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.12193059921264648 class loss 0.008862270973622799 dist loss 0.06950986385345459 older dist loss 0.04355845972895622\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.11260005831718445 class loss 0.007301076781004667 dist loss 0.06481286883354187 older dist loss 0.04048611596226692\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.11938382685184479 class loss 0.0062419213354587555 dist loss 0.06958573311567307 older dist loss 0.043556176126003265\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12264227122068405 class loss 0.010743975639343262 dist loss 0.0688207596540451 older dist loss 0.043077535927295685\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.1205291897058487 class loss 0.007698992732912302 dist loss 0.07006032764911652 older dist loss 0.042769864201545715\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.1159687414765358 class loss 0.008386345580220222 dist loss 0.0669618472456932 older dist loss 0.04062055051326752\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.12319403886795044 class loss 0.00825535785406828 dist loss 0.06905876845121384 older dist loss 0.04587990790605545\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.11367404460906982 class loss 0.007080321665853262 dist loss 0.06667302548885345 older dist loss 0.03992069885134697\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11155937612056732 class loss 0.006527315825223923 dist loss 0.06564153730869293 older dist loss 0.039390526711940765\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.11032256484031677 class loss 0.00462165055796504 dist loss 0.06452208757400513 older dist loss 0.041178829967975616\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.10504555702209473 class loss 0.005537310149520636 dist loss 0.06134442612528801 older dist loss 0.03816382214426994\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.10508392751216888 class loss 0.004021185915917158 dist loss 0.06259411573410034 older dist loss 0.0384686216711998\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.1087239682674408 class loss 0.003893645480275154 dist loss 0.06337717175483704 older dist loss 0.041453152894973755\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.10676856338977814 class loss 0.004226326942443848 dist loss 0.06323932856321335 older dist loss 0.03930290415883064\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.10465773195028305 class loss 0.0034434248227626085 dist loss 0.06263350695371628 older dist loss 0.03858080133795738\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.10565181076526642 class loss 0.003356377827003598 dist loss 0.062489449977874756 older dist loss 0.03980598598718643\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.10143516212701797 class loss 0.0030877620447427034 dist loss 0.0601738765835762 older dist loss 0.03817352280020714\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.10887200385332108 class loss 0.004106041509658098 dist loss 0.06337856501340866 older dist loss 0.041387397795915604\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.10682818293571472 class loss 0.004070590250194073 dist loss 0.06369315832853317 older dist loss 0.0390644297003746\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.10449042916297913 class loss 0.004797419998794794 dist loss 0.06104167178273201 older dist loss 0.03865133225917816\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.10538099706172943 class loss 0.0035683077294379473 dist loss 0.06290788948535919 older dist loss 0.03890480101108551\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.10076797008514404 class loss 0.00258781504817307 dist loss 0.06037203595042229 older dist loss 0.037808116525411606\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.10124130547046661 class loss 0.002726351609453559 dist loss 0.059940483421087265 older dist loss 0.03857446834445\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.10367540270090103 class loss 0.002512641716748476 dist loss 0.062171317636966705 older dist loss 0.03899144381284714\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.09939773380756378 class loss 0.0021888623014092445 dist loss 0.0594160258769989 older dist loss 0.03779284283518791\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.09795065969228745 class loss 0.0018384529976174235 dist loss 0.05941762402653694 older dist loss 0.03669458255171776\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.10390995442867279 class loss 0.0024500933941453695 dist loss 0.06215512380003929 older dist loss 0.039304737001657486\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.1029803529381752 class loss 0.0028812342789024115 dist loss 0.061342041939496994 older dist loss 0.0387570783495903\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.10181528329849243 class loss 0.002991626039147377 dist loss 0.0608937107026577 older dist loss 0.037929944694042206\n",
            "Reducing each exemplar set to size: 40\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 65.54\n",
            "\n",
            "Test Accuracy (all groups seen so far): 56.76\n",
            "\n",
            "the model knows 50 classes:\n",
            " \n",
            "GROUP:  6\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1330995112657547 class loss 0.027291886508464813 dist loss 0.06428682804107666 older dist loss 0.041520796716213226\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12652668356895447 class loss 0.019318895414471626 dist loss 0.06564359366893768 older dist loss 0.04156418889760971\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.12585747241973877 class loss 0.017229003831744194 dist loss 0.06646151840686798 older dist loss 0.04216695576906204\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.12734904885292053 class loss 0.01937231793999672 dist loss 0.06585979461669922 older dist loss 0.042116936296224594\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12648466229438782 class loss 0.019393520429730415 dist loss 0.06600343436002731 older dist loss 0.041087713092565536\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.12799778580665588 class loss 0.016952913254499435 dist loss 0.0685301199555397 older dist loss 0.042514752596616745\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12373203784227371 class loss 0.01598355919122696 dist loss 0.06581275910139084 older dist loss 0.041935719549655914\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.13004881143569946 class loss 0.019041679799556732 dist loss 0.06813936680555344 older dist loss 0.04286776855587959\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12780418992042542 class loss 0.01685725525021553 dist loss 0.06844398379325867 older dist loss 0.042502958327531815\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.1224306970834732 class loss 0.012431270442903042 dist loss 0.06720193475484848 older dist loss 0.04279748722910881\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12555637955665588 class loss 0.015420433133840561 dist loss 0.06861318647861481 older dist loss 0.041522763669490814\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.12001073360443115 class loss 0.012209497392177582 dist loss 0.0667029470205307 older dist loss 0.04109828546643257\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12357825040817261 class loss 0.01386985369026661 dist loss 0.06749971210956573 older dist loss 0.042208682745695114\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.1270734816789627 class loss 0.015143281780183315 dist loss 0.0688803419470787 older dist loss 0.043049853295087814\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12191460281610489 class loss 0.010413255542516708 dist loss 0.06909587234258652 older dist loss 0.04240547493100166\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.11966066062450409 class loss 0.01092031504958868 dist loss 0.06727173924446106 older dist loss 0.041468601673841476\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12196022272109985 class loss 0.009845991618931293 dist loss 0.06958533823490143 older dist loss 0.042528893798589706\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.1214136928319931 class loss 0.011158224195241928 dist loss 0.06779008358716965 older dist loss 0.042465388774871826\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12151293456554413 class loss 0.010231919586658478 dist loss 0.0688118264079094 older dist loss 0.04246918484568596\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12857890129089355 class loss 0.012569556944072247 dist loss 0.07181133329868317 older dist loss 0.04419800266623497\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12200414389371872 class loss 0.009202508255839348 dist loss 0.0695246234536171 older dist loss 0.043277014046907425\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.11947938799858093 class loss 0.010792149230837822 dist loss 0.06727394461631775 older dist loss 0.04141329601407051\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12108515202999115 class loss 0.00930868275463581 dist loss 0.06824978440999985 older dist loss 0.04352668300271034\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.12470591813325882 class loss 0.00913990382105112 dist loss 0.07099423557519913 older dist loss 0.044571779668331146\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12543651461601257 class loss 0.010180791839957237 dist loss 0.07188649475574493 older dist loss 0.04336922988295555\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12223470211029053 class loss 0.009493069723248482 dist loss 0.06895418465137482 older dist loss 0.04378744959831238\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.11992602050304413 class loss 0.007971562445163727 dist loss 0.06985298544168472 older dist loss 0.04210147261619568\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12390792369842529 class loss 0.009296913631260395 dist loss 0.0713927373290062 older dist loss 0.04321827366948128\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.1235148161649704 class loss 0.008769139647483826 dist loss 0.06998538970947266 older dist loss 0.04476028308272362\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.1180783212184906 class loss 0.007670236751437187 dist loss 0.06805409491062164 older dist loss 0.04235399141907692\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.12391763180494308 class loss 0.007735030259937048 dist loss 0.07236403971910477 older dist loss 0.04381856322288513\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.1236872524023056 class loss 0.00899851880967617 dist loss 0.06934543699026108 older dist loss 0.045343298465013504\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.11798274517059326 class loss 0.006757989060133696 dist loss 0.06875841319561005 older dist loss 0.04246634244918823\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.11774173378944397 class loss 0.00752907944843173 dist loss 0.06771848350763321 older dist loss 0.04249417409300804\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.1174868494272232 class loss 0.005433610640466213 dist loss 0.06927286088466644 older dist loss 0.042780376970767975\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.12331843376159668 class loss 0.010076410137116909 dist loss 0.07043878734111786 older dist loss 0.042803239077329636\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12250567227602005 class loss 0.008196808397769928 dist loss 0.0717267170548439 older dist loss 0.04258214682340622\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.11913546919822693 class loss 0.006659958511590958 dist loss 0.06902007758617401 older dist loss 0.04345543310046196\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.11914190649986267 class loss 0.007611971348524094 dist loss 0.06884214282035828 older dist loss 0.0426877960562706\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.1232241615653038 class loss 0.007693242281675339 dist loss 0.07252942770719528 older dist loss 0.04300149157643318\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12442371994256973 class loss 0.006996859796345234 dist loss 0.07180014997720718 older dist loss 0.04562671110033989\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.11817102879285812 class loss 0.0062158904038369656 dist loss 0.06941080093383789 older dist loss 0.04254433885216713\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.11645525693893433 class loss 0.0044227889738976955 dist loss 0.0695708617568016 older dist loss 0.042461611330509186\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12012805044651031 class loss 0.00605755764991045 dist loss 0.06981370598077774 older dist loss 0.0442567877471447\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12095696479082108 class loss 0.005700749345123768 dist loss 0.0719832256436348 older dist loss 0.043272990733385086\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.11892075836658478 class loss 0.005157181061804295 dist loss 0.07037331163883209 older dist loss 0.04339026287198067\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.11732615530490875 class loss 0.006100323051214218 dist loss 0.06899350136518478 older dist loss 0.04223233088850975\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.11798754334449768 class loss 0.005074142478406429 dist loss 0.06912026554346085 older dist loss 0.04379313811659813\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.11828035116195679 class loss 0.005332784727215767 dist loss 0.06992285698652267 older dist loss 0.043024707585573196\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11263018846511841 class loss 0.003971956204622984 dist loss 0.06695514917373657 older dist loss 0.041703082621097565\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.10987456142902374 class loss 0.004217606503516436 dist loss 0.06579580157995224 older dist loss 0.039861150085926056\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.11094070971012115 class loss 0.002211008220911026 dist loss 0.06716124713420868 older dist loss 0.04156845808029175\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.1096268892288208 class loss 0.001978673506528139 dist loss 0.06684285402297974 older dist loss 0.040805358439683914\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11029627174139023 class loss 0.0040234532207250595 dist loss 0.06641896814107895 older dist loss 0.03985384851694107\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.11221567541360855 class loss 0.002987575251609087 dist loss 0.06767520308494568 older dist loss 0.04155289754271507\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.11008407175540924 class loss 0.0027972194366157055 dist loss 0.066370390355587 older dist loss 0.04091646522283554\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.10921234637498856 class loss 0.0019804234616458416 dist loss 0.06658767908811569 older dist loss 0.040644243359565735\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.1078210175037384 class loss 0.002667738124728203 dist loss 0.06516390293836594 older dist loss 0.03998937830328941\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.10770668089389801 class loss 0.00217030243948102 dist loss 0.06504383683204651 older dist loss 0.04049254581332207\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.11255265027284622 class loss 0.003336216788738966 dist loss 0.06777026504278183 older dist loss 0.04144616797566414\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.10761590301990509 class loss 0.002912770723924041 dist loss 0.06470561772584915 older dist loss 0.039997514337301254\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.10639216005802155 class loss 0.0024762318935245275 dist loss 0.06476181745529175 older dist loss 0.03915410861372948\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.10927744209766388 class loss 0.00239763455465436 dist loss 0.06531371176242828 older dist loss 0.04156609997153282\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.11138482391834259 class loss 0.003222509054467082 dist loss 0.0674046203494072 older dist loss 0.04075769707560539\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.10795960575342178 class loss 0.002274580765515566 dist loss 0.0660451129078865 older dist loss 0.039639912545681\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.10639291256666183 class loss 0.0020943176932632923 dist loss 0.06431465595960617 older dist loss 0.03998393937945366\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.10904130339622498 class loss 0.0029375464655458927 dist loss 0.06580597162246704 older dist loss 0.04029778763651848\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.10778158903121948 class loss 0.0020885029807686806 dist loss 0.06583207845687866 older dist loss 0.03986101225018501\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.11032652854919434 class loss 0.001963951624929905 dist loss 0.06671760976314545 older dist loss 0.04164496809244156\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.11036911606788635 class loss 0.0022466625086963177 dist loss 0.06654892861843109 older dist loss 0.041573524475097656\n",
            "Reducing each exemplar set to size: 34\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 59.96\n",
            "\n",
            "Test Accuracy (all groups seen so far): 53.50\n",
            "\n",
            "the model knows 60 classes:\n",
            " \n",
            "GROUP:  7\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1327027678489685 class loss 0.018581993877887726 dist loss 0.07111486792564392 older dist loss 0.04300589859485626\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12799550592899323 class loss 0.015475747175514698 dist loss 0.07004846632480621 older dist loss 0.04247128963470459\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.12993620336055756 class loss 0.013255159370601177 dist loss 0.07254151999950409 older dist loss 0.04413952678442001\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.12781265377998352 class loss 0.011265236884355545 dist loss 0.0725734755396843 older dist loss 0.04397394880652428\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12496665120124817 class loss 0.012826958671212196 dist loss 0.07041756063699722 older dist loss 0.0417221337556839\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.1282631903886795 class loss 0.014774415642023087 dist loss 0.07088200747966766 older dist loss 0.04260677099227905\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12932337820529938 class loss 0.013316833414137363 dist loss 0.07246574014425278 older dist loss 0.04354080930352211\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.12526032328605652 class loss 0.011394104920327663 dist loss 0.07155278325080872 older dist loss 0.04231344163417816\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12680913507938385 class loss 0.010674749501049519 dist loss 0.07245936989784241 older dist loss 0.0436750128865242\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.12068232893943787 class loss 0.008492864668369293 dist loss 0.07014692574739456 older dist loss 0.04204254224896431\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12110200524330139 class loss 0.007598765194416046 dist loss 0.07105877995491028 older dist loss 0.04244445636868477\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.125388041138649 class loss 0.012730871327221394 dist loss 0.0708702951669693 older dist loss 0.04178687185049057\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12288102507591248 class loss 0.008394662290811539 dist loss 0.0714113712310791 older dist loss 0.04307498782873154\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.12868660688400269 class loss 0.011288071982562542 dist loss 0.07357516139745712 older dist loss 0.04382336512207985\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.1251448094844818 class loss 0.009246639907360077 dist loss 0.07270719856023788 older dist loss 0.04319097474217415\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.12171909213066101 class loss 0.008352807722985744 dist loss 0.07070807367563248 older dist loss 0.04265821352601051\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12197182327508926 class loss 0.007405875716358423 dist loss 0.07230411469936371 older dist loss 0.04226183146238327\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.1261860728263855 class loss 0.0085772555321455 dist loss 0.0735647976398468 older dist loss 0.04404401779174805\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12290191650390625 class loss 0.007895078510046005 dist loss 0.07233428955078125 older dist loss 0.04267255216836929\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12583866715431213 class loss 0.008115504868328571 dist loss 0.07386814802885056 older dist loss 0.043855007737874985\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12118746340274811 class loss 0.007947955280542374 dist loss 0.07102750241756439 older dist loss 0.042212001979351044\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.13004907965660095 class loss 0.009071733802556992 dist loss 0.07576246559619904 older dist loss 0.04521487280726433\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.122651107609272 class loss 0.008513171225786209 dist loss 0.07196177542209625 older dist loss 0.04217616096138954\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.12604013085365295 class loss 0.008094677701592445 dist loss 0.07390977442264557 older dist loss 0.04403567686676979\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12331657111644745 class loss 0.006574698723852634 dist loss 0.0737244263291359 older dist loss 0.043017443269491196\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12353357672691345 class loss 0.0071490793488919735 dist loss 0.07384485006332397 older dist loss 0.042539652436971664\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.1253204047679901 class loss 0.00760579714551568 dist loss 0.0741615891456604 older dist loss 0.04355301707983017\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12353672087192535 class loss 0.006208476610481739 dist loss 0.07379508018493652 older dist loss 0.043533165007829666\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.12104782462120056 class loss 0.006642490159720182 dist loss 0.07136784493923187 older dist loss 0.04303748905658722\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12795701622962952 class loss 0.006450043525546789 dist loss 0.07579366117715836 older dist loss 0.045713312923908234\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.1222267597913742 class loss 0.007750363554805517 dist loss 0.07169315963983536 older dist loss 0.04278324171900749\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.1198636144399643 class loss 0.007331748027354479 dist loss 0.07124021649360657 older dist loss 0.04129164665937424\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12083260715007782 class loss 0.007140750996768475 dist loss 0.07124816626310349 older dist loss 0.042443688958883286\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12614178657531738 class loss 0.0064231595024466515 dist loss 0.07545887678861618 older dist loss 0.04425975680351257\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.125176340341568 class loss 0.007024089805781841 dist loss 0.07402560114860535 older dist loss 0.04412664473056793\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.12613558769226074 class loss 0.006913614924997091 dist loss 0.07459288090467453 older dist loss 0.044629085808992386\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12431424856185913 class loss 0.0051707858219742775 dist loss 0.07481993734836578 older dist loss 0.04432353004813194\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.12213271856307983 class loss 0.004438743926584721 dist loss 0.0740559920668602 older dist loss 0.04363797977566719\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12176639586687088 class loss 0.005422257352620363 dist loss 0.07318919897079468 older dist loss 0.043154940009117126\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12151478230953217 class loss 0.005042248871177435 dist loss 0.07358698546886444 older dist loss 0.04288554936647415\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12395548820495605 class loss 0.004088092129677534 dist loss 0.07523389905691147 older dist loss 0.04463350027799606\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.12291194498538971 class loss 0.005001971963793039 dist loss 0.0737588107585907 older dist loss 0.04415115714073181\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.12334638833999634 class loss 0.004617819096893072 dist loss 0.07506630569696426 older dist loss 0.04366226866841316\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12165428698062897 class loss 0.004194948822259903 dist loss 0.07456042617559433 older dist loss 0.04289891570806503\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12121346592903137 class loss 0.005009534303098917 dist loss 0.07300379127264023 older dist loss 0.043200138956308365\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.124749556183815 class loss 0.006437842734158039 dist loss 0.07457160204648972 older dist loss 0.043740108609199524\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.12389028817415237 class loss 0.005976089276373386 dist loss 0.07467035949230194 older dist loss 0.04324384033679962\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.12525536119937897 class loss 0.004792910069227219 dist loss 0.07621504366397858 older dist loss 0.044247403740882874\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12163519859313965 class loss 0.0054263705387711525 dist loss 0.07321375608444214 older dist loss 0.04299507662653923\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11275827884674072 class loss 0.003502177307382226 dist loss 0.06848013401031494 older dist loss 0.04077596589922905\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.11070399731397629 class loss 0.0024744542315602303 dist loss 0.0682779923081398 older dist loss 0.03995155170559883\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.11400364339351654 class loss 0.0022548248525708914 dist loss 0.07035960257053375 older dist loss 0.041389212012290955\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.11450706422328949 class loss 0.002362500876188278 dist loss 0.07097126543521881 older dist loss 0.0411733016371727\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11439970135688782 class loss 0.00233457307331264 dist loss 0.0707550048828125 older dist loss 0.04131012782454491\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.11179396510124207 class loss 0.0029993481002748013 dist loss 0.06845013797283173 older dist loss 0.0403444804251194\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.11423831433057785 class loss 0.001969454577192664 dist loss 0.0708998516201973 older dist loss 0.041369009763002396\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.11533220112323761 class loss 0.003280481556430459 dist loss 0.06977185606956482 older dist loss 0.042279861867427826\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.1138925701379776 class loss 0.0021591591648757458 dist loss 0.07004135847091675 older dist loss 0.04169205576181412\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.11277888715267181 class loss 0.0023916896898299456 dist loss 0.06946789473295212 older dist loss 0.04091930761933327\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.11556293815374374 class loss 0.0021451041102409363 dist loss 0.07090822607278824 older dist loss 0.04250960797071457\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.11376391351222992 class loss 0.0013012426206842065 dist loss 0.07117006182670593 older dist loss 0.04129260778427124\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.11267854273319244 class loss 0.002491184277459979 dist loss 0.06908223778009415 older dist loss 0.04110511764883995\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.11731432378292084 class loss 0.0030776781495660543 dist loss 0.071729876101017 older dist loss 0.04250676929950714\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.11215174943208694 class loss 0.002104851882904768 dist loss 0.06977757066488266 older dist loss 0.04026932641863823\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.11081090569496155 class loss 0.003257243661209941 dist loss 0.06824895739555359 older dist loss 0.0393046997487545\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.11237333714962006 class loss 0.0017951217014342546 dist loss 0.0703364685177803 older dist loss 0.04024174436926842\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.11688992381095886 class loss 0.0016442531486973166 dist loss 0.07209338992834091 older dist loss 0.04315228387713432\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.11177130043506622 class loss 0.002023837296292186 dist loss 0.06911757588386536 older dist loss 0.04062988981604576\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.11672624945640564 class loss 0.0021620607003569603 dist loss 0.07182382047176361 older dist loss 0.04274037107825279\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.11564904451370239 class loss 0.001645738841034472 dist loss 0.07237885892391205 older dist loss 0.041624441742897034\n",
            "Reducing each exemplar set to size: 29\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 61.96\n",
            "\n",
            "Test Accuracy (all groups seen so far): 51.23\n",
            "\n",
            "the model knows 70 classes:\n",
            " \n",
            "GROUP:  8\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.12415363639593124 class loss 0.016390232369303703 dist loss 0.06780527532100677 older dist loss 0.039958130568265915\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12966632843017578 class loss 0.01733977533876896 dist loss 0.07037196308374405 older dist loss 0.04195459187030792\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.1277002990245819 class loss 0.01671198569238186 dist loss 0.06917492300271988 older dist loss 0.04181338846683502\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.1290624737739563 class loss 0.01615234650671482 dist loss 0.07056104391813278 older dist loss 0.042349085211753845\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12784898281097412 class loss 0.015976758673787117 dist loss 0.0699286237359047 older dist loss 0.041943591088056564\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.1239357590675354 class loss 0.0134874377399683 dist loss 0.06931618601083755 older dist loss 0.041132133454084396\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12998132407665253 class loss 0.013034315779805183 dist loss 0.07340193539857864 older dist loss 0.04354507476091385\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.12513619661331177 class loss 0.011535844765603542 dist loss 0.07239579409360886 older dist loss 0.041204553097486496\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12498921155929565 class loss 0.011842495761811733 dist loss 0.07116670906543732 older dist loss 0.04198000580072403\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.12383276224136353 class loss 0.010808167979121208 dist loss 0.07108806073665619 older dist loss 0.04193653538823128\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12045568227767944 class loss 0.010179336182773113 dist loss 0.06907662749290466 older dist loss 0.04119971767067909\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.12804242968559265 class loss 0.010394211858510971 dist loss 0.07320618629455566 older dist loss 0.044442035257816315\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12316785007715225 class loss 0.010109647177159786 dist loss 0.07123876363039017 older dist loss 0.041819438338279724\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.1273074746131897 class loss 0.012542963027954102 dist loss 0.07255431264638901 older dist loss 0.04221019521355629\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12244729697704315 class loss 0.009492299519479275 dist loss 0.07120653986930847 older dist loss 0.04174845665693283\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.1255556046962738 class loss 0.011577698402106762 dist loss 0.07199705392122269 older dist loss 0.04198085516691208\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12312744557857513 class loss 0.008124135434627533 dist loss 0.07208985090255737 older dist loss 0.04291345551609993\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.12755900621414185 class loss 0.011908954940736294 dist loss 0.072478286921978 older dist loss 0.04317177087068558\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12756134569644928 class loss 0.01017141342163086 dist loss 0.07363059371709824 older dist loss 0.043759338557720184\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12125575542449951 class loss 0.009751024655997753 dist loss 0.07005759328603745 older dist loss 0.04144713282585144\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12384536862373352 class loss 0.01083973329514265 dist loss 0.07174092531204224 older dist loss 0.04126470535993576\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.1260538399219513 class loss 0.011381189338862896 dist loss 0.07198070734739304 older dist loss 0.042691949754953384\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12459434568881989 class loss 0.010264436714351177 dist loss 0.07289707660675049 older dist loss 0.041432835161685944\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.12437489628791809 class loss 0.010945065878331661 dist loss 0.07170451432466507 older dist loss 0.04172531142830849\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12569108605384827 class loss 0.009325381368398666 dist loss 0.07368332147598267 older dist loss 0.042682383209466934\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12386850267648697 class loss 0.011045346036553383 dist loss 0.07126325368881226 older dist loss 0.04155990481376648\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.12297545373439789 class loss 0.00801656860858202 dist loss 0.07291591167449951 older dist loss 0.04204297438263893\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12590347230434418 class loss 0.008194433525204659 dist loss 0.07486128062009811 older dist loss 0.042847756296396255\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.12296045571565628 class loss 0.0075319246388971806 dist loss 0.07300981879234314 older dist loss 0.0424187108874321\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12355291843414307 class loss 0.0068239085376262665 dist loss 0.07375476509332657 older dist loss 0.04297424480319023\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.12416404485702515 class loss 0.007426326163113117 dist loss 0.07334961742162704 older dist loss 0.04338810592889786\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.1266835331916809 class loss 0.0085471011698246 dist loss 0.07398071885108948 older dist loss 0.04415570944547653\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12307119369506836 class loss 0.006456504110246897 dist loss 0.07384224981069565 older dist loss 0.042772434651851654\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12707208096981049 class loss 0.007507959846407175 dist loss 0.07541588693857193 older dist loss 0.04414822906255722\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12421512603759766 class loss 0.006544253323227167 dist loss 0.07388590276241302 older dist loss 0.0437849685549736\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.1251670867204666 class loss 0.008523666299879551 dist loss 0.07383396476507187 older dist loss 0.04280945658683777\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12545135617256165 class loss 0.008475224487483501 dist loss 0.07368221879005432 older dist loss 0.04329390451312065\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.126204714179039 class loss 0.007249416317790747 dist loss 0.07520542293787003 older dist loss 0.04374987632036209\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12324121594429016 class loss 0.007710437756031752 dist loss 0.07242351770401001 older dist loss 0.04310726374387741\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12293294072151184 class loss 0.005513864103704691 dist loss 0.07414928823709488 older dist loss 0.04326978698372841\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12135171890258789 class loss 0.005809950176626444 dist loss 0.07256519794464111 older dist loss 0.04297656565904617\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.12866222858428955 class loss 0.008635890670120716 dist loss 0.07602667808532715 older dist loss 0.04399966448545456\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1276712715625763 class loss 0.0069952295161783695 dist loss 0.07627078890800476 older dist loss 0.044405244290828705\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.1232997477054596 class loss 0.006315756589174271 dist loss 0.07408669590950012 older dist loss 0.042897291481494904\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12719367444515228 class loss 0.006365455221384764 dist loss 0.07617097347974777 older dist loss 0.04465724900364876\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.12237685918807983 class loss 0.004211850464344025 dist loss 0.07487548142671585 older dist loss 0.04328952729701996\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.12630006670951843 class loss 0.006939233280718327 dist loss 0.07547736167907715 older dist loss 0.04388347640633583\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.12152142822742462 class loss 0.005845492705702782 dist loss 0.07304361462593079 older dist loss 0.042632319033145905\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12111042439937592 class loss 0.005525801330804825 dist loss 0.07326795905828476 older dist loss 0.04231666773557663\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11407479643821716 class loss 0.003854132490232587 dist loss 0.0692129135131836 older dist loss 0.04100774973630905\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.11464528739452362 class loss 0.003309287829324603 dist loss 0.07017411291599274 older dist loss 0.04116188362240791\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.12013187259435654 class loss 0.004098699428141117 dist loss 0.07345356047153473 older dist loss 0.042579613626003265\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.11900751292705536 class loss 0.0036039785481989384 dist loss 0.07321786135435104 older dist loss 0.04218567535281181\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11557751893997192 class loss 0.0033081979490816593 dist loss 0.07093749940395355 older dist loss 0.04133181646466255\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.11448037624359131 class loss 0.0035119266249239445 dist loss 0.06983596086502075 older dist loss 0.04113248363137245\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.11521923542022705 class loss 0.002493009902536869 dist loss 0.07092087715864182 older dist loss 0.04180534556508064\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.11183692514896393 class loss 0.002693837508559227 dist loss 0.06917300820350647 older dist loss 0.03997007757425308\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.11657143384218216 class loss 0.0036243645008653402 dist loss 0.07099303603172302 older dist loss 0.04195403307676315\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.11323338001966476 class loss 0.0022237335797399282 dist loss 0.07027734071016312 older dist loss 0.040732305496931076\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.11362157016992569 class loss 0.0021910748910158873 dist loss 0.07029152661561966 older dist loss 0.041138969361782074\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.11422114074230194 class loss 0.0030851184856146574 dist loss 0.07018541544675827 older dist loss 0.040950603783130646\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.11315205693244934 class loss 0.002689334098249674 dist loss 0.06987909227609634 older dist loss 0.04058362916111946\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.11570557951927185 class loss 0.0038375796284526587 dist loss 0.07069607824087143 older dist loss 0.04117191955447197\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.11431309580802917 class loss 0.0027287600096315145 dist loss 0.07083369046449661 older dist loss 0.040750641375780106\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.11419779807329178 class loss 0.002282469067722559 dist loss 0.07026074081659317 older dist loss 0.04165458679199219\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.11442558467388153 class loss 0.002492733998224139 dist loss 0.07039866596460342 older dist loss 0.0415341891348362\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.11486039310693741 class loss 0.002615513978525996 dist loss 0.07127420604228973 older dist loss 0.04097067192196846\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.11226144433021545 class loss 0.00203019380569458 dist loss 0.06955563277006149 older dist loss 0.04067561402916908\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.11452552676200867 class loss 0.00207249796949327 dist loss 0.07061449438333511 older dist loss 0.04183853417634964\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.11517520248889923 class loss 0.0028546650428324938 dist loss 0.07008785754442215 older dist loss 0.042232681065797806\n",
            "Reducing each exemplar set to size: 25\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 55.76\n",
            "\n",
            "Test Accuracy (all groups seen so far): 47.99\n",
            "\n",
            "the model knows 80 classes:\n",
            " \n",
            "GROUP:  9\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.13113002479076385 class loss 0.01343674398958683 dist loss 0.07479092478752136 older dist loss 0.04290235787630081\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.12428232282400131 class loss 0.012360204942524433 dist loss 0.07166446000337601 older dist loss 0.04025765880942345\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.1259373426437378 class loss 0.012362738139927387 dist loss 0.0725850835442543 older dist loss 0.040989529341459274\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.1317576766014099 class loss 0.010827240534126759 dist loss 0.07755357772111893 older dist loss 0.0433768667280674\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.12713523209095 class loss 0.010531792417168617 dist loss 0.07439170032739639 older dist loss 0.04221173748373985\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.12530359625816345 class loss 0.010623196139931679 dist loss 0.07323793321847916 older dist loss 0.04144245758652687\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12468092143535614 class loss 0.00816263910382986 dist loss 0.07439487427473068 older dist loss 0.04212341085076332\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.1294671595096588 class loss 0.009204824455082417 dist loss 0.07716630399227142 older dist loss 0.04309602454304695\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12551839649677277 class loss 0.009703423827886581 dist loss 0.07394254952669144 older dist loss 0.04187241941690445\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.12895694375038147 class loss 0.008361577056348324 dist loss 0.07678905874490738 older dist loss 0.04380631446838379\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12509717047214508 class loss 0.009321676567196846 dist loss 0.07440047711133957 older dist loss 0.041375018656253815\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.12737970054149628 class loss 0.00962898414582014 dist loss 0.07494277507066727 older dist loss 0.042807940393686295\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.11916130781173706 class loss 0.007402260322123766 dist loss 0.0717637836933136 older dist loss 0.03999526798725128\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.12671178579330444 class loss 0.009825375862419605 dist loss 0.07498559355735779 older dist loss 0.04190082475543022\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12582042813301086 class loss 0.0068525527603924274 dist loss 0.0767081156373024 older dist loss 0.04225975647568703\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.12581904232501984 class loss 0.00731565710157156 dist loss 0.07606378942728043 older dist loss 0.042439598590135574\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12302671372890472 class loss 0.006940244231373072 dist loss 0.07433346658945084 older dist loss 0.041752997785806656\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.12356805056333542 class loss 0.006268764846026897 dist loss 0.07450942695140839 older dist loss 0.04278985783457756\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.12467782199382782 class loss 0.0063077486120164394 dist loss 0.07603738456964493 older dist loss 0.04233269393444061\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12156395614147186 class loss 0.006431287620216608 dist loss 0.07349751889705658 older dist loss 0.041635144501924515\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.1255055069923401 class loss 0.005434584338217974 dist loss 0.07716012746095657 older dist loss 0.042910803109407425\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.12231127917766571 class loss 0.005246463697403669 dist loss 0.0750705748796463 older dist loss 0.041994236409664154\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.12512551248073578 class loss 0.007160752080380917 dist loss 0.07530830055475235 older dist loss 0.042656462639570236\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.12611398100852966 class loss 0.006237382534891367 dist loss 0.07702121883630753 older dist loss 0.042855385690927505\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12514078617095947 class loss 0.006289549637585878 dist loss 0.07618039101362228 older dist loss 0.042670838534832\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12170533835887909 class loss 0.005475986283272505 dist loss 0.0746770054101944 older dist loss 0.0415523424744606\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.12539586424827576 class loss 0.005597495008260012 dist loss 0.07641024142503738 older dist loss 0.043388135731220245\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12299422174692154 class loss 0.005198327358812094 dist loss 0.07525760680437088 older dist loss 0.04253828898072243\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.12095777690410614 class loss 0.005677673500031233 dist loss 0.07408042997121811 older dist loss 0.04119967296719551\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12422074377536774 class loss 0.0066996170207858086 dist loss 0.07526260614395142 older dist loss 0.042258523404598236\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.12617994844913483 class loss 0.005833636969327927 dist loss 0.07781152427196503 older dist loss 0.04253478720784187\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.12469050288200378 class loss 0.006191081367433071 dist loss 0.07634441554546356 older dist loss 0.042155008763074875\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.12116066366434097 class loss 0.004873365163803101 dist loss 0.0744352787733078 older dist loss 0.04185201972723007\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12098246812820435 class loss 0.004289531614631414 dist loss 0.07484305649995804 older dist loss 0.041849877685308456\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12393935024738312 class loss 0.004621414002031088 dist loss 0.07673703879117966 older dist loss 0.04258090257644653\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.12468015402555466 class loss 0.005958809982985258 dist loss 0.07617239654064178 older dist loss 0.04254894703626633\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12582512199878693 class loss 0.006195397116243839 dist loss 0.07650510966777802 older dist loss 0.043124619871377945\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.12499082833528519 class loss 0.004830658435821533 dist loss 0.07748512923717499 older dist loss 0.042675040662288666\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12336955964565277 class loss 0.0032435611356049776 dist loss 0.07697319239377975 older dist loss 0.043152809143066406\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12577222287654877 class loss 0.005532040260732174 dist loss 0.07732858508825302 older dist loss 0.0429115928709507\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12307018041610718 class loss 0.004982029087841511 dist loss 0.07552311569452286 older dist loss 0.042565032839775085\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.12327788770198822 class loss 0.0038319069426506758 dist loss 0.07673395425081253 older dist loss 0.0427120216190815\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.1204875186085701 class loss 0.00484762666746974 dist loss 0.0742485448718071 older dist loss 0.04139134660363197\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12260954082012177 class loss 0.0033149414230138063 dist loss 0.07654080539941788 older dist loss 0.04275379702448845\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.12026602029800415 class loss 0.003679858287796378 dist loss 0.07452008128166199 older dist loss 0.04206608608365059\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.12318649142980576 class loss 0.002936369739472866 dist loss 0.077141672372818 older dist loss 0.04310844838619232\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.12234069406986237 class loss 0.003399715991690755 dist loss 0.07616709172725677 older dist loss 0.042773887515068054\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.12195183336734772 class loss 0.0037187763955444098 dist loss 0.07632223516702652 older dist loss 0.041910819709300995\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12415091693401337 class loss 0.0032140950206667185 dist loss 0.07764595746994019 older dist loss 0.043290864676237106\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.11916209757328033 class loss 0.0028708309400826693 dist loss 0.07490304857492447 older dist loss 0.04138821363449097\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.11685311794281006 class loss 0.002120682504028082 dist loss 0.07379503548145294 older dist loss 0.0409373976290226\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.11952802538871765 class loss 0.0028507935348898172 dist loss 0.07461316138505936 older dist loss 0.04206406697630882\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.11553564667701721 class loss 0.0020336280576884747 dist loss 0.07293402403593063 older dist loss 0.04056799039244652\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11687654256820679 class loss 0.0024884266313165426 dist loss 0.07363692671060562 older dist loss 0.04075118526816368\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.11395810544490814 class loss 0.0021904853638261557 dist loss 0.07141788303852081 older dist loss 0.04034973680973053\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.11728256940841675 class loss 0.0028548864647746086 dist loss 0.07318589091300964 older dist loss 0.04124179482460022\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.12002145498991013 class loss 0.002422550693154335 dist loss 0.07555371522903442 older dist loss 0.042045190930366516\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.11741190403699875 class loss 0.002380960388109088 dist loss 0.07356257736682892 older dist loss 0.04146836698055267\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.11546681821346283 class loss 0.002137394156306982 dist loss 0.07242760807275772 older dist loss 0.04090182110667229\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.11540603637695312 class loss 0.002538675209507346 dist loss 0.07211600989103317 older dist loss 0.04075135663151741\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.11800415068864822 class loss 0.001739587401971221 dist loss 0.07496074587106705 older dist loss 0.04130381718277931\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.11688092350959778 class loss 0.0024884045124053955 dist loss 0.07322534918785095 older dist loss 0.04116716980934143\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.11658160388469696 class loss 0.0014346095267683268 dist loss 0.07379809767007828 older dist loss 0.0413488931953907\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.11397145688533783 class loss 0.0017989656189456582 dist loss 0.07185200601816177 older dist loss 0.04032048583030701\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.11591829359531403 class loss 0.002068395260721445 dist loss 0.0728629007935524 older dist loss 0.04098699614405632\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.11643721163272858 class loss 0.0022358077112585306 dist loss 0.07361774146556854 older dist loss 0.04058365896344185\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.11319395899772644 class loss 0.0014419585932046175 dist loss 0.07152821868658066 older dist loss 0.040223781019449234\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.1146375760436058 class loss 0.0022158753126859665 dist loss 0.07233624160289764 older dist loss 0.040085457265377045\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.11544322967529297 class loss 0.0021197921596467495 dist loss 0.0725150778889656 older dist loss 0.040808357298374176\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.11510781943798065 class loss 0.0018072298262268305 dist loss 0.0729760229587555 older dist loss 0.04032456874847412\n",
            "Reducing each exemplar set to size: 23\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 53.50\n",
            "\n",
            "Test Accuracy (all groups seen so far): 46.46\n",
            "\n",
            "the model knows 90 classes:\n",
            " \n",
            "GROUP:  10\n",
            "NUM_EPOCHS:  0 / 70\n",
            "LOSS:  0.1285182237625122 class loss 0.012381189502775669 dist loss 0.07349303364753723 older dist loss 0.04264400526881218\n",
            "NUM_EPOCHS:  1 / 70\n",
            "LOSS:  0.1250077188014984 class loss 0.010771861299872398 dist loss 0.07298611849546432 older dist loss 0.041249729692935944\n",
            "NUM_EPOCHS:  2 / 70\n",
            "LOSS:  0.12834683060646057 class loss 0.011041810736060143 dist loss 0.07483448833227158 older dist loss 0.0424705408513546\n",
            "NUM_EPOCHS:  3 / 70\n",
            "LOSS:  0.12854470312595367 class loss 0.009447535499930382 dist loss 0.07594351470470428 older dist loss 0.04315365478396416\n",
            "NUM_EPOCHS:  4 / 70\n",
            "LOSS:  0.13008752465248108 class loss 0.01004026923328638 dist loss 0.07702749222517014 older dist loss 0.043019771575927734\n",
            "NUM_EPOCHS:  5 / 70\n",
            "LOSS:  0.12719488143920898 class loss 0.009582116268575191 dist loss 0.0748535692691803 older dist loss 0.042759187519550323\n",
            "NUM_EPOCHS:  6 / 70\n",
            "LOSS:  0.12489750981330872 class loss 0.00779491662979126 dist loss 0.07422904670238495 older dist loss 0.042873550206422806\n",
            "NUM_EPOCHS:  7 / 70\n",
            "LOSS:  0.12552089989185333 class loss 0.008627629838883877 dist loss 0.07528826594352722 older dist loss 0.04160500690340996\n",
            "NUM_EPOCHS:  8 / 70\n",
            "LOSS:  0.12735582888126373 class loss 0.007297398056834936 dist loss 0.07639617472887039 older dist loss 0.04366225376725197\n",
            "NUM_EPOCHS:  9 / 70\n",
            "LOSS:  0.13059203326702118 class loss 0.008944720961153507 dist loss 0.07789362221956253 older dist loss 0.043753691017627716\n",
            "NUM_EPOCHS:  10 / 70\n",
            "LOSS:  0.12444773316383362 class loss 0.007065053563565016 dist loss 0.07523739337921143 older dist loss 0.04214528203010559\n",
            "NUM_EPOCHS:  11 / 70\n",
            "LOSS:  0.1268320381641388 class loss 0.008662032894790173 dist loss 0.07566384971141815 older dist loss 0.042506154626607895\n",
            "NUM_EPOCHS:  12 / 70\n",
            "LOSS:  0.12933585047721863 class loss 0.008175794035196304 dist loss 0.07733706384897232 older dist loss 0.0438230000436306\n",
            "NUM_EPOCHS:  13 / 70\n",
            "LOSS:  0.13057009875774384 class loss 0.008999980986118317 dist loss 0.07763875275850296 older dist loss 0.04393136128783226\n",
            "NUM_EPOCHS:  14 / 70\n",
            "LOSS:  0.12880641222000122 class loss 0.006933432072401047 dist loss 0.07821626216173172 older dist loss 0.04365672171115875\n",
            "NUM_EPOCHS:  15 / 70\n",
            "LOSS:  0.1293659806251526 class loss 0.0076420302502810955 dist loss 0.07801376283168793 older dist loss 0.04371017962694168\n",
            "NUM_EPOCHS:  16 / 70\n",
            "LOSS:  0.12443362176418304 class loss 0.006433483213186264 dist loss 0.07570403069257736 older dist loss 0.04229610413312912\n",
            "NUM_EPOCHS:  17 / 70\n",
            "LOSS:  0.12736102938652039 class loss 0.007585068698972464 dist loss 0.07647286355495453 older dist loss 0.04330309107899666\n",
            "NUM_EPOCHS:  18 / 70\n",
            "LOSS:  0.1303514838218689 class loss 0.00849822536110878 dist loss 0.0777992531657219 older dist loss 0.044054001569747925\n",
            "NUM_EPOCHS:  19 / 70\n",
            "LOSS:  0.12520955502986908 class loss 0.00576237216591835 dist loss 0.07669174671173096 older dist loss 0.042755432426929474\n",
            "NUM_EPOCHS:  20 / 70\n",
            "LOSS:  0.12750571966171265 class loss 0.007008284330368042 dist loss 0.07727421820163727 older dist loss 0.043223217129707336\n",
            "NUM_EPOCHS:  21 / 70\n",
            "LOSS:  0.12638048827648163 class loss 0.006516754161566496 dist loss 0.07693357765674591 older dist loss 0.042930152267217636\n",
            "NUM_EPOCHS:  22 / 70\n",
            "LOSS:  0.1241646260023117 class loss 0.0066773309372365475 dist loss 0.07536673545837402 older dist loss 0.042120564728975296\n",
            "NUM_EPOCHS:  23 / 70\n",
            "LOSS:  0.1256161630153656 class loss 0.006508443504571915 dist loss 0.07638563960790634 older dist loss 0.04272208735346794\n",
            "NUM_EPOCHS:  24 / 70\n",
            "LOSS:  0.12887467443943024 class loss 0.006175498012453318 dist loss 0.07870423793792725 older dist loss 0.04399494081735611\n",
            "NUM_EPOCHS:  25 / 70\n",
            "LOSS:  0.12787479162216187 class loss 0.0062269787304103374 dist loss 0.07772964239120483 older dist loss 0.043918173760175705\n",
            "NUM_EPOCHS:  26 / 70\n",
            "LOSS:  0.12854884564876556 class loss 0.00660735322162509 dist loss 0.07831667363643646 older dist loss 0.043624818325042725\n",
            "NUM_EPOCHS:  27 / 70\n",
            "LOSS:  0.12609747052192688 class loss 0.005604706238955259 dist loss 0.07728716731071472 older dist loss 0.04320560023188591\n",
            "NUM_EPOCHS:  28 / 70\n",
            "LOSS:  0.13015879690647125 class loss 0.006147665437310934 dist loss 0.07948269695043564 older dist loss 0.04452843219041824\n",
            "NUM_EPOCHS:  29 / 70\n",
            "LOSS:  0.12169624865055084 class loss 0.005694001913070679 dist loss 0.07426003366708755 older dist loss 0.04174221307039261\n",
            "NUM_EPOCHS:  30 / 70\n",
            "LOSS:  0.1238061711192131 class loss 0.0059606702998280525 dist loss 0.07555679976940155 older dist loss 0.042288701981306076\n",
            "NUM_EPOCHS:  31 / 70\n",
            "LOSS:  0.12422743439674377 class loss 0.005032235290855169 dist loss 0.07664770632982254 older dist loss 0.04254749044775963\n",
            "NUM_EPOCHS:  32 / 70\n",
            "LOSS:  0.1257081925868988 class loss 0.005501297768205404 dist loss 0.07698008418083191 older dist loss 0.04322681948542595\n",
            "NUM_EPOCHS:  33 / 70\n",
            "LOSS:  0.12534698843955994 class loss 0.004895436577498913 dist loss 0.07706209272146225 older dist loss 0.0433894582092762\n",
            "NUM_EPOCHS:  34 / 70\n",
            "LOSS:  0.12908169627189636 class loss 0.005671530030667782 dist loss 0.07887273281812668 older dist loss 0.04453743249177933\n",
            "NUM_EPOCHS:  35 / 70\n",
            "LOSS:  0.12687307596206665 class loss 0.004847950767725706 dist loss 0.07818290591239929 older dist loss 0.04384222626686096\n",
            "NUM_EPOCHS:  36 / 70\n",
            "LOSS:  0.12479852139949799 class loss 0.0037635040935128927 dist loss 0.07784460484981537 older dist loss 0.04319040849804878\n",
            "NUM_EPOCHS:  37 / 70\n",
            "LOSS:  0.12367502599954605 class loss 0.005175538826733828 dist loss 0.07568582892417908 older dist loss 0.042813658714294434\n",
            "NUM_EPOCHS:  38 / 70\n",
            "LOSS:  0.12712079286575317 class loss 0.004778791684657335 dist loss 0.07842922955751419 older dist loss 0.043912772089242935\n",
            "NUM_EPOCHS:  39 / 70\n",
            "LOSS:  0.12362886965274811 class loss 0.004657205659896135 dist loss 0.07613012939691544 older dist loss 0.042841535061597824\n",
            "NUM_EPOCHS:  40 / 70\n",
            "LOSS:  0.12434442341327667 class loss 0.0040691872127354145 dist loss 0.07697860896587372 older dist loss 0.043296631425619125\n",
            "NUM_EPOCHS:  41 / 70\n",
            "LOSS:  0.12366820871829987 class loss 0.004055095370858908 dist loss 0.07677590847015381 older dist loss 0.04283720254898071\n",
            "NUM_EPOCHS:  42 / 70\n",
            "LOSS:  0.12616758048534393 class loss 0.0032298441510647535 dist loss 0.07879653573036194 older dist loss 0.04414120316505432\n",
            "NUM_EPOCHS:  43 / 70\n",
            "LOSS:  0.12547197937965393 class loss 0.00389621639624238 dist loss 0.07769780606031418 older dist loss 0.04387795552611351\n",
            "NUM_EPOCHS:  44 / 70\n",
            "LOSS:  0.1211581900715828 class loss 0.003932153806090355 dist loss 0.07521463185548782 older dist loss 0.042011406272649765\n",
            "NUM_EPOCHS:  45 / 70\n",
            "LOSS:  0.12484127283096313 class loss 0.005400584079325199 dist loss 0.07725013792514801 older dist loss 0.0421905517578125\n",
            "NUM_EPOCHS:  46 / 70\n",
            "LOSS:  0.1275954246520996 class loss 0.0054364581592381 dist loss 0.07819739729166031 older dist loss 0.04396156594157219\n",
            "NUM_EPOCHS:  47 / 70\n",
            "LOSS:  0.1263607144355774 class loss 0.0048833140172064304 dist loss 0.07749832421541214 older dist loss 0.04397907108068466\n",
            "NUM_EPOCHS:  48 / 70\n",
            "LOSS:  0.12585942447185516 class loss 0.0035402264911681414 dist loss 0.07827626913785934 older dist loss 0.044042930006980896\n",
            "NUM_EPOCHS:  49 / 70\n",
            "LOSS:  0.12007764726877213 class loss 0.0035540421959012747 dist loss 0.07487880438566208 older dist loss 0.04164480045437813\n",
            "NUM_EPOCHS:  50 / 70\n",
            "LOSS:  0.12079675495624542 class loss 0.002341055078431964 dist loss 0.07602206617593765 older dist loss 0.042433638125658035\n",
            "NUM_EPOCHS:  51 / 70\n",
            "LOSS:  0.12064647674560547 class loss 0.0023392054717987776 dist loss 0.07573429495096207 older dist loss 0.042572975158691406\n",
            "NUM_EPOCHS:  52 / 70\n",
            "LOSS:  0.12109706550836563 class loss 0.0025497511960566044 dist loss 0.07608921080827713 older dist loss 0.042458102107048035\n",
            "NUM_EPOCHS:  53 / 70\n",
            "LOSS:  0.11946430802345276 class loss 0.0027050410863012075 dist loss 0.07484381645917892 older dist loss 0.04191545397043228\n",
            "NUM_EPOCHS:  54 / 70\n",
            "LOSS:  0.1189257949590683 class loss 0.0024144041817635298 dist loss 0.0750667080283165 older dist loss 0.041444677859544754\n",
            "NUM_EPOCHS:  55 / 70\n",
            "LOSS:  0.12408645451068878 class loss 0.0026570619083940983 dist loss 0.07778823375701904 older dist loss 0.04364115744829178\n",
            "NUM_EPOCHS:  56 / 70\n",
            "LOSS:  0.11846651136875153 class loss 0.002087431726977229 dist loss 0.0747760459780693 older dist loss 0.041603028774261475\n",
            "NUM_EPOCHS:  57 / 70\n",
            "LOSS:  0.11965455114841461 class loss 0.0019522709771990776 dist loss 0.07537129521369934 older dist loss 0.04233098775148392\n",
            "NUM_EPOCHS:  58 / 70\n",
            "LOSS:  0.12075603753328323 class loss 0.0023569639306515455 dist loss 0.07600411772727966 older dist loss 0.04239495471119881\n",
            "NUM_EPOCHS:  59 / 70\n",
            "LOSS:  0.11688560247421265 class loss 0.002141749719157815 dist loss 0.0735013484954834 older dist loss 0.04124249890446663\n",
            "NUM_EPOCHS:  60 / 70\n",
            "LOSS:  0.11838673055171967 class loss 0.0028408083599060774 dist loss 0.07397712022066116 older dist loss 0.04156879708170891\n",
            "NUM_EPOCHS:  61 / 70\n",
            "LOSS:  0.12004712224006653 class loss 0.0020077982917428017 dist loss 0.07565844058990479 older dist loss 0.04238088056445122\n",
            "NUM_EPOCHS:  62 / 70\n",
            "LOSS:  0.12061507999897003 class loss 0.0029280399903655052 dist loss 0.07498086243867874 older dist loss 0.04270617291331291\n",
            "NUM_EPOCHS:  63 / 70\n",
            "LOSS:  0.12038056552410126 class loss 0.002487904392182827 dist loss 0.07548709213733673 older dist loss 0.04240557178854942\n",
            "NUM_EPOCHS:  64 / 70\n",
            "LOSS:  0.12042401731014252 class loss 0.002285690512508154 dist loss 0.07557979226112366 older dist loss 0.04255853220820427\n",
            "NUM_EPOCHS:  65 / 70\n",
            "LOSS:  0.11801166087388992 class loss 0.001746795023791492 dist loss 0.07449392229318619 older dist loss 0.04177094250917435\n",
            "NUM_EPOCHS:  66 / 70\n",
            "LOSS:  0.11714020371437073 class loss 0.0022361157462000847 dist loss 0.07372075319290161 older dist loss 0.04118333384394646\n",
            "NUM_EPOCHS:  67 / 70\n",
            "LOSS:  0.11843201518058777 class loss 0.0019501856295391917 dist loss 0.0744292363524437 older dist loss 0.04205259308218956\n",
            "NUM_EPOCHS:  68 / 70\n",
            "LOSS:  0.11858315765857697 class loss 0.0018323460826650262 dist loss 0.07501468807458878 older dist loss 0.04173612222075462\n",
            "NUM_EPOCHS:  69 / 70\n",
            "LOSS:  0.11675872653722763 class loss 0.0020733678247779608 dist loss 0.07322366535663605 older dist loss 0.04146169498562813\n",
            "Reducing each exemplar set to size: 20\n",
            "Constructing exemplar sets class...\n",
            "Performing classification...\n",
            "Train Accuracy (on current group): 47.74\n",
            "\n",
            "Test Accuracy (all groups seen so far): 44.12\n",
            "\n",
            "the model knows 100 classes:\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_B01Oa82wF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5199bf9-da64-4591-cd22-ac8cd1dcd7b1"
      },
      "source": [
        "if herding:\n",
        "  method = 'iCaRL_{}_herding'.format(classifier)\n",
        "else:\n",
        "  method = 'iCaRL_{}_random'.format(classifier)\n",
        "\n",
        "print(\"metrics iCaRL for seed {}\".format(RANDOM_SEED))\n",
        "\n",
        "# accuracy \n",
        "data_plot_line=[]\n",
        "\n",
        "classes_per_group = 10\n",
        "for group_classes in range(0,10):\n",
        "    data_plot_line.append(((group_classes + 1)*classes_per_group, accuracies[group_classes]))\n",
        "\n",
        "# plot accuracy trend\n",
        "utils.plotAccuracyTrend(method, data_plot_line, RANDOM_SEED)\n",
        "\n",
        "# confusion matrix\n",
        "confusionMatrixData = confusion_matrix(all_labels_cm, all_preds_cm)\n",
        "utils.plotConfusionMatrix(method, confusionMatrixData, RANDOM_SEED)\n",
        "\n",
        "# write to JSON file\n",
        "utils.writeMetrics(method, RANDOM_SEED, accuracies, confusionMatrixData)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "metrics iCaRL for seed 66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtGxWw4fyzyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}